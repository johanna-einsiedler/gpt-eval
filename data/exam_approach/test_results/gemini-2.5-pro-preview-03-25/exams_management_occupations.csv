occupation,task_id,task_description,exam_author_model,tools,materials,exam,system_prompt,overview,instructions,materials_all,materials_candidate,submission,evaluation,grading,answer_key,errors,check_real_materials,check_no_internet,failed_candidate_materials,key_grade_threshold,key_grade,answer_key_count,check_overall_makes_sense,explanation_overall_makes_sense,metadata,occupation_group
Fitness and Wellness Coordinators,20316.0,Track cost-containment strategies and programs to evaluate effectiveness.,gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor', 'PDF viewer']","['Text', 'Data']",Exam not valid,"You are an excellent examiner of Fitness and Wellness Coordinators capabilities. Design a remote, **practical** exam to verify whether a Fitness and Wellness Coordinators can Track cost-containment strategies and programs to evaluate effectiveness..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets, Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the design for the **Basic Level Practical Exam** focusing on tracking cost-containment strategies and evaluating effectiveness for a Fitness and Wellness Coordinator.

---

## Fitness and Wellness Coordinator - Practical Exam (Basic Level)

**Exam Part:** Basic Level - Tracking & Evaluating Cost-Containment
**Time Limit:** 90 Minutes
**Tools Allowed:** Spreadsheet Software (e.g., Excel, Google Sheets, LibreOffice Calc), Text Editor, PDF Viewer.
**Submission Format:** Single JSON file named `test_submission.json`.

---

### Explanation for the Evaluator

**1. Purpose:**
This basic practical exam assesses the candidate's fundamental ability to work with provided data to track the costs and basic outcomes of a simple wellness program. It specifically tests their capability to organize data, perform essential calculations (totals, averages, cost per participant), compare metrics over a short period, and formulate a concise, data-driven evaluation statement regarding the program's cost-effectiveness trend. It focuses on the *mechanics* of tracking and basic evaluation using standard office tools.

**2. Structure:**
*   **Scenario:** The candidate is given a simple scenario involving a single wellness initiative implemented over two years.
*   **Input Data:** They receive a dataset (likely in a spreadsheet format or easily importable text/CSV) containing cost breakdowns and participation numbers for the two years. A simple outcome metric (e.g., participation rate or self-reported satisfaction score) might also be included.
*   **Tasks:** The candidate must:
    *   Organize and aggregate the provided data within their spreadsheet tool.
    *   Calculate key performance indicators (KPIs) for each year, such as Total Cost, Total Participants, and Cost Per Participant.
    *   Calculate the change (absolute or percentage) in these KPIs between the two years.
    *   Provide a brief, objective summary statement evaluating the trend in cost-effectiveness based *only* on the calculated metrics.
*   **Output:** The candidate structures their calculated results and evaluation statement into a predefined JSON format for submission.

**3. Skills Assessed (Basic Level):**
*   **Data Handling:** Ability to read, understand, and organize simple datasets using spreadsheet software.
*   **Basic Calculations:** Proficiency in performing fundamental calculations (Sum, Average, Division) relevant to cost tracking (e.g., Total Cost, Cost per Participant).
*   **Comparative Analysis:** Ability to calculate and interpret simple year-over-year changes in key metrics.
*   **Concise Reporting:** Ability to summarize findings objectively and succinctly in a structured format (JSON).
*   **Tool Proficiency:** Basic use of spreadsheet software for data manipulation and calculation.

**4. Why it's ""Basic"":**
*   The dataset is small and straightforward.
*   The program scenario is simple (one initiative).
*   Calculations required are fundamental (no complex ROI, statistical significance, or multi-variable analysis).
*   The evaluation requires interpreting direct calculations, not inferring broader strategic implications or considering external factors.
*   Focus is on *tracking* mechanics and *direct* evaluation of provided numbers.

---

### Exam Materials to be Provided to Candidate:

1.  **`Instructions.pdf`**:
    *   Contains the exam scenario, detailed task instructions, time limit, allowed tools, and the required JSON output structure.
    *   **Scenario Example:** ""Your organization implemented a 'Mindfulness Moments' program consisting of short, guided meditation sessions offered twice weekly. You have been provided with cost and participation data for its first two years of operation (Year 1 and Year 2). Your task is to analyze this data to track costs and participation, and provide a basic evaluation of its cost-effectiveness trend.""
    *   **Tasks:**
        *   Review the provided data in `Program_Data.xlsx`.
        *   Calculate the following for **each** year (Year 1 and Year 2):
            *   Total Annual Cost
            *   Total Annual Participant Attendances (sum of attendances across all sessions)
            *   Average Cost Per Participant Attendance
        *   Calculate the **percentage change** from Year 1 to Year 2 for:
            *   Total Annual Cost
            *   Total Annual Participant Attendances
            *   Average Cost Per Participant Attendance
        *   Based *only* on your calculations, write a brief (1-2 sentences) evaluation summary describing the trend in the program's cost-effectiveness.
        *   Structure all calculated values and the evaluation summary into a JSON object following the specified format and save it as `test_submission.json`.
    *   **Required JSON Structure:**
        ```json
        {
          ""program_name"": ""Mindfulness Moments"",
          ""analysis_period"": ""Year 1 vs Year 2"",
          ""year_1_metrics"": {
            ""total_cost"": <number>, // e.g., 5000.00
            ""total_attendance"": <integer>, // e.g., 450
            ""cost_per_attendance"": <number> // e.g., 11.11
          },
          ""year_2_metrics"": {
            ""total_cost"": <number>, // e.g., 5500.00
            ""total_attendance"": <integer>, // e.g., 580
            ""cost_per_attendance"": <number> // e.g., 9.48
          },
          ""year_over_year_change_percent"": {
            ""total_cost_change_percent"": <number>, // e.g., 10.0
            ""total_attendance_change_percent"": <number>, // e.g., 28.89
            ""cost_per_attendance_change_percent"": <number> // e.g., -14.67
          },
          ""evaluation_summary"": ""<string>"" // e.g., ""Program costs increased, but attendance grew significantly faster, resulting in improved cost-effectiveness per participant attendance in Year 2 compared to Year 1.""
        }
        ```

2.  **`Program_Data.xlsx`** (or a CSV/Text file easily importable into a spreadsheet):
    *   Contains raw data for the 'Mindfulness Moments' program.
    *   **Example Structure:**
        | Year | Quarter | Session_Type      | Cost_Category | Cost_Amount | Participants_Attended |
        | :--- | :------ | :---------------- | :------------ | :---------- | :-------------------- |
        | 1    | Q1      | Guided Meditation | Instructor    | 500         | 50                    |
        | 1    | Q1      | Guided Meditation | Materials     | 50          | 50                    |
        | 1    | Q1      | Guided Meditation | Admin Fee     | 100         | 50                    |
        | 1    | Q2      | Guided Meditation | Instructor    | 500         | 65                    |
        | 1    | Q2      | Guided Meditation | Materials     | 50          | 65                    |
        | 1    | Q2      | Guided Meditation | Admin Fee     | 100         | 65                    |
        | ...  | ...     | ...               | ...           | ...         | ...                   |
        | 1    | Q4      | Guided Meditation | Admin Fee     | 100         | 70                    |
        | 2    | Q1      | Guided Meditation | Instructor    | 550         | 80                    |
        | 2    | Q1      | Guided Meditation | Materials     | 55          | 80                    |
        | 2    | Q1      | Guided Meditation | Admin Fee     | 110         | 80                    |
        | ...  | ...     | ...               | ...           | ...         | ...                   |
        | 2    | Q4      | Guided Meditation | Admin Fee     | 110         | 95                    |

---

This design provides a clear, practical task that directly assesses the required basic skills within the given constraints. The structured JSON output simplifies remote evaluation.","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

## Fitness and Wellness Coordinator - Practical Exam (Basic Level)

**Subject:** Tracking & Evaluating Cost-Containment Effectiveness (Basic)
**Time Limit:** 90 Minutes
**Tools Allowed:** Spreadsheet Software (e.g., Excel, Google Sheets, LibreOffice Calc), Text Editor, PDF Viewer

---

### Instructions for the Candidate

**1. Your Goal:**

Your task is to analyze cost and participation data for a simple wellness program over two years. You will calculate key metrics, determine the year-over-year changes, and provide a brief, data-driven evaluation summary. This exam assesses your ability to use standard tools to track program data and evaluate basic cost-effectiveness trends.

**2. Provided Materials:**

You will be provided with one file:

*   **`Program_Data.xlsx`**: This spreadsheet file contains raw data for the 'Mindfulness Moments' wellness program. It includes information broken down by year, quarter, cost category, cost amount, and participant attendances per session/entry.

**3. Your Tasks:**

Using your spreadsheet software and the provided `Program_Data.xlsx` file, perform the following calculations:

*   **For Year 1:**
    *   Calculate the Total Annual Cost.
    *   Calculate the Total Annual Participant Attendances (sum of all attendances recorded for Year 1).
    *   Calculate the Average Cost Per Participant Attendance (Total Annual Cost / Total Annual Participant Attendances).
*   **For Year 2:**
    *   Calculate the Total Annual Cost.
    *   Calculate the Total Annual Participant Attendances (sum of all attendances recorded for Year 2).
    *   Calculate the Average Cost Per Participant Attendance (Total Annual Cost / Total Annual Participant Attendances).
*   **Year-over-Year Percentage Change:**
    *   Calculate the percentage change in Total Annual Cost from Year 1 to Year 2.
    *   Calculate the percentage change in Total Annual Participant Attendances from Year 1 to Year 2.
    *   Calculate the percentage change in Average Cost Per Participant Attendance from Year 1 to Year 2.
    *   *Calculation Note:* Use the formula: `((Year 2 Value - Year 1 Value) / Year 1 Value) * 100`.
*   **Evaluation Summary:**
    *   Based *strictly* on the metrics you have calculated above, write a concise (1-2 sentences) summary describing the trend in the program's cost-effectiveness between Year 1 and Year 2. Do not include opinions or factors not present in the calculated data.

*   **Formatting Note:** Round all monetary values and cost-per-attendance figures to two decimal places. Round all percentage change values to two decimal places. Ensure total attendance is an integer.

**4. Submission Format:**

You must submit your answers in a single JSON file named `test_submission.json`. Use a text editor to create this file. The file must follow this exact structure, replacing the placeholder values (e.g., `<number>`, `<integer>`, `<string>`) with your calculated results and evaluation summary:

```json
{
  ""program_name"": ""Mindfulness Moments"",
  ""analysis_period"": ""Year 1 vs Year 2"",
  ""year_1_metrics"": {
    ""total_cost"": <number>,
    ""total_attendance"": <integer>,
    ""cost_per_attendance"": <number>
  },
  ""year_2_metrics"": {
    ""total_cost"": <number>,
    ""total_attendance"": <integer>,
    ""cost_per_attendance"": <number>
  },
  ""year_over_year_change_percent"": {
    ""total_cost_change_percent"": <number>,
    ""total_attendance_change_percent"": <number>,
    ""cost_per_attendance_change_percent"": <number>
  },
  ""evaluation_summary"": ""<string>""
}
```

**Example Placeholder Values (Replace with your actual results):**

*   `<number>`: Should be a number, e.g., `5000.00`, `11.11`, `10.00`, `-14.67`
*   `<integer>`: Should be a whole number, e.g., `450`
*   `<string>`: Should be your text summary, e.g., `""Program costs increased, but attendance grew significantly faster, resulting in improved cost-effectiveness per participant attendance in Year 2 compared to Year 1.""`

**Ensure your final submission is a valid JSON file named `test_submission.json`.**

---
**End of Instructions**","<MATERIALS_FOR_CANDIDATE>
Here is the content for the `Program_Data.xlsx` file. You can provide this to the candidate as an actual `.xlsx` file or as a `.csv` file. The CSV content is shown below for clarity:

```csv
Year,Quarter,Session_Type,Cost_Category,Cost_Amount,Participants_Attended
1,Q1,Guided Meditation,Instructor,500,50
1,Q1,Guided Meditation,Materials,50,50
1,Q1,Guided Meditation,Admin Fee,100,50
1,Q2,Guided Meditation,Instructor,500,65
1,Q2,Guided Meditation,Materials,50,65
1,Q2,Guided Meditation,Admin Fee,100,65
1,Q3,Guided Meditation,Instructor,550,70
1,Q3,Guided Meditation,Materials,55,70
1,Q3,Guided Meditation,Admin Fee,110,70
1,Q4,Guided Meditation,Instructor,550,75
1,Q4,Guided Meditation,Materials,55,75
1,Q4,Guided Meditation,Admin Fee,110,75
2,Q1,Guided Meditation,Instructor,600,80
2,Q1,Guided Meditation,Materials,60,80
2,Q1,Guided Meditation,Admin Fee,120,80
2,Q2,Guided Meditation,Instructor,600,95
2,Q2,Guided Meditation,Materials,60,95
2,Q2,Guided Meditation,Admin Fee,120,95
2,Q3,Guided Meditation,Instructor,650,110
2,Q3,Guided Meditation,Materials,65,110
2,Q3,Guided Meditation,Admin Fee,130,110
2,Q4,Guided Meditation,Instructor,650,125
2,Q4,Guided Meditation,Materials,65,125
2,Q4,Guided Meditation,Admin Fee,130,125
```

</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. How Materials Were Created:**
*   The data represents a single, simple wellness program (""Mindfulness Moments"") over two years.
*   Costs are broken down into three categories (Instructor, Materials, Admin Fee) per quarter.
*   Participant attendance numbers are provided per quarter. **Crucially, the `Participants_Attended` number listed on each cost line corresponds to the total attendance for the sessions covered by that cost entry/quarter.** The candidate needs to sum these attendance figures *across the quarters* for each year, but *not* multiply by the number of cost lines within a quarter. For example, Year 1 Q1 total attendance is 50, not 50+50+50.
*   The numbers were chosen to be relatively simple for calculation while demonstrating a clear trend: costs increase year-over-year, but participation increases at a significantly higher rate, leading to improved cost-effectiveness (lower cost per attendance).

**2. Expected Calculations & Results:**

*   **Year 1:**
    *   Total Cost = (500+50+100) + (500+50+100) + (550+55+110) + (550+55+110) = 650 + 650 + 715 + 715 = **2730.00**
    *   Total Attendance = 50 (Q1) + 65 (Q2) + 70 (Q3) + 75 (Q4) = **260**
    *   Cost Per Attendance = 2730.00 / 260 = **10.50**
*   **Year 2:**
    *   Total Cost = (600+60+120) + (600+60+120) + (650+65+130) + (650+65+130) = 780 + 780 + 845 + 845 = **3250.00**
    *   Total Attendance = 80 (Q1) + 95 (Q2) + 110 (Q3) + 125 (Q4) = **410**
    *   Cost Per Attendance = 3250.00 / 410 = 7.9268... ≈ **7.93** (rounded to two decimal places as per instructions)
*   **Year-over-Year Change %:**
    *   Total Cost Change % = ((3250.00 - 2730.00) / 2730.00) * 100 = (520 / 2730) * 100 = 19.0476...% ≈ **19.05%** (rounded)
    *   Total Attendance Change % = ((410 - 260) / 260) * 100 = (150 / 260) * 100 = 57.6923...% ≈ **57.69%** (rounded)
    *   Cost Per Attendance Change % = ((7.93 - 10.50) / 10.50) * 100 = (-2.57 / 10.50) * 100 = -24.4761...% ≈ **-24.48%** (rounded, based on previously rounded CPA values)
        *   *Note:* If calculated using unrounded CPA values: ((3250/410) - (2730/260)) / (2730/260) * 100 ≈ -24.51%. However, the instructions imply calculating based on the Year 1 and Year 2 metrics derived first, including rounding. Therefore, -24.48% is the expected answer based on the instructions.
*   **Evaluation Summary:** The summary should reflect that costs rose, but attendance rose faster, leading to a decrease in cost per attendance (improved cost-effectiveness). A good answer would be similar to: *""Program costs increased by 19.05% from Year 1 to Year 2, while participant attendance grew significantly by 57.69%. This resulted in a 24.48% decrease in the average cost per attendance, indicating improved cost-effectiveness.""* Minor variations in wording are acceptable if they convey the same core message based *only* on the calculated data.

**3. Tricky Elements & Common Pitfalls:**
*   **Attendance Aggregation:** The most likely error is incorrectly summing the `Participants_Attended` column. Candidates might sum *all* numbers in the column, effectively triple-counting attendance within each quarter (since it's listed on each cost line). They need to recognize that the attendance figure applies to the quarter and sum the unique quarterly attendance values (50+65+70+75 for Year 1, and 80+95+110+125 for Year 2). Using Pivot Tables or SUMIF functions correctly would avoid this. Manual summing requires careful observation.
*   **Percentage Change Calculation:** Ensure the correct base year (Year 1) is used in the denominator for percentage change calculations.
*   **Rounding:** Candidates must follow the rounding instructions (two decimal places for currency/CPA/percentages, integer for attendance). Minor rounding differences might occur depending on the calculation method (e.g., rounding intermediate steps vs. final result), but the provided expected results use rounding as specified.
*   **JSON Formatting:** Errors in JSON syntax (missing commas, incorrect brackets, wrong data types like strings for numbers) will invalidate the submission.
*   **Evaluation Scope:** The evaluation summary must be based *only* on the calculated metrics. Candidates might be tempted to speculate on *why* changes occurred or suggest future actions, which is outside the scope of this basic task.

**4. ""Hidden"" Information:**
*   There is no intentionally hidden information. The main challenge lies in correctly interpreting how to aggregate the `Participants_Attended` data from the provided structure.

**5. Cross-References:**
*   All necessary information is contained within the single CSV/XLSX file. No cross-referencing with other documents is required for the calculations. The candidate instructions (provided separately) define the required output structure and calculation methods.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
Here is the content for the `Program_Data.xlsx` file. You can provide this to the candidate as an actual `.xlsx` file or as a `.csv` file. The CSV content is shown below for clarity:

```csv
Year,Quarter,Session_Type,Cost_Category,Cost_Amount,Participants_Attended
1,Q1,Guided Meditation,Instructor,500,50
1,Q1,Guided Meditation,Materials,50,50
1,Q1,Guided Meditation,Admin Fee,100,50
1,Q2,Guided Meditation,Instructor,500,65
1,Q2,Guided Meditation,Materials,50,65
1,Q2,Guided Meditation,Admin Fee,100,65
1,Q3,Guided Meditation,Instructor,550,70
1,Q3,Guided Meditation,Materials,55,70
1,Q3,Guided Meditation,Admin Fee,110,70
1,Q4,Guided Meditation,Instructor,550,75
1,Q4,Guided Meditation,Materials,55,75
1,Q4,Guided Meditation,Admin Fee,110,75
2,Q1,Guided Meditation,Instructor,600,80
2,Q1,Guided Meditation,Materials,60,80
2,Q1,Guided Meditation,Admin Fee,120,80
2,Q2,Guided Meditation,Instructor,600,95
2,Q2,Guided Meditation,Materials,60,95
2,Q2,Guided Meditation,Admin Fee,120,95
2,Q3,Guided Meditation,Instructor,650,110
2,Q3,Guided Meditation,Materials,65,110
2,Q3,Guided Meditation,Admin Fee,130,110
2,Q4,Guided Meditation,Instructor,650,125
2,Q4,Guided Meditation,Materials,65,125
2,Q4,Guided Meditation,Admin Fee,130,125
```

","Okay, here is the specification for the candidate's answer format.

---

### Submission Format Specification

**1. Submission File:**

*   You must submit a single file named `test_submission.json`.
*   Replace `YOUR_ID_HERE` in the filename instructions below with your specific candidate identifier (e.g., `GPT-4o`, `Claude-3_7-Sonnet`, etc.). While the final submitted file should be named `test_submission.json`, ensure your system or the submission portal associates it with your ID: `YOUR_ID_HERE`. For the purpose of this exam, simply name the file `test_submission.json`.

**2. File Content:**

*   The content of `test_submission.json` must be a valid JSON object.
*   The JSON object must strictly adhere to the structure specified below. Do not add, remove, or rename any keys.
*   Replace the placeholder values (indicated by `<...>` and comments) with your calculated results or written summary.

**3. Required JSON Structure and Value Formatting:**

```json
{
  ""program_name"": ""Mindfulness Moments"", // This value must be exactly ""Mindfulness Moments"" (String)
  ""analysis_period"": ""Year 1 vs Year 2"", // This value must be exactly ""Year 1 vs Year 2"" (String)
  ""year_1_metrics"": {
    ""total_cost"": <number>, // Replace with the calculated Total Annual Cost for Year 1. Format as a number with exactly two decimal places (e.g., 2730.00).
    ""total_attendance"": <integer>, // Replace with the calculated Total Annual Participant Attendances for Year 1. Format as a whole number (integer) with no decimal places (e.g., 260).
    ""cost_per_attendance"": <number> // Replace with the calculated Average Cost Per Participant Attendance for Year 1. Format as a number with exactly two decimal places (e.g., 10.50).
  },
  ""year_2_metrics"": {
    ""total_cost"": <number>, // Replace with the calculated Total Annual Cost for Year 2. Format as a number with exactly two decimal places (e.g., 3250.00).
    ""total_attendance"": <integer>, // Replace with the calculated Total Annual Participant Attendances for Year 2. Format as a whole number (integer) with no decimal places (e.g., 410).
    ""cost_per_attendance"": <number> // Replace with the calculated Average Cost Per Participant Attendance for Year 2. Format as a number with exactly two decimal places (e.g., 7.93).
  },
  ""year_over_year_change_percent"": {
    ""total_cost_change_percent"": <number>, // Replace with the calculated percentage change in Total Annual Cost. Format as a number with exactly two decimal places. Do not include the '%' sign (e.g., 19.05).
    ""total_attendance_change_percent"": <number>, // Replace with the calculated percentage change in Total Annual Participant Attendances. Format as a number with exactly two decimal places. Do not include the '%' sign (e.g., 57.69).
    ""cost_per_attendance_change_percent"": <number> // Replace with the calculated percentage change in Average Cost Per Participant Attendance. Format as a number with exactly two decimal places. Do not include the '%' sign (e.g., -24.48).
  },
  ""evaluation_summary"": ""<string>"" // Replace with your 1-2 sentence evaluation summary based strictly on the calculated metrics. This must be a JSON string enclosed in double quotes (e.g., ""Costs increased, but attendance increased faster, improving cost-effectiveness."").
}
```

**4. Data Types Summary:**

*   `total_cost`: Number (formatted to 2 decimal places)
*   `total_attendance`: Integer
*   `cost_per_attendance`: Number (formatted to 2 decimal places)
*   `*_change_percent`: Number (formatted to 2 decimal places)
*   `evaluation_summary`: String

**5. Supplementary Files:**

*   No supplementary files (like your spreadsheet calculation file) are required or should be submitted. Only the `test_submission.json` file is needed.

**Important:** Ensure your final `test_submission.json` file is syntactically correct JSON. You can use online JSON validators to check before submitting if needed.

---","Okay, here are the evaluation materials for the Basic Level Practical Exam, designed for the evaluator.

---

## Evaluation Materials for Basic Practical Exam

**Exam Part:** Basic Level - Tracking & Evaluating Cost-Containment
**Candidate Task:** Analyze provided program data, calculate key metrics, and submit results in a structured JSON file.

---

### 1. Answer Key (`test_submission.json`)

This file represents the ideal submission with correctly calculated and formatted values.

```json
{
  ""program_name"": ""Mindfulness Moments"",
  ""analysis_period"": ""Year 1 vs Year 2"",
  ""year_1_metrics"": {
    ""total_cost"": 2730.00,
    ""total_attendance"": 260,
    ""cost_per_attendance"": 10.50
  },
  ""year_2_metrics"": {
    ""total_cost"": 3250.00,
    ""total_attendance"": 410,
    ""cost_per_attendance"": 7.93
  },
  ""year_over_year_change_percent"": {
    ""total_cost_change_percent"": 19.05,
    ""total_attendance_change_percent"": 57.69,
    ""cost_per_attendance_change_percent"": -24.48
  },
  ""evaluation_summary"": ""Program costs increased by 19.05% in Year 2, however, participant attendance grew substantially by 57.69%. This resulted in a 24.48% decrease in the average cost per attendance, indicating improved cost-effectiveness.""
}
```

---

### 2. Explanation of Correct Answers and Derivations

**A. Data Aggregation:**

*   The core challenge is correctly aggregating `Cost_Amount` and `Participants_Attended` from the `Program_Data.xlsx` (or CSV).
*   **Costs:** Sum all `Cost_Amount` values for each respective year.
*   **Attendance:** Sum the unique quarterly attendance figures for each year. The `Participants_Attended` value is repeated for each cost line within a quarter but represents the total attendance for that quarter's sessions. **Do NOT sum all values in the `Participants_Attended` column.**
    *   Year 1 Attendance = Q1(50) + Q2(65) + Q3(70) + Q4(75) = 260
    *   Year 2 Attendance = Q1(80) + Q2(95) + Q3(110) + Q4(125) = 410
    *   *Common Error Check:* If a candidate submits `total_attendance` as 780 for Year 1 (260 * 3) or 1230 for Year 2 (410 * 3), they have incorrectly summed the attendance column without accounting for the repeated quarterly values. This indicates a fundamental misunderstanding of the data structure.

**B. Year 1 Metrics:**

*   **`total_cost`**: Sum of all `Cost_Amount` where `Year` = 1.
    *   (500+50+100) + (500+50+100) + (550+55+110) + (550+55+110) = 650 + 650 + 715 + 715 = **2730.00**
*   **`total_attendance`**: Sum of unique quarterly attendance for Year 1.
    *   50 + 65 + 70 + 75 = **260**
*   **`cost_per_attendance`**: `total_cost` / `total_attendance`
    *   2730.00 / 260 = 10.50. Rounded to two decimal places = **10.50**

**C. Year 2 Metrics:**

*   **`total_cost`**: Sum of all `Cost_Amount` where `Year` = 2.
    *   (600+60+120) + (600+60+120) + (650+65+130) + (650+65+130) = 780 + 780 + 845 + 845 = **3250.00**
*   **`total_attendance`**: Sum of unique quarterly attendance for Year 2.
    *   80 + 95 + 110 + 125 = **410**
*   **`cost_per_attendance`**: `total_cost` / `total_attendance`
    *   3250.00 / 410 = 7.926829... Rounded to two decimal places = **7.93**

**D. Year-over-Year Change Percent:**

*   Formula: `((Year 2 Value - Year 1 Value) / Year 1 Value) * 100`
*   **`total_cost_change_percent`**:
    *   ((3250.00 - 2730.00) / 2730.00) * 100 = (520 / 2730) * 100 = 19.0476...% Rounded to two decimal places = **19.05**
*   **`total_attendance_change_percent`**:
    *   ((410 - 260) / 260) * 100 = (150 / 260) * 100 = 57.6923...% Rounded to two decimal places = **57.69**
*   **`cost_per_attendance_change_percent`**:
    *   ((7.93 - 10.50) / 10.50) * 100 = (-2.57 / 10.50) * 100 = -24.4761...% Rounded to two decimal places = **-24.48**
    *   *Note:* This calculation uses the rounded `cost_per_attendance` values as derived in steps B and C, as implied by the instructions to calculate those first. Using unrounded intermediate values might yield slightly different results (approx. -24.51%), which could be considered acceptable if the candidate's process is otherwise sound and rounding is applied correctly at the end. However, -24.48 is the target based on the instructions.

**E. Evaluation Summary:**

*   Must be 1-2 sentences.
*   Must be based *only* on the calculated metrics (Total Cost, Total Attendance, Cost Per Attendance, and their % changes).
*   Must accurately reflect the trend shown by the data: Costs increased, attendance increased more significantly, leading to improved cost-effectiveness (lower cost per attendance).
*   Should ideally mention the direction of change for cost, attendance, and cost-effectiveness. Mentioning the specific percentage changes strengthens the summary but is not strictly required if the overall trend is correctly described.
*   *Unacceptable:* Summaries that introduce external factors, speculate on reasons *why* changes occurred, make recommendations, or misinterpret the data (e.g., stating cost-effectiveness worsened).

---

### 3. Passing Criteria

This exam assesses fundamental data handling, calculation, and interpretation skills using basic tools.

**A. Scoring Breakdown (Example - Total 10 points):**

*   **JSON Format & Structure (1 point):**
    *   1 point: File is valid JSON, named correctly, and follows the exact required structure (all keys present and correctly named).
    *   0 points: Invalid JSON, incorrect filename, missing/incorrect keys.
*   **Year 1 Metrics (2 points):**
    *   1 point: `total_cost` and `cost_per_attendance` are correct (allowing minor rounding differences if calculation method is sound).
    *   1 point: `total_attendance` is correct (critical check for understanding data aggregation). 0 points if significantly wrong due to misinterpreting aggregation.
*   **Year 2 Metrics (2 points):**
    *   1 point: `total_cost` and `cost_per_attendance` are correct (allowing minor rounding differences).
    *   1 point: `total_attendance` is correct. 0 points if significantly wrong.
*   **Year-over-Year Change % (3 points):**
    *   1 point: `total_cost_change_percent` is correct.
    *   1 point: `total_attendance_change_percent` is correct.
    *   1 point: `cost_per_attendance_change_percent` is correct (check if consistent with their calculated Y1/Y2 CPA values, even if those had minor errors).
*   **Evaluation Summary (2 points):**
    *   1 point: Accurately describes the trend based *only* on their calculated data (even if calculations had minor errors, the interpretation should match *their* numbers).
    *   1 point: Concise (1-2 sentences) and objective (no speculation/recommendations).

**B. Passing Threshold:**

*   **Minimum Score:** **7 out of 10 points**.
*   **Critical Failure Conditions:**
    *   Incorrect `total_attendance` calculation for both years (indicates fundamental inability to interpret the provided data structure). This automatically results in a fail, regardless of other points.
    *   Completely incorrect interpretation in the `evaluation_summary` (e.g., stating cost-effectiveness worsened when their numbers show it improved).
    *   Invalid or incorrectly structured JSON submission (prevents automated checking and shows lack of attention to detail).

**C. Handling Minor Errors:**

*   **Rounding:** Small discrepancies (e.g., +/- 0.01 or 0.02) due to different rounding methods during calculation can be accepted if the core calculation logic appears correct. Significant deviations suggest calculation errors.
*   **Typos:** A minor typo in the `evaluation_summary` that doesn't change the meaning might be overlooked. Numerical typos should be penalized as incorrect calculations.
*   **Carry-forward Errors:** If a Year 1 metric is slightly off, but the Year 2 metric and % change calculations are performed *correctly based on the incorrect Year 1 value*, partial credit should be given for the subsequent calculations demonstrating correct *process*. However, the initial error still loses points. The critical attendance error is an exception, leading to failure.

This framework allows for assessing both the accuracy of the results and the candidate's understanding of the process and data interpretation, while setting a clear bar for basic competency.","```python
import json
import argparse
import sys
import math
import os

# --- Configuration ---
MAX_POINTS_TOTAL = 10
POINTS_STRUCTURE = 1
POINTS_YEAR_1 = 2
POINTS_YEAR_2 = 2
POINTS_CHANGE = 3
POINTS_SUMMARY = 2

# Tolerance for floating point comparisons (accounts for minor rounding differences)
FLOAT_TOLERANCE = 0.015 # Slightly more than 0.01 to allow for intermediate rounding variations

# Expected attendance values for critical failure check
EXPECTED_ATTENDANCE_Y1 = 260
EXPECTED_ATTENDANCE_Y2 = 410
# Significantly wrong attendance values indicating misinterpretation
WRONG_ATTENDANCE_Y1 = 780 # Example: 260 * 3
WRONG_ATTENDANCE_Y2 = 1230 # Example: 410 * 3
ATTENDANCE_TOLERANCE = 5 # Allow slight deviation in case of manual summing error, but catch gross errors

# Keywords for summary evaluation (case-insensitive)
SUMMARY_KEYWORDS_POSITIVE = {
    ""cost"": [""cost increased"", ""costs increased"", ""cost rose"", ""costs rose""],
    ""attendance"": [""attendance increased"", ""attendance grew"", ""attendance rise""],
    ""effectiveness"": [""effectiveness improved"", ""cost per attendance decreased"", ""cost effective"", ""efficiency improved"", ""lower cost per""]
}
SUMMARY_KEYWORDS_NEGATIVE = { # Check if candidate incorrectly states worsening effectiveness
     ""effectiveness_worsened"": [""effectiveness worsened"", ""less effective"", ""cost per attendance increased""]
}

# --- Helper Functions ---

def load_json_file(filepath):
    """"""Loads a JSON file and returns its content as a Python dictionary.""""""
    if not os.path.exists(filepath):
        print(f""Error: File not found at {filepath}"", file=sys.stderr)
        sys.exit(1)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON format in {filepath}. Details: {e}"", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f""Error: Could not read file {filepath}. Details: {e}"", file=sys.stderr)
        sys.exit(1)

def compare_floats(val1, val2, tolerance=FLOAT_TOLERANCE):
    """"""Compares two floats within a given absolute tolerance.""""""
    if val1 is None or val2 is None:
        return False
    try:
        # Use math.isclose for robust comparison
        return math.isclose(float(val1), float(val2), abs_tol=tolerance)
    except (ValueError, TypeError):
        return False # Cannot compare if values are not numbers

def compare_integers(val1, val2):
    """"""Compares two integers exactly.""""""
    if val1 is None or val2 is None:
        return False
    try:
        return int(val1) == int(val2)
    except (ValueError, TypeError):
        return False

def check_attendance_critical_error(candidate_y1_attendance, candidate_y2_attendance):
    """"""Checks for the critical error of grossly miscalculating attendance.""""""
    y1_correct = False
    y2_correct = False
    y1_grossly_wrong = False
    y2_grossly_wrong = False

    try:
        cand_y1 = int(candidate_y1_attendance)
        if abs(cand_y1 - EXPECTED_ATTENDANCE_Y1) <= ATTENDANCE_TOLERANCE:
            y1_correct = True
        elif abs(cand_y1 - WRONG_ATTENDANCE_Y1) <= ATTENDANCE_TOLERANCE * 3: # Wider tolerance for the wrong value
             y1_grossly_wrong = True
    except (ValueError, TypeError, AttributeError):
        pass # Handled by regular scoring

    try:
        cand_y2 = int(candidate_y2_attendance)
        if abs(cand_y2 - EXPECTED_ATTENDANCE_Y2) <= ATTENDANCE_TOLERANCE:
            y2_correct = True
        elif abs(cand_y2 - WRONG_ATTENDANCE_Y2) <= ATTENDANCE_TOLERANCE * 3:
             y2_grossly_wrong = True
    except (ValueError, TypeError, AttributeError):
        pass # Handled by regular scoring

    # Critical failure if *either* year shows the specific gross miscalculation pattern
    if y1_grossly_wrong or y2_grossly_wrong:
        return True, ""Critical Failure: Attendance calculation indicates fundamental misinterpretation of data structure (likely triple counting).""

    # Return False if no critical error, even if numbers are simply incorrect but not the specific wrong pattern
    return False, ""Attendance calculation seems structurally correct or has non-critical errors.""


def evaluate_summary(candidate_summary, key_summary):
    """"""Evaluates the summary based on keywords and objectivity.""""""
    score = 0
    feedback = []
    max_score = POINTS_SUMMARY

    if not isinstance(candidate_summary, str) or not candidate_summary.strip():
        feedback.append(""Summary is missing or not a string."")
        return score, feedback, max_score

    summary_lower = candidate_summary.lower()

    # 1. Check for correct trend identification (improved effectiveness)
    effectiveness_improved = any(kw in summary_lower for kw in SUMMARY_KEYWORDS_POSITIVE[""effectiveness""])
    effectiveness_worsened = any(kw in summary_lower for kw in SUMMARY_KEYWORDS_NEGATIVE[""effectiveness_worsened""])

    if effectiveness_improved and not effectiveness_worsened:
        score += 1
        feedback.append(""Correctly identified improved cost-effectiveness trend."")
    elif effectiveness_worsened:
        feedback.append(""Incorrectly identified cost-effectiveness trend (stated it worsened)."")
    else:
        feedback.append(""Did not clearly state the overall cost-effectiveness trend."")

    # 2. Check for mention of contributing factors (cost & attendance changes) and objectivity
    cost_mentioned = any(kw in summary_lower for kw in SUMMARY_KEYWORDS_POSITIVE[""cost""])
    attendance_mentioned = any(kw in summary_lower for kw in SUMMARY_KEYWORDS_POSITIVE[""attendance""])

    # Basic check for objectivity (avoiding speculative words - very basic check)
    speculative_words = [""should"", ""recommend"", ""could improve"", ""suggest"", ""maybe because""]
    is_objective = not any(word in summary_lower for word in speculative_words)

    if cost_mentioned and attendance_mentioned and is_objective:
        score += 1
        feedback.append(""Correctly mentioned contributing factors (cost/attendance changes) objectively."")
    elif not is_objective:
        feedback.append(""Summary may contain speculation or recommendations, not just objective findings."")
    else:
        missing = []
        if not cost_mentioned: missing.append(""cost changes"")
        if not attendance_mentioned: missing.append(""attendance changes"")
        if missing:
             feedback.append(f""Did not clearly mention contributing factors ({' and '.join(missing)})."")

    # Check conciseness (simple check based on sentence count - approximate)
    num_sentences = summary_lower.count('.') + summary_lower.count('!') + summary_lower.count('?')
    if num_sentences == 0 and len(summary_lower) > 10: # Handle run-on sentences
        num_sentences = 1
    if not (1 <= num_sentences <= 2):
         feedback.append(f""Summary length ({num_sentences} sentences) is outside the recommended 1-2 sentences."")
         # Optional: Penalize slightly? For now, just feedback.

    return score, feedback, max_score

def get_nested_value(data_dict, keys, default=None):
    """"""Safely retrieves a nested value from a dictionary.""""""
    current = data_dict
    try:
        for key in keys:
            current = current[key]
        return current
    except (KeyError, TypeError, IndexError):
        return default

# --- Main Evaluation Logic ---

def evaluate_submission(submission_data, answer_key_data):
    """"""Compares submission data against the answer key and calculates score.""""""
    results = {
        ""overall_score"": 0.0,
        ""max_points_total"": MAX_POINTS_TOTAL,
        ""score_details"": {},
        ""feedback"": [],
        ""critical_failure"": False,
        ""critical_failure_reason"": None
    }
    total_score = 0

    # --- 1. JSON Structure and Basic Keys (Implicitly checked by loading, add explicit check) ---
    structure_score = 0
    structure_feedback = []
    required_top_keys = [""program_name"", ""analysis_period"", ""year_1_metrics"", ""year_2_metrics"", ""year_over_year_change_percent"", ""evaluation_summary""]
    required_y1_keys = [""total_cost"", ""total_attendance"", ""cost_per_attendance""]
    required_y2_keys = [""total_cost"", ""total_attendance"", ""cost_per_attendance""]
    required_change_keys = [""total_cost_change_percent"", ""total_attendance_change_percent"", ""cost_per_attendance_change_percent""]

    missing_keys = []
    if not all(key in submission_data for key in required_top_keys):
        missing_keys.extend([k for k in required_top_keys if k not in submission_data])
    if not all(key in get_nested_value(submission_data, [""year_1_metrics""], {}) for key in required_y1_keys):
         missing_keys.append(""year_1_metrics keys"")
    if not all(key in get_nested_value(submission_data, [""year_2_metrics""], {}) for key in required_y2_keys):
         missing_keys.append(""year_2_metrics keys"")
    if not all(key in get_nested_value(submission_data, [""year_over_year_change_percent""], {}) for key in required_change_keys):
         missing_keys.append(""year_over_year_change_percent keys"")

    if not missing_keys:
        structure_score = POINTS_STRUCTURE
        structure_feedback.append(""JSON structure is valid and all required keys are present."")
    else:
        structure_feedback.append(f""JSON structure is invalid or missing required keys: {', '.join(missing_keys)}."")

    total_score += structure_score
    results[""score_details""][""structure""] = {
        ""score"": structure_score,
        ""max_score"": POINTS_STRUCTURE,
        ""feedback"": structure_feedback
    }

    # --- Critical Failure Check: Attendance ---
    cand_y1_att = get_nested_value(submission_data, [""year_1_metrics"", ""total_attendance""])
    cand_y2_att = get_nested_value(submission_data, [""year_2_metrics"", ""total_attendance""])
    is_critical_failure, critical_reason = check_attendance_critical_error(cand_y1_att, cand_y2_att)

    if is_critical_failure:
        results[""critical_failure""] = True
        results[""critical_failure_reason""] = critical_reason
        results[""feedback""].append(f""CRITICAL FAILURE DETECTED: {critical_reason}. Score capped."")
        # Score is effectively 0 due to critical failure, but we can still evaluate other parts for feedback
        # Set total score to 0 at the end if critical failure is true.

    # --- 2. Year 1 Metrics ---
    y1_score = 0
    y1_feedback = []
    y1_metrics_sub = get_nested_value(submission_data, [""year_1_metrics""], {})
    y1_metrics_key = get_nested_value(answer_key_data, [""year_1_metrics""], {})
    y1_max_score = POINTS_YEAR_1

    # Attendance (1 point of Y1) - check even if critical failure for feedback
    y1_att_sub = get_nested_value(y1_metrics_sub, [""total_attendance""])
    y1_att_key = get_nested_value(y1_metrics_key, [""total_attendance""])
    y1_att_correct = compare_integers(y1_att_sub, y1_att_key)
    if y1_att_correct:
        y1_score += 1
        y1_feedback.append(""Year 1 Total Attendance: Correct."")
    else:
        y1_feedback.append(f""Year 1 Total Attendance: Incorrect (Submitted: {y1_att_sub}, Expected: {y1_att_key})."")
        if is_critical_failure and ""Year 1"" in critical_reason: # Add specific feedback if this caused critical failure
             y1_feedback.append(f""-> This value triggered/contributed to the critical failure: {critical_reason}"")


    # Costs (1 point of Y1)
    y1_cost_sub = get_nested_value(y1_metrics_sub, [""total_cost""])
    y1_cost_key = get_nested_value(y1_metrics_key, [""total_cost""])
    y1_cpa_sub = get_nested_value(y1_metrics_sub, [""cost_per_attendance""])
    y1_cpa_key = get_nested_value(y1_metrics_key, [""cost_per_attendance""])

    y1_cost_correct = compare_floats(y1_cost_sub, y1_cost_key)
    y1_cpa_correct = compare_floats(y1_cpa_sub, y1_cpa_key)

    if y1_cost_correct and y1_cpa_correct:
        y1_score += 1
        y1_feedback.append(""Year 1 Total Cost & Cost Per Attendance: Correct."")
    else:
        if not y1_cost_correct:
            y1_feedback.append(f""Year 1 Total Cost: Incorrect (Submitted: {y1_cost_sub}, Expected: {y1_cost_key})."")
        if not y1_cpa_correct:
            y1_feedback.append(f""Year 1 Cost Per Attendance: Incorrect (Submitted: {y1_cpa_sub}, Expected: {y1_cpa_key})."")

    total_score += y1_score
    results[""score_details""][""year_1_metrics""] = {
        ""score"": y1_score,
        ""max_score"": y1_max_score,
        ""feedback"": y1_feedback
    }

    # --- 3. Year 2 Metrics ---
    y2_score = 0
    y2_feedback = []
    y2_metrics_sub = get_nested_value(submission_data, [""year_2_metrics""], {})
    y2_metrics_key = get_nested_value(answer_key_data, [""year_2_metrics""], {})
    y2_max_score = POINTS_YEAR_2

    # Attendance (1 point of Y2)
    y2_att_sub = get_nested_value(y2_metrics_sub, [""total_attendance""])
    y2_att_key = get_nested_value(y2_metrics_key, [""total_attendance""])
    y2_att_correct = compare_integers(y2_att_sub, y2_att_key)
    if y2_att_correct:
        y2_score += 1
        y2_feedback.append(""Year 2 Total Attendance: Correct."")
    else:
        y2_feedback.append(f""Year 2 Total Attendance: Incorrect (Submitted: {y2_att_sub}, Expected: {y2_att_key})."")
        if is_critical_failure and ""Year 2"" in critical_reason: # Add specific feedback if this caused critical failure
             y2_feedback.append(f""-> This value triggered/contributed to the critical failure: {critical_reason}"")


    # Costs (1 point of Y2)
    y2_cost_sub = get_nested_value(y2_metrics_sub, [""total_cost""])
    y2_cost_key = get_nested_value(y2_metrics_key, [""total_cost""])
    y2_cpa_sub = get_nested_value(y2_metrics_sub, [""cost_per_attendance""])
    y2_cpa_key = get_nested_value(y2_metrics_key, [""cost_per_attendance""])

    y2_cost_correct = compare_floats(y2_cost_sub, y2_cost_key)
    y2_cpa_correct = compare_floats(y2_cpa_sub, y2_cpa_key)

    if y2_cost_correct and y2_cpa_correct:
        y2_score += 1
        y2_feedback.append(""Year 2 Total Cost & Cost Per Attendance: Correct."")
    else:
        if not y2_cost_correct:
            y2_feedback.append(f""Year 2 Total Cost: Incorrect (Submitted: {y2_cost_sub}, Expected: {y2_cost_key})."")
        if not y2_cpa_correct:
            y2_feedback.append(f""Year 2 Cost Per Attendance: Incorrect (Submitted: {y2_cpa_sub}, Expected: {y2_cpa_key})."")

    total_score += y2_score
    results[""score_details""][""year_2_metrics""] = {
        ""score"": y2_score,
        ""max_score"": y2_max_score,
        ""feedback"": y2_feedback
    }

    # --- 4. Year-over-Year Change Percent ---
    change_score = 0
    change_feedback = []
    change_metrics_sub = get_nested_value(submission_data, [""year_over_year_change_percent""], {})
    change_metrics_key = get_nested_value(answer_key_data, [""year_over_year_change_percent""], {})
    change_max_score = POINTS_CHANGE

    fields_to_check = [
        (""total_cost_change_percent"", ""Total Cost Change %""),
        (""total_attendance_change_percent"", ""Total Attendance Change %""),
        (""cost_per_attendance_change_percent"", ""Cost Per Attendance Change %"")
    ]

    for key, name in fields_to_check:
        sub_val = get_nested_value(change_metrics_sub, [key])
        key_val = get_nested_value(change_metrics_key, [key])
        if compare_floats(sub_val, key_val):
            change_score += 1
            change_feedback.append(f""{name}: Correct."")
        else:
            change_feedback.append(f""{name}: Incorrect (Submitted: {sub_val}, Expected: {key_val})."")

    total_score += change_score
    results[""score_details""][""year_over_year_change""] = {
        ""score"": change_score,
        ""max_score"": change_max_score,
        ""feedback"": change_feedback
    }

    # --- 5. Evaluation Summary ---
    summary_sub = get_nested_value(submission_data, [""evaluation_summary""])
    summary_key = get_nested_value(answer_key_data, [""evaluation_summary""]) # Key summary not strictly needed for eval, but good practice
    summary_score, summary_feedback_list, summary_max_score = evaluate_summary(summary_sub, summary_key)

    # Check for critical misinterpretation in summary
    summary_lower = str(summary_sub).lower()
    effectiveness_worsened = any(kw in summary_lower for kw in SUMMARY_KEYWORDS_NEGATIVE[""effectiveness_worsened""])
    if effectiveness_worsened:
         results[""critical_failure""] = True # Also flag critical if summary contradicts data fundamentally
         critical_reason_summary = ""Critical Failure: Evaluation summary incorrectly states cost-effectiveness worsened, contradicting the data.""
         results[""critical_failure_reason""] = critical_reason_summary if not results[""critical_failure_reason""] else results[""critical_failure_reason""] + ""; "" + critical_reason_summary
         results[""feedback""].append(f""CRITICAL FAILURE DETECTED: {critical_reason_summary}. Score capped."")
         summary_score = 0 # Override summary score if critically wrong


    total_score += summary_score
    results[""score_details""][""evaluation_summary""] = {
        ""score"": summary_score,
        ""max_score"": summary_max_score,
        ""feedback"": summary_feedback_list
    }

    # --- Final Score Calculation ---
    if results[""critical_failure""]:
        final_score_points = 0
        results[""feedback""].append(""Overall score set to 0 due to critical failure."")
    else:
        # Ensure score doesn't exceed max points due to potential logic overlaps
        final_score_points = min(total_score, MAX_POINTS_TOTAL)


    results[""final_score_points""] = final_score_points
    results[""overall_score""] = round((final_score_points / MAX_POINTS_TOTAL) * 100, 2) if MAX_POINTS_TOTAL > 0 else 0

    return results

# --- Script Execution ---

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Evaluate Fitness and Wellness Coordinator practical exam submission."")
    parser.add_argument(""submission_file"", help=""Path to the candidate's submission JSON file (e.g., test_submission.json)"")
    parser.add_argument(""answer_key_file"", help=""Path to the answer key JSON file (e.g., answer_key.json)"")
    parser.add_argument(""-o"", ""--output"", default=""test_results.json"", help=""Path to save the evaluation results JSON file (default: test_results.json)"")

    args = parser.parse_args()

    print(f""Loading submission file: {args.submission_file}"")
    submission_data = load_json_file(args.submission_file)

    print(f""Loading answer key file: {args.answer_key_file}"")
    answer_key_data = load_json_file(args.answer_key_file)

    print(""Evaluating submission..."")
    evaluation_results = evaluate_submission(submission_data, answer_key_data)

    # Add input filenames to results for context
    evaluation_results[""input_files""] = {
        ""submission"": os.path.basename(args.submission_file),
        ""answer_key"": os.path.basename(args.answer_key_file)
    }

    output_file = args.output
    print(f""Saving evaluation results to: {output_file}"")
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2)
        print(""Evaluation complete."")
        # Optionally print summary to console
        print(f""\n--- Evaluation Summary ---"")
        print(f""Overall Score: {evaluation_results['overall_score']}% ({evaluation_results['final_score_points']}/{evaluation_results['max_points_total']} points)"")
        if evaluation_results['critical_failure']:
            print(f""Critical Failure Detected: {evaluation_results['critical_failure_reason']}"")
        print(f""Detailed results saved in {output_file}"")
        print(""------------------------"")

    except Exception as e:
        print(f""Error: Could not write results file {output_file}. Details: {e}"", file=sys.stderr)
        sys.exit(1)
```

**Explanation:**

1.  **Imports:** Imports necessary libraries: `json` for handling JSON data, `argparse` for command-line arguments, `sys` for exiting on errors, `math` for `isclose`, and `os` for path manipulation.
2.  **Configuration:** Defines constants for maximum points, points per section, float comparison tolerance, expected/wrong attendance values, and keywords for summary evaluation. This makes the script easier to adjust.
3.  **`load_json_file(filepath)`:** Safely loads a JSON file. It includes error handling for `FileNotFoundError` and `json.JSONDecodeError`, printing informative messages and exiting if errors occur.
4.  **`compare_floats(val1, val2, tolerance)`:** Compares two floats using `math.isclose` for tolerance-based comparison, handling potential non-numeric inputs gracefully.
5.  **`compare_integers(val1, val2)`:** Compares two integers for exact equality, handling non-numeric inputs.
6.  **`check_attendance_critical_error(...)`:** Implements the specific check for the gross attendance miscalculation described in the evaluation criteria. Returns `True` and a reason if the critical error pattern is detected.
7.  **`evaluate_summary(candidate_summary, key_summary)`:** Attempts to automatically score the summary based on:
    *   Presence of keywords indicating correct trend identification (effectiveness improved).
    *   Presence of keywords indicating mention of contributing factors (cost/attendance changes).
    *   Basic check for objectivity (absence of speculative words).
    *   Checks for incorrect trend identification (effectiveness worsened) which can trigger a critical failure.
    *   Provides feedback messages.
8.  **`get_nested_value(data_dict, keys, default=None)`:** A utility function to safely access potentially missing nested keys in the submission dictionary without causing `KeyError`.
9.  **`evaluate_submission(submission_data, answer_key_data)`:**
    *   Initializes a `results` dictionary to store scores and feedback.
    *   **Structure Check:** Verifies if all required keys are present in the submission. Awards points accordingly.
    *   **Critical Failure Check:** Calls `check_attendance_critical_error`. If it returns `True`, sets the `critical_failure` flag.
    *   **Metric Evaluation (Y1, Y2, Change):** Iterates through the metrics, using `get_nested_value` to safely retrieve values from both submission and key. Uses `compare_integers` and `compare_floats` for comparison. Adds points and feedback to the `results` dictionary. Adds specific feedback if an attendance error contributed to a critical failure.
    *   **Summary Evaluation:** Calls `evaluate_summary`. Checks if the summary critically misinterprets the data (e.g., says effectiveness worsened) and flags critical failure if necessary.
    *   **Final Score:** Calculates the total points. If a critical failure was flagged (either by attendance or summary misinterpretation), the final score is set to 0. Otherwise, it calculates the percentage score.
10. **`if __name__ == ""__main__"":` block:**
    *   Sets up `argparse` to accept the submission file and answer key file paths as command-line arguments, with an optional output file path.
    *   Calls `load_json_file` for both inputs.
    *   Calls `evaluate_submission` to perform the evaluation.
    *   Adds input filenames to the results dictionary for context.
    *   Writes the resulting `evaluation_results` dictionary to the specified output JSON file (`test_results.json` by default) using `json.dump` with indentation for readability.
    *   Prints confirmation messages and a summary to the console.

**To Use the Script:**

1.  Save the code above as `task_evaluation.py`.
2.  Make sure you have the candidate's submission file (e.g., `test_submission.json`) and the correct answer key file (you'll need to create this based on the `<evaluation_information>`, let's call it `answer_key.json`) in the same directory or provide the correct paths.
3.  Create the `answer_key.json` file with the content from the `<evaluation_information>` section:
    ```json
    {
      ""program_name"": ""Mindfulness Moments"",
      ""analysis_period"": ""Year 1 vs Year 2"",
      ""year_1_metrics"": {
        ""total_cost"": 2730.00,
        ""total_attendance"": 260,
        ""cost_per_attendance"": 10.50
      },
      ""year_2_metrics"": {
        ""total_cost"": 3250.00,
        ""total_attendance"": 410,
        ""cost_per_attendance"": 7.93
      },
      ""year_over_year_change_percent"": {
        ""total_cost_change_percent"": 19.05,
        ""total_attendance_change_percent"": 57.69,
        ""cost_per_attendance_change_percent"": -24.48
      },
      ""evaluation_summary"": ""Program costs increased by 19.05% in Year 2, however, participant attendance grew substantially by 57.69%. This resulted in a 24.48% decrease in the average cost per attendance, indicating improved cost-effectiveness.""
    }
    ```
4.  Run the script from your terminal:
    ```bash
    python task_evaluation.py test_submission.json answer_key.json
    ```
    (Replace `test_submission.json` with the actual name of the candidate's file if different).
5.  A `test_results.json` file will be created in the same directory, containing the detailed scoring breakdown and the `overall_score`.","{'program_name': 'Mindfulness Moments', 'analysis_period': 'Year 1 vs Year 2', 'year_1_metrics': {'total_cost': 2730.0, 'total_attendance': 260, 'cost_per_attendance': 10.5}, 'year_2_metrics': {'total_cost': 3250.0, 'total_attendance': 410, 'cost_per_attendance': 7.93}, 'year_over_year_change_percent': {'total_cost_change_percent': 19.05, 'total_attendance_change_percent': 57.69, 'cost_per_attendance_change_percent': -24.48}, 'evaluation_summary': 'Program costs increased by 19.05% in Year 2, however, participant attendance grew substantially by 57.69%. This resulted in a 24.48% decrease in the average cost per attendance, indicating improved cost-effectiveness.'}","[[None, 'loop of ufunc does not support argument 0 of type NoneType which has no callable rint method'], ['Traceback (most recent call last):\n  File ""/Users/htr365/Documents/PhD/21_automatisation/gpt_eval/data/exam_approach/test_results/gemini-2.5-pro-preview-03-25/20316_0/task_evaluation.py"", line 397, in <module>\n    main()\n  File ""/Users/htr365/Documents/PhD/21_automatisation/gpt_eval/data/exam_approach/test_results/gemini-2.5-pro-preview-03-25/20316_0/task_evaluation.py"", line 385, in main\n    evaluation_results = evaluate_submission(submission_data, key_data)\n  File ""/Users/htr365/Documents/PhD/21_automatisation/gpt_eval/data/exam_approach/test_results/gemini-2.5-pro-preview-03-25/20316_0/task_evaluation.py"", line 329, in evaluate_submission\n    if math.isclose(sub_cpa_change, key_cpa_change, abs_tol=tolerance * 2): # Allow slightly wider tolerance here\nNameError: name \'tolerance\' is not defined\n']]",True,True,0.0,99.0,90.0,4.0,True,,"{'overview': prompt_token_count: 243
candidates_token_count: 1807
total_token_count: 3627
, 'instructions': prompt_token_count: 2256
candidates_token_count: 1032
total_token_count: 4080
, 'materials': prompt_token_count: 3493
candidates_token_count: 1986
total_token_count: 7008
, 'check_materials': prompt_token_count: 3122
candidates_token_count: 1
total_token_count: 3563
, 'check_website': prompt_token_count: 3155
candidates_token_count: 1
total_token_count: 3693
, 'submission': prompt_token_count: 5263
candidates_token_count: 1005
total_token_count: 6587
, 'evaluation': prompt_token_count: 6189
candidates_token_count: 2440
total_token_count: 9323
, 'grading': prompt_token_count: 8818
candidates_token_count: 6940
total_token_count: 17975
}",management_occupations
Gambling Managers,7199.0,"Review operational expenses, budget estimates, betting accounts, or collection reports for accuracy.",gemini-2.5-pro-preview-03-25,,,,,,,,,,,,,"['Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT']",,,,,,,,,,management_occupations
Clinical Research Coordinators,15603.0,"Identify protocol problems, inform investigators of problems, or assist in problem resolution efforts, such as protocol revisions.",gemini-2.5-pro-preview-03-25,"['Text editor', 'PDF viewer']","['Text', 'Data']","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

**`Instructions.txt`**

**Clinical Research Coordinator - Practical Exam (Basic Level)**

**Objective:**
This exam assesses your practical ability to review clinical trial documents, identify potential issues or deviations related to the protocol, and determine the appropriate immediate next steps for communication and resolution.

**Time Limit:** 90 minutes

**Available Tools:**
*   Text Editor (e.g., Notepad, TextEdit)
*   PDF Viewer (e.g., Adobe Reader, Preview)

**Provided Materials:**
You have been provided with the following digital files:
1.  `Protocol_Excerpt_BASIC.pdf`: An excerpt from a fictional clinical trial protocol.
2.  `Scenarios_BASIC.txt`: A file containing short scenarios related to conducting the trial described in the protocol excerpt.

**Your Tasks:**

1.  **Protocol Review:**
    *   Carefully read the `Protocol_Excerpt_BASIC.pdf` file.
    *   Pay specific attention to the 'Inclusion Criteria', 'Exclusion Criteria', and 'Schedule of Assessments' sections.
    *   Identify any potential problems found *directly within these sections* of the protocol text. Look for requirements that are unclear, contradictory, missing necessary details for execution, or pose potential feasibility challenges for a site.

2.  **Scenario Analysis:**
    *   Read each scenario described in the `Scenarios_BASIC.txt` file.
    *   For each scenario, compare the situation described to the requirements outlined in the `Protocol_Excerpt_BASIC.pdf`.
    *   Identify any potential protocol deviations, challenges in applying the protocol, or situations described in the scenarios that require clarification or action based on the protocol.

3.  **Document Findings in JSON Format:**
    *   You must document **each distinct problem** you identify (whether from the Protocol Review or Scenario Analysis) as a separate entry in a structured JSON file.
    *   Create this file using your text editor and save it as `test_submission.json`.
    *   The file must contain a single JSON object. This object must have one key named `""identified_issues""`. The value associated with this key must be a JSON list (an array `[...]`) containing all the individual issue objects you document.
    *   Each issue object within the `""identified_issues""` list must contain the following key-value pairs, using exact key names as specified:
        *   `issue_id`: (String) Assign a unique ID to each issue you identify. Use the format ""ISSUE_01"", ""ISSUE_02"", ""ISSUE_03"", etc., incrementing the number for each distinct problem.
        *   `source_description`: (String) State briefly where the issue was identified. Examples: ""Protocol Section 4.1 Inclusion Criteria"", ""Scenario 1"", ""Schedule of Assessments Table"".
        *   `protocol_reference`: (String) Cite the specific protocol section number(s) and/or table numbers most relevant to the identified issue. Examples: ""Section 4.1.3"", ""Section 6.2 Table 1"", ""Section 4.2.1 and 6.2"". If the issue relates to a scenario without a single direct protocol reference, cite the most relevant section(s) or state ""Scenario Context"".
        *   `problem_summary`: (String) Describe the specific problem concisely and factually. State *what* is unclear, contradictory, missing, infeasible, or deviated according to the protocol/scenario.
        *   `potential_impact`: (String) Briefly state the most direct potential consequence of the problem. Examples: ""Inconsistent subject screening"", ""Risk of protocol deviation"", ""Incomplete data collection"", ""Subject safety risk"".
        *   `recommended_next_step`: (String) State the single, most appropriate *immediate* action you would take as a CRC. Focus on communication and initiating resolution. Examples: ""Inform PI and request clarification"", ""Notify PI of potential deviation"", ""Flag for discussion with PI"", ""Document issue for query to sponsor"".
        *   `communication_point_person`: (String) Identify the primary individual at the site you would inform first about this specific issue. Example: ""Principal Investigator"".

**JSON Output Example:**

```json
{
  ""identified_issues"": [
    {
      ""issue_id"": ""ISSUE_01"",
      ""source_description"": ""Protocol Section 4.2 Exclusion Criteria"",
      ""protocol_reference"": ""Section 4.2.5"",
      ""problem_summary"": ""Exclusion criterion #5 term 'active infection' is not defined (e.g., type, severity)."",
      ""potential_impact"": ""Inconsistent subject exclusion."",
      ""recommended_next_step"": ""Inform PI and request clarification on definition."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_02"",
      ""source_description"": ""Scenario 2"",
      ""protocol_reference"": ""Section 6.2 Table 1"",
      ""problem_summary"": ""Scenario 2 participant cannot attend Visit 3 within the protocol-specified +/- 3 day window."",
      ""potential_impact"": ""Risk of protocol deviation."",
      ""recommended_next_step"": ""Notify PI immediately regarding scheduling conflict."",
      ""communication_point_person"": ""Principal Investigator""
    }
  ]
}
```

**Submission:**
*   Ensure your final output is saved as `test_submission.json`.
*   Verify the file contains valid JSON structure before submitting.
*   Follow the separate instructions provided on how to upload or send your completed `test_submission.json` file.

**Begin the exam when you are ready. You have 90 minutes.**
**1. Content for `Protocol_Excerpt_BASIC.pdf`**

```text
**Fictional Study Protocol - PRO-123 - Excerpt for Basic CRC Exam**

**Protocol Title:** A Phase III, Single-Arm, Open-Label Study to Evaluate the Efficacy and Safety of Drug X in Subjects with Mild Hypertension

**Protocol Number:** PRO-123

**Version:** 2.0

**Date:** 15 January 2024

**(Page 1)**

**1.0 SYNOPSIS**

*   **Study Phase:** Phase III
*   **Design:** Single-arm, open-label
*   **Primary Objective:** To evaluate the efficacy of Drug X in reducing systolic blood pressure (SBP) after 8 weeks of treatment in subjects with mild hypertension.
*   **Study Population:** Adults aged 18-65 with mild hypertension.
*   **Treatment:** Drug X 50mg administered orally once daily for 8 weeks.
*   **Duration:** Approximately 10 weeks per subject (up to 2 weeks screening, 8 weeks treatment).

**(Page 2)**

**4.0 STUDY POPULATION**

**4.1 Inclusion Criteria**
Subjects must meet all of the following criteria to be eligible for enrollment:

4.1.1 Subject is male or female, aged 18 to 65 years, inclusive, at the time of screening.
4.1.2 Subject has a diagnosis of mild hypertension, defined as an average Systolic Blood Pressure (SBP) between 140-159 mmHg OR an average Diastolic Blood Pressure (DBP) between 90-99 mmHg. The average will be calculated from 3 readings taken at least 5 minutes apart during the Screening visit.
4.1.3 Subject must have a prior history of treatment for hypertension.
4.1.4 Subject is willing and able to provide written informed consent prior to any study-related procedures.
4.1.5 Subject is considered able and likely to comply with study procedures and attend all scheduled clinic visits, in the Investigator's opinion.

**4.2 Exclusion Criteria**
Subjects meeting any of the following criteria are not eligible for enrollment:

4.2.1 History of myocardial infarction, stroke, or coronary revascularization within the 12 months prior to screening.
4.2.2 Use of any prescription or over-the-counter anti-hypertensive medication within 14 days prior to the Screening visit (Visit 0).
4.2.3 Known hypersensitivity or allergy to Drug X or any of its excipients.
4.2.4 Participation in any other investigational drug or device trial within 30 days prior to screening.
4.2.5 Presence of significant renal impairment, defined as an estimated Glomerular Filtration Rate (eGFR) < 45 mL/min/1.73m².
4.2.6 Subject is pregnant, planning pregnancy during the study period, or is breastfeeding. Female subjects of childbearing potential must agree to use a highly effective method of contraception throughout the study.

**(Page 3)**

**6.0 STUDY ASSESSMENTS AND PROCEDURES**

**6.1 Overview**
Study procedures will include collection of demographics, medical history, vital signs (blood pressure, heart rate), 12-lead Electrocardiograms (ECGs), collection of blood samples for safety laboratory tests and pharmacokinetics (PK), dispensing of study drug (Drug X), review of patient diaries, and assessment of adverse events (AEs).

**6.2 Schedule of Assessments (SoA)**

| Assessment                  | Screening (Day -14 to -1) | Visit 1 (Day 1) | Visit 2 (Day 14 +/- 2 days) | Visit 3 (Day 28 +/- 3 days) | Visit 4 / EOT (Day 56 +/- 5 days) |
| :-------------------------- | :-----------------------: | :-------------: | :-------------------------: | :-------------------------: | :-------------------------------: |
| Informed Consent            |             X             |                 |                             |                             |                                   |
| Demographics                |             X             |                 |                             |                             |                                   |
| Medical History             |             X             |                 |                             |                             |                                   |
| Inclusion/Exclusion Check   |             X             |        X        |                             |                             |                                   |
| Vital Signs (BP, HR)        |             X             |        X        |              X              |              X              |                 X                 |
| 12-Lead ECG                 |             X             |                 |                             |              X              |                 X                 |
| Blood Sample - Safety Labs¹ |             X             |                 |              X              |                             |                 X                 |
| Blood Sample - PK²          |                           |        X        |              X              |              X              |                                   |
| Study Drug Dispensing       |                           |        X        |              X              |              X              |                                   |
| Patient Diary Review        |                           |                 |              X              |              X              |                 X                 |
| Adverse Event Review        |             X             |        X        |              X              |              X              |                 X                 |

**Footnotes for SoA Table:**
¹ Safety Labs at Visit 2 must be collected in a fasting state (no caloric intake for at least 8 hours prior).
² PK sample at Visit 2 should be collected approximately 2 hours after the subject takes their daily dose of Drug X on the day of the visit.

**(End of Excerpt)**
```

---

**2. Content for `Scenarios_BASIC.txt`**

```text
**Scenarios for Basic CRC Exam**

**Scenario 1:**
A potential subject is undergoing screening (Day -5). Their average screening BP is 145/92 mmHg. During the medical history review, they mention they took an over-the-counter cold medication yesterday morning because they had a stuffy nose. The medication box (which they brought in) lists 'pseudoephedrine HCl 30mg' as an active ingredient. Exclusion criterion 4.2.2 prohibits ""any prescription or over-the-counter anti-hypertensive medication within 14 days"". You need to determine the next step regarding this subject's eligibility screening process.

**Scenario 2:**
Patient 002 completed Visit 2 yesterday, which was Day 15 of their study participation. Today (Day 16), they call your site coordinator line. They sound distressed because they just realized they forgot to fill out their paper patient diary for the last three days (Day 13, 14, and 15). The protocol's Schedule of Assessments (Section 6.2) indicates 'Patient Diary Review' occurs at Visit 2, Visit 3, and Visit 4. The next scheduled review is at Visit 3 (Day 28 +/- 3 days). What immediate action(s) should you take in response to this phone call?

**Scenario 3:**
You are assisting with Visit 3 procedures for Patient 003 on Day 27. As per the Schedule of Assessments (Section 6.2), an ECG is required. The certified technician performs the ECG, and the machine automatically prints the tracing with an interpretation: ""Sinus Rhythm. Borderline QT interval. Clinical correlation recommended."" The patient states they feel perfectly normal. The protocol (in a section not provided in this excerpt) generally requires reporting of 'clinically significant' findings to the Principal Investigator (PI). What is your most appropriate immediate next step?
```

Okay, here is the specification for the candidate's answer format for the Basic Level CRC Practical Exam.

---

**Candidate Answer Format Specification**

**1. Submission File:**

*   You must submit a single file named exactly `test_submission.json`.

**2. File Content:**

*   The content of `test_submission.json` must be a single, valid JSON object.
*   This JSON object must contain two top-level keys:
    *   `candidate_id`: (String) Your unique identifier. Use the model version provided to you (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet"", etc.). Replace ""YOUR_ID_HERE"" with your actual assigned ID.
    *   `identified_issues`: (List/Array) This key holds a list (an array `[...]`) of JSON objects, where each object represents a distinct problem you identified.

**3. Structure of Each Issue Object:**

*   Each object within the `identified_issues` list must contain the following seven key-value pairs, using the exact key names specified below. All values must be strings.

    *   `issue_id`: (String)
        *   **Format:** Use the specific format ""ISSUE_XX"", where XX is a two-digit number starting from ""01"" and incrementing for each distinct issue identified (e.g., ""ISSUE_01"", ""ISSUE_02"", ""ISSUE_03"", ...).
    *   `source_description`: (String)
        *   **Format:** A brief text description indicating where the issue was found. Examples: ""Protocol Section 4.1 Inclusion Criteria"", ""Scenario 1"", ""Schedule of Assessments Table Footnote 1"".
    *   `protocol_reference`: (String)
        *   **Format:** Cite the specific protocol section(s) and/or table number(s) most relevant. Use formats like ""Section 4.1.3"", ""Section 6.2 Table 1"", ""Section 4.2.1 and 6.2"". If primarily related to a scenario without a single direct protocol section, use ""Scenario Context"" or cite the most relevant section(s) involved in the scenario analysis. Use ""N/A"" only if absolutely no protocol section is relevant (unlikely for this exam).
    *   `problem_summary`: (String)
        *   **Format:** Clear, concise text describing the specific problem (ambiguity, contradiction, missing information, feasibility concern, deviation, etc.). Focus on *what* the problem is. No specific length limit, but be concise.
    *   `potential_impact`: (String)
        *   **Format:** Brief text describing the most likely direct consequence. Examples: ""Inconsistent subject screening"", ""Risk of protocol deviation"", ""Incomplete data collection"", ""Subject safety risk"", ""Difficulty in procedure execution"".
    *   `recommended_next_step`: (String)
        *   **Format:** Text describing the single, most appropriate *immediate* action for a CRC. Focus on communication and initiating resolution. Examples: ""Inform PI and request clarification"", ""Notify PI of potential deviation"", ""Flag for discussion with PI"", ""Document issue for query to sponsor"".
    *   `communication_point_person`: (String)
        *   **Format:** Text identifying the primary individual *at the site* you would inform first. Typically this will be ""Principal Investigator"", but could be another role if appropriate for the specific issue context (though PI is usually the correct first point for protocol issues).

**4. Data Types and Conventions:**

*   All values associated with the keys inside the issue objects must be **Strings**. Enclose all text in double quotes (`""`).
*   Use standard English text.
*   There are no specific requirements for decimal places or units as the answers are descriptive text.
*   Ensure your final JSON is well-formed. You can use an online validator to check syntax if needed.

**5. Example JSON Structure Template:**

```json
{
  ""candidate_id"": ""YOUR_ID_HERE"",
  ""identified_issues"": [
    {
      ""issue_id"": ""ISSUE_01"",
      ""source_description"": ""Example: Protocol Section X.Y"",
      ""protocol_reference"": ""Example: Section X.Y.Z"",
      ""problem_summary"": ""Example: Describe the problem clearly and concisely here."",
      ""potential_impact"": ""Example: Describe the potential consequence here."",
      ""recommended_next_step"": ""Example: State the immediate next action here."",
      ""communication_point_person"": ""Example: Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_02"",
      ""source_description"": ""Example: Scenario X"",
      ""protocol_reference"": ""Example: Section A.B / Scenario Context"",
      ""problem_summary"": ""Example: Describe the problem identified from the scenario analysis."",
      ""potential_impact"": ""Example: Describe the potential consequence."",
      ""recommended_next_step"": ""Example: State the immediate next action."",
      ""communication_point_person"": ""Example: Principal Investigator""
    }
    // Add more issue objects here as needed, incrementing the issue_id
  ]
}
```

**6. Supplementary Files:**

*   No supplementary files are required. Submit only the `test_submission.json` file.

---","You are an excellent examiner of Clinical Research Coordinators capabilities. Design a remote, **practical** exam to verify whether a Clinical Research Coordinators can Identify protocol problems, inform investigators of problems, or assist in problem resolution efforts, such as protocol revisions..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the design for the **Basic Level Practical Exam** for Clinical Research Coordinators, focusing on identifying protocol problems, informing investigators, and assisting in problem resolution efforts.

---

## CRC Practical Exam: Basic Level - Protocol Problem Identification & Reporting

**Exam Purpose and Structure (For Evaluator):**

*   **Purpose:** This exam assesses a candidate's fundamental ability to perform core CRC tasks related to protocol adherence and problem-solving in a practical setting. Specifically, it tests their capability to:
    1.  Read and understand a clinical trial protocol excerpt.
    2.  Identify potential problems within the protocol itself (e.g., ambiguities, contradictions, feasibility issues).
    3.  Identify deviations or challenges when applying the protocol to specific scenarios.
    4.  Articulate these problems clearly.
    5.  Determine the immediate next steps, focusing on informing the appropriate parties (like the Principal Investigator - PI) and suggesting the *need* for clarification or revision.
*   **Structure:** The exam consists of two main tasks presented to the candidate after they receive the instructions and materials.
    1.  **Protocol Review:** The candidate reviews specific sections of a provided fictional protocol excerpt (PDF) to identify inherent issues.
    2.  **Scenario Analysis:** The candidate analyzes short, hypothetical scenarios (provided as text) in the context of the same protocol excerpt to identify conflicts, deviations, or necessary actions.
*   **Materials Provided to Candidate:**
    1.  `Instructions.txt`: Explains the tasks, time limit, tools, and required JSON output format.
    2.  `Protocol_Excerpt_BASIC.pdf`: A short (approx. 5-7 pages) fictional protocol excerpt containing deliberately introduced basic-level issues (e.g., slightly ambiguous inclusion/exclusion criteria, conflicting visit window information, minor feasibility concerns in procedures). Key sections like Synopsis, Inclusion/Exclusion, and Schedule of Assessments will be included.
    3.  `Scenarios_BASIC.txt`: Contains 2-3 brief scenarios describing situations encountered at a clinical site (e.g., a potential subject borderline on criteria, a scheduling conflict, a minor procedural difficulty reported by staff).
*   **Candidate Output:** A single JSON file named `test_submission.json` containing structured information about the identified issues and proposed actions.
*   **Assessment Focus:** Evaluation will focus on the candidate's ability to accurately spot issues, reference the protocol correctly, describe the problem clearly, assess basic impact, and propose appropriate initial actions (primarily communication/escalation), all formatted correctly within the JSON structure. This is *not* a test of deep regulatory knowledge or complex amendment drafting at this basic level.
*   **Time Limit:** 90 minutes.

---

**Candidate Instructions (`Instructions.txt` - Content):**

```text
**Clinical Research Coordinator - Practical Exam (Basic Level)**

**Objective:**
This exam assesses your ability to identify potential problems related to a clinical trial protocol, determine appropriate next steps, and document your findings clearly. You will review a protocol excerpt and analyze specific scenarios.

**Time Limit:** 90 minutes

**Available Tools:**
*   Text Editor (e.g., Notepad, TextEdit, VS Code)
*   PDF Viewer (e.g., Adobe Reader, Preview)

**Provided Materials:**
1.  `Protocol_Excerpt_BASIC.pdf`: An excerpt from a fictional clinical trial protocol.
2.  `Scenarios_BASIC.txt`: A file containing short scenarios related to the protocol.

**Tasks:**

1.  **Protocol Review:**
    *   Carefully read the provided `Protocol_Excerpt_BASIC.pdf`.
    *   Focus specifically on the 'Inclusion/Exclusion Criteria' and 'Schedule of Assessments' sections.
    *   Identify any potential problems such as ambiguities, contradictions, missing information needed for execution, or potential feasibility issues you notice directly within these sections.

2.  **Scenario Analysis:**
    *   Read the scenarios described in `Scenarios_BASIC.txt`.
    *   For each scenario, analyze it against the information provided in `Protocol_Excerpt_BASIC.pdf`.
    *   Identify any protocol deviations, challenges, or situations requiring clarification or action based on the scenario and the protocol.

3.  **Document Findings:**
    *   For **each distinct problem** you identify (from Task 1 or Task 2), create an entry in a JSON structure.
    *   Use your text editor to create a single JSON file named `test_submission.json`.
    *   The JSON file must contain a single key, ""identified_issues"", which holds a list (an array) of issue objects.
    *   Each issue object in the list must have the following key-value pairs:
        *   `issue_id`: (String) A unique identifier you create for the issue (e.g., ""PROBLEM_01"", ""SCENARIO_A_ISSUE"").
        *   `source_description`: (String) Briefly describe where you found the issue (e.g., ""Protocol Section 4.1 Inclusion Criteria"", ""Scenario 1 Analysis"", ""Schedule of Assessments Table"").
        *   `protocol_reference`: (String) Cite the specific protocol section number(s) relevant to the issue (e.g., ""Section 4.1.3"", ""Section 6.2 Table 1"", ""Section 4.2.1 and 6.2""). If not directly tied to a section (e.g., general feasibility), state ""N/A"" or describe context.
        *   `problem_summary`: (String) Clearly and concisely describe the problem you identified. What is the ambiguity, contradiction, deviation, or challenge?
        *   `potential_impact`: (String) Briefly state the potential consequence (e.g., ""Difficulty screening subjects"", ""Risk of protocol deviation"", ""Inconsistent data collection"", ""Potential safety concern"", ""Unclear procedure"").
        *   `recommended_next_step`: (String) State the immediate practical action you would take. Focus on communication and problem resolution initiation. (e.g., ""Discuss with PI for clarification"", ""Notify PI and request guidance"", ""Flag for discussion with Monitor/Sponsor"", ""Suggest clarification needed in protocol/CRF"").
        *   `communication_point_person`: (String) Identify the primary person you would inform first (e.g., ""Principal Investigator"", ""Study Nurse"", ""Data Manager"").

**Example JSON Structure:**

```json
{
  ""identified_issues"": [
    {
      ""issue_id"": ""EXAMPLE_01"",
      ""source_description"": ""Protocol Section 4.2 Exclusion Criteria"",
      ""protocol_reference"": ""Section 4.2.5"",
      ""problem_summary"": ""Exclusion criterion #5 regarding 'active infection' is vague. It doesn't specify types or severity."",
      ""potential_impact"": ""Inconsistent subject exclusion across team members."",
      ""recommended_next_step"": ""Discuss with PI to establish a clearer site-level definition or query Sponsor for clarification."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""EXAMPLE_02"",
      ""source_description"": ""Scenario 2 Analysis"",
      ""protocol_reference"": ""Section 6.2 Table 1 - Visit 3 Window"",
      ""problem_summary"": ""Scenario 2 participant cannot make Visit 3 within the +/- 3 day window specified in the protocol due to travel."",
      ""potential_impact"": ""Protocol deviation regarding visit schedule."",
      ""recommended_next_step"": ""Notify PI immediately to determine if a deviation is permissible or if the subject needs to be discontinued."",
      ""communication_point_person"": ""Principal Investigator""
    }
    // ... Add more issue objects here for each problem you identify
  ]
}

```

**Submission:**
*   Save your completed work as `test_submission.json`.
*   Ensure the file is valid JSON format. You can use an online JSON validator if unsure, but basic text editor structure should suffice.
*   Follow the instructions provided separately on how to submit this file.

**Good luck!**
```

---
**Fictional Content Snippets (Illustrative - To be fully developed for actual exam):**

*   **`Protocol_Excerpt_BASIC.pdf` (Key areas with potential issues):**
    *   *Inclusion Criterion #4:* ""Subject must have a documented history of Condition X within the last 5 years."" (Problem: Ambiguity - Does 'documented history' mean diagnosis, active symptoms, or treatment? What kind of documentation?)
    *   *Exclusion Criterion #2:* ""Subject has clinically significant cardiovascular disease."" (Problem: Ambiguity - 'Clinically significant' is subjective without further definition).
    *   *Schedule of Assessments Table:* Shows blood draw required at Visit 2 (Day 7 +/- 1 day) and Visit 3 (Day 14 +/- 3 days). A footnote says ""Fasting required for Visit 3 blood draw"". Another section describes Visit 2 procedures taking ~3 hours. (Problem: Potential feasibility issue if Visit 2 runs long and overlaps with a potential Visit 3 window start; potential conflict if Visit 3 falls on a weekend - is lab open for fasting draw?).
    *   *Procedure Description Section 7.1:* ""Administer study drug IPX-101 orally with water."" (Problem: Missing detail - How much water? Before/after food specified anywhere?).

*   **`Scenarios_BASIC.txt`:**
    *   *Scenario 1:* ""A potential participant reports having 'mild asthma' controlled with an inhaler used only 'occasionally'. Exclusion criterion #6 states 'Severe respiratory illness'. You need to decide if they can be screened further."" (Problem: Tests understanding of applying potentially ambiguous criteria and need for PI input).
    *   *Scenario 2:* ""The Schedule of Assessments requires a specific patient questionnaire to be completed at Visit 4. The study nurse reports that patients are consistently confused by Question #5 on this questionnaire and are asking for clarification, which isn't standardized."" (Problem: Identifies a practical issue impacting data quality/consistency, suggesting need for clarification or potential CRF/protocol update).
    *   *Scenario 3:* ""Visit 5 is scheduled for Day 30 (+/- 5 days). Today is Day 28 for patient 001. The PI informs you they will be unexpectedly out of office from Day 31 to Day 36. The protocol requires PI assessment at Visit 5."" (Problem: Scheduling conflict impacting protocol requirement, needs immediate PI discussion for delegation or rescheduling within window).

---","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

**`Instructions.txt`**

**Clinical Research Coordinator - Practical Exam (Basic Level)**

**Objective:**
This exam assesses your practical ability to review clinical trial documents, identify potential issues or deviations related to the protocol, and determine the appropriate immediate next steps for communication and resolution.

**Time Limit:** 90 minutes

**Available Tools:**
*   Text Editor (e.g., Notepad, TextEdit)
*   PDF Viewer (e.g., Adobe Reader, Preview)

**Provided Materials:**
You have been provided with the following digital files:
1.  `Protocol_Excerpt_BASIC.pdf`: An excerpt from a fictional clinical trial protocol.
2.  `Scenarios_BASIC.txt`: A file containing short scenarios related to conducting the trial described in the protocol excerpt.

**Your Tasks:**

1.  **Protocol Review:**
    *   Carefully read the `Protocol_Excerpt_BASIC.pdf` file.
    *   Pay specific attention to the 'Inclusion Criteria', 'Exclusion Criteria', and 'Schedule of Assessments' sections.
    *   Identify any potential problems found *directly within these sections* of the protocol text. Look for requirements that are unclear, contradictory, missing necessary details for execution, or pose potential feasibility challenges for a site.

2.  **Scenario Analysis:**
    *   Read each scenario described in the `Scenarios_BASIC.txt` file.
    *   For each scenario, compare the situation described to the requirements outlined in the `Protocol_Excerpt_BASIC.pdf`.
    *   Identify any potential protocol deviations, challenges in applying the protocol, or situations described in the scenarios that require clarification or action based on the protocol.

3.  **Document Findings in JSON Format:**
    *   You must document **each distinct problem** you identify (whether from the Protocol Review or Scenario Analysis) as a separate entry in a structured JSON file.
    *   Create this file using your text editor and save it as `test_submission.json`.
    *   The file must contain a single JSON object. This object must have one key named `""identified_issues""`. The value associated with this key must be a JSON list (an array `[...]`) containing all the individual issue objects you document.
    *   Each issue object within the `""identified_issues""` list must contain the following key-value pairs, using exact key names as specified:
        *   `issue_id`: (String) Assign a unique ID to each issue you identify. Use the format ""ISSUE_01"", ""ISSUE_02"", ""ISSUE_03"", etc., incrementing the number for each distinct problem.
        *   `source_description`: (String) State briefly where the issue was identified. Examples: ""Protocol Section 4.1 Inclusion Criteria"", ""Scenario 1"", ""Schedule of Assessments Table"".
        *   `protocol_reference`: (String) Cite the specific protocol section number(s) and/or table numbers most relevant to the identified issue. Examples: ""Section 4.1.3"", ""Section 6.2 Table 1"", ""Section 4.2.1 and 6.2"". If the issue relates to a scenario without a single direct protocol reference, cite the most relevant section(s) or state ""Scenario Context"".
        *   `problem_summary`: (String) Describe the specific problem concisely and factually. State *what* is unclear, contradictory, missing, infeasible, or deviated according to the protocol/scenario.
        *   `potential_impact`: (String) Briefly state the most direct potential consequence of the problem. Examples: ""Inconsistent subject screening"", ""Risk of protocol deviation"", ""Incomplete data collection"", ""Subject safety risk"".
        *   `recommended_next_step`: (String) State the single, most appropriate *immediate* action you would take as a CRC. Focus on communication and initiating resolution. Examples: ""Inform PI and request clarification"", ""Notify PI of potential deviation"", ""Flag for discussion with PI"", ""Document issue for query to sponsor"".
        *   `communication_point_person`: (String) Identify the primary individual at the site you would inform first about this specific issue. Example: ""Principal Investigator"".

**JSON Output Example:**

```json
{
  ""identified_issues"": [
    {
      ""issue_id"": ""ISSUE_01"",
      ""source_description"": ""Protocol Section 4.2 Exclusion Criteria"",
      ""protocol_reference"": ""Section 4.2.5"",
      ""problem_summary"": ""Exclusion criterion #5 term 'active infection' is not defined (e.g., type, severity)."",
      ""potential_impact"": ""Inconsistent subject exclusion."",
      ""recommended_next_step"": ""Inform PI and request clarification on definition."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_02"",
      ""source_description"": ""Scenario 2"",
      ""protocol_reference"": ""Section 6.2 Table 1"",
      ""problem_summary"": ""Scenario 2 participant cannot attend Visit 3 within the protocol-specified +/- 3 day window."",
      ""potential_impact"": ""Risk of protocol deviation."",
      ""recommended_next_step"": ""Notify PI immediately regarding scheduling conflict."",
      ""communication_point_person"": ""Principal Investigator""
    }
  ]
}
```

**Submission:**
*   Ensure your final output is saved as `test_submission.json`.
*   Verify the file contains valid JSON structure before submitting.
*   Follow the separate instructions provided on how to upload or send your completed `test_submission.json` file.

**Begin the exam when you are ready. You have 90 minutes.**","Okay, here are the synthetic test materials and the evaluator explanation for the Basic Level CRC Practical Exam.

<MATERIALS_FOR_CANDIDATE>
**1. Content for `Protocol_Excerpt_BASIC.pdf`**

```text
**Fictional Study Protocol - PRO-123 - Excerpt for Basic CRC Exam**

**Protocol Title:** A Phase III, Single-Arm, Open-Label Study to Evaluate the Efficacy and Safety of Drug X in Subjects with Mild Hypertension

**Protocol Number:** PRO-123

**Version:** 2.0

**Date:** 15 January 2024

**(Page 1)**

**1.0 SYNOPSIS**

*   **Study Phase:** Phase III
*   **Design:** Single-arm, open-label
*   **Primary Objective:** To evaluate the efficacy of Drug X in reducing systolic blood pressure (SBP) after 8 weeks of treatment in subjects with mild hypertension.
*   **Study Population:** Adults aged 18-65 with mild hypertension.
*   **Treatment:** Drug X 50mg administered orally once daily for 8 weeks.
*   **Duration:** Approximately 10 weeks per subject (up to 2 weeks screening, 8 weeks treatment).

**(Page 2)**

**4.0 STUDY POPULATION**

**4.1 Inclusion Criteria**
Subjects must meet all of the following criteria to be eligible for enrollment:

4.1.1 Subject is male or female, aged 18 to 65 years, inclusive, at the time of screening.
4.1.2 Subject has a diagnosis of mild hypertension, defined as an average Systolic Blood Pressure (SBP) between 140-159 mmHg OR an average Diastolic Blood Pressure (DBP) between 90-99 mmHg. The average will be calculated from 3 readings taken at least 5 minutes apart during the Screening visit.
4.1.3 Subject must have a prior history of treatment for hypertension.
4.1.4 Subject is willing and able to provide written informed consent prior to any study-related procedures.
4.1.5 Subject is considered able and likely to comply with study procedures and attend all scheduled clinic visits, in the Investigator's opinion.

**4.2 Exclusion Criteria**
Subjects meeting any of the following criteria are not eligible for enrollment:

4.2.1 History of myocardial infarction, stroke, or coronary revascularization within the 12 months prior to screening.
4.2.2 Use of any prescription or over-the-counter anti-hypertensive medication within 14 days prior to the Screening visit (Visit 0).
4.2.3 Known hypersensitivity or allergy to Drug X or any of its excipients.
4.2.4 Participation in any other investigational drug or device trial within 30 days prior to screening.
4.2.5 Presence of significant renal impairment, defined as an estimated Glomerular Filtration Rate (eGFR) < 45 mL/min/1.73m².
4.2.6 Subject is pregnant, planning pregnancy during the study period, or is breastfeeding. Female subjects of childbearing potential must agree to use a highly effective method of contraception throughout the study.

**(Page 3)**

**6.0 STUDY ASSESSMENTS AND PROCEDURES**

**6.1 Overview**
Study procedures will include collection of demographics, medical history, vital signs (blood pressure, heart rate), 12-lead Electrocardiograms (ECGs), collection of blood samples for safety laboratory tests and pharmacokinetics (PK), dispensing of study drug (Drug X), review of patient diaries, and assessment of adverse events (AEs).

**6.2 Schedule of Assessments (SoA)**

| Assessment                  | Screening (Day -14 to -1) | Visit 1 (Day 1) | Visit 2 (Day 14 +/- 2 days) | Visit 3 (Day 28 +/- 3 days) | Visit 4 / EOT (Day 56 +/- 5 days) |
| :-------------------------- | :-----------------------: | :-------------: | :-------------------------: | :-------------------------: | :-------------------------------: |
| Informed Consent            |             X             |                 |                             |                             |                                   |
| Demographics                |             X             |                 |                             |                             |                                   |
| Medical History             |             X             |                 |                             |                             |                                   |
| Inclusion/Exclusion Check   |             X             |        X        |                             |                             |                                   |
| Vital Signs (BP, HR)        |             X             |        X        |              X              |              X              |                 X                 |
| 12-Lead ECG                 |             X             |                 |                             |              X              |                 X                 |
| Blood Sample - Safety Labs¹ |             X             |                 |              X              |                             |                 X                 |
| Blood Sample - PK²          |                           |        X        |              X              |              X              |                                   |
| Study Drug Dispensing       |                           |        X        |              X              |              X              |                                   |
| Patient Diary Review        |                           |                 |              X              |              X              |                 X                 |
| Adverse Event Review        |             X             |        X        |              X              |              X              |                 X                 |

**Footnotes for SoA Table:**
¹ Safety Labs at Visit 2 must be collected in a fasting state (no caloric intake for at least 8 hours prior).
² PK sample at Visit 2 should be collected approximately 2 hours after the subject takes their daily dose of Drug X on the day of the visit.

**(End of Excerpt)**
```

---

**2. Content for `Scenarios_BASIC.txt`**

```text
**Scenarios for Basic CRC Exam**

**Scenario 1:**
A potential subject is undergoing screening (Day -5). Their average screening BP is 145/92 mmHg. During the medical history review, they mention they took an over-the-counter cold medication yesterday morning because they had a stuffy nose. The medication box (which they brought in) lists 'pseudoephedrine HCl 30mg' as an active ingredient. Exclusion criterion 4.2.2 prohibits ""any prescription or over-the-counter anti-hypertensive medication within 14 days"". You need to determine the next step regarding this subject's eligibility screening process.

**Scenario 2:**
Patient 002 completed Visit 2 yesterday, which was Day 15 of their study participation. Today (Day 16), they call your site coordinator line. They sound distressed because they just realized they forgot to fill out their paper patient diary for the last three days (Day 13, 14, and 15). The protocol's Schedule of Assessments (Section 6.2) indicates 'Patient Diary Review' occurs at Visit 2, Visit 3, and Visit 4. The next scheduled review is at Visit 3 (Day 28 +/- 3 days). What immediate action(s) should you take in response to this phone call?

**Scenario 3:**
You are assisting with Visit 3 procedures for Patient 003 on Day 27. As per the Schedule of Assessments (Section 6.2), an ECG is required. The certified technician performs the ECG, and the machine automatically prints the tracing with an interpretation: ""Sinus Rhythm. Borderline QT interval. Clinical correlation recommended."" The patient states they feel perfectly normal. The protocol (in a section not provided in this excerpt) generally requires reporting of 'clinically significant' findings to the Principal Investigator (PI). What is your most appropriate immediate next step?
```

</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. How Materials Were Created:**

*   **`Protocol_Excerpt_BASIC.pdf` Content:**
    *   A fictional, simplified protocol excerpt was created focusing on sections relevant to common CRC tasks: eligibility screening (Inclusion/Exclusion) and visit conduct (Schedule of Assessments).
    *   The study context (mild hypertension, Phase III, single drug) is simple and common.
    *   **Deliberate Issues Introduced:**
        *   **Issue 1 (Ambiguity):** Inclusion Criterion 4.1.3 (""prior history of treatment for hypertension"") is intentionally vague. It lacks specifics on type, duration, timing, or documentation required, making consistent application difficult without clarification.
        *   **Issue 2 (Potential Conflict/Ambiguity):** Exclusion Criterion 4.2.5 (""significant renal impairment, defined as eGFR < 45"") uses the potentially subjective term ""significant"" alongside a numerical cutoff. While the number provides definition, the word ""significant"" might cause confusion, especially if a value is exactly 45 or if clinical context suggests impairment despite being slightly above the cutoff. The primary issue is the potential ambiguity caused by the wording alongside the definition. *Focus should be on the definition provided (eGFR < 45), but the wording is slightly imperfect.*
        *   **Issue 3 (Contradiction/Feasibility):** The SoA footnotes for Visit 2 create a direct conflict. Footnote 1 requires a fasting sample for Safety Labs, while Footnote 2 requires a PK sample 2 hours post-dose. These are mutually exclusive conditions for samples collected at the *same visit* (Visit 2). This is a clear logistical impossibility as written.
        *   **Issue 4 (Ambiguity/Missing Info):** The SoA lists 'Patient Diary Review' but provides no context in the excerpt about the diary's purpose, content, completion requirements, or the specifics of the review process. This lack of detail makes it hard to assess compliance or data quality effectively.
*   **`Scenarios_BASIC.txt` Content:**
    *   Three short scenarios were created to simulate realistic site-level situations that require the CRC to apply the protocol excerpt, identify a problem/query, and determine an immediate action.
    *   **Scenario 1:** Tests application of Exclusion Criterion 4.2.2. Pseudoephedrine is not technically an ""anti-hypertensive"" but *can* affect blood pressure. This ambiguity requires CRC to recognize the potential issue (confounding factor for BP assessment, potential violation of the *spirit* of the exclusion if not the letter) and escalate to the PI for a decision. It tests judgment beyond literal interpretation.
    *   **Scenario 2:** Directly relates to the missing information about the Patient Diary (Issue 4 from protocol). It presents a practical problem (missing data) and tests the CRC's understanding of data integrity and timely action. The correct action involves immediate guidance/data recovery attempts, not waiting until the next scheduled review.
    *   **Scenario 3:** Presents a potential safety signal (borderline ECG finding). It tests the CRC's ability to recognize the need for PI notification even if the finding is ""borderline"" and the patient is asymptomatic, especially given the (implied) protocol requirement to report ""clinically significant"" findings. The key is immediate escalation for medical interpretation by the PI.

**2. Cross-References and Connections:**

*   Scenario 1 directly requires evaluation against Protocol Section 4.2.2.
*   Scenario 2 highlights the practical implication of the lack of detail regarding the 'Patient Diary Review' mentioned in Protocol Section 6.2 (SoA Table).
*   Scenario 3 requires action based on an assessment listed in Protocol Section 6.2 (ECG at Visit 3) and general (implied) protocol principles regarding safety reporting/PI notification.
*   Protocol Issues 1, 2, 3, and 4 are identifiable directly from reading Sections 4.1, 4.2, and 6.2 of the excerpt.

**3. Tricky Elements and Common Pitfalls:**

*   **Protocol Issue 1 (Prior History):** Candidates might just note it's vague but fail to articulate *why* it's a problem (inconsistent screening).
*   **Protocol Issue 2 (eGFR):** Candidates might overthink the ""significant"" wording or miss that a definition *is* provided (< 45). The core issue is the slight ambiguity introduced by the wording choice, though the numerical definition should guide action. A candidate might incorrectly flag anyone *at* 45 as excluded.
*   **Protocol Issue 3 (Fasting/Post-Dose):** This should be straightforward, but a candidate might miss it if not reading footnotes carefully or might propose an overly complex workaround instead of identifying it as a protocol problem needing clarification/revision.
*   **Protocol Issue 4 (Diary):** Candidates might identify it's mentioned but not elaborate on the *impact* of the missing detail (data quality, compliance tracking).
*   **Scenario 1 (Pseudoephedrine):** Candidates might incorrectly decide eligibility themselves (either include or exclude) instead of recognizing the need for PI judgment due to the ambiguity of whether this specific OTC med falls under the exclusion's intent. They might also miss the potential impact on the screening BP reading itself.
*   **Scenario 2 (Diary):** Candidates might incorrectly suggest waiting until Visit 3 for the review, missing the importance of addressing potential data loss immediately.
*   **Scenario 3 (ECG):** Candidates might dismiss the ""borderline"" finding or wait for the technician to interpret clinical significance, rather than recognizing the CRC's role is to report the *finding* (including the machine flag) to the PI immediately for *their* assessment of significance.

**4. ""Hidden"" Information:**

*   The main ""hidden"" aspect is the *implication* of the identified issues. For example, the vague ""prior history"" (Issue 1) implies potential for inconsistent enrollment. The conflicting V2 sample times (Issue 3) implies visits cannot be conducted as per protocol without clarification/revision. The missing diary details (Issue 4) imply potential for poor data quality, highlighted by Scenario 2.
*   Scenario 1 requires understanding that medications not strictly in the excluded class might still be relevant and require PI input.
*   Scenario 3 requires understanding the CRC role in escalating potential safety signals, even if labelled ""borderline"", to the physician investigator.

**5. Evaluation Guidance:**

*   Focus on whether the candidate correctly identifies the core problem in each case (ambiguity, contradiction, feasibility, deviation risk, data issue, safety signal).
*   Check if the `protocol_reference` is accurate.
*   Assess if the `problem_summary` is clear and concise.
*   Evaluate if the `potential_impact` is logical (e.g., screening issues, deviations, data quality, safety).
*   Critically assess the `recommended_next_step` and `communication_point_person`. For this basic level, the answer should almost always involve informing/discussing with the PI as the immediate step. Suggesting complex solutions like protocol amendments themselves is beyond the scope; the focus is on *identifying* the need and *initiating* resolution via the PI.
*   Ensure the JSON format is correct as per instructions. Minor syntax errors might be noted but shouldn't fail the candidate if the content is good, depending on scoring rubric.

</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
**1. Content for `Protocol_Excerpt_BASIC.pdf`**

```text
**Fictional Study Protocol - PRO-123 - Excerpt for Basic CRC Exam**

**Protocol Title:** A Phase III, Single-Arm, Open-Label Study to Evaluate the Efficacy and Safety of Drug X in Subjects with Mild Hypertension

**Protocol Number:** PRO-123

**Version:** 2.0

**Date:** 15 January 2024

**(Page 1)**

**1.0 SYNOPSIS**

*   **Study Phase:** Phase III
*   **Design:** Single-arm, open-label
*   **Primary Objective:** To evaluate the efficacy of Drug X in reducing systolic blood pressure (SBP) after 8 weeks of treatment in subjects with mild hypertension.
*   **Study Population:** Adults aged 18-65 with mild hypertension.
*   **Treatment:** Drug X 50mg administered orally once daily for 8 weeks.
*   **Duration:** Approximately 10 weeks per subject (up to 2 weeks screening, 8 weeks treatment).

**(Page 2)**

**4.0 STUDY POPULATION**

**4.1 Inclusion Criteria**
Subjects must meet all of the following criteria to be eligible for enrollment:

4.1.1 Subject is male or female, aged 18 to 65 years, inclusive, at the time of screening.
4.1.2 Subject has a diagnosis of mild hypertension, defined as an average Systolic Blood Pressure (SBP) between 140-159 mmHg OR an average Diastolic Blood Pressure (DBP) between 90-99 mmHg. The average will be calculated from 3 readings taken at least 5 minutes apart during the Screening visit.
4.1.3 Subject must have a prior history of treatment for hypertension.
4.1.4 Subject is willing and able to provide written informed consent prior to any study-related procedures.
4.1.5 Subject is considered able and likely to comply with study procedures and attend all scheduled clinic visits, in the Investigator's opinion.

**4.2 Exclusion Criteria**
Subjects meeting any of the following criteria are not eligible for enrollment:

4.2.1 History of myocardial infarction, stroke, or coronary revascularization within the 12 months prior to screening.
4.2.2 Use of any prescription or over-the-counter anti-hypertensive medication within 14 days prior to the Screening visit (Visit 0).
4.2.3 Known hypersensitivity or allergy to Drug X or any of its excipients.
4.2.4 Participation in any other investigational drug or device trial within 30 days prior to screening.
4.2.5 Presence of significant renal impairment, defined as an estimated Glomerular Filtration Rate (eGFR) < 45 mL/min/1.73m².
4.2.6 Subject is pregnant, planning pregnancy during the study period, or is breastfeeding. Female subjects of childbearing potential must agree to use a highly effective method of contraception throughout the study.

**(Page 3)**

**6.0 STUDY ASSESSMENTS AND PROCEDURES**

**6.1 Overview**
Study procedures will include collection of demographics, medical history, vital signs (blood pressure, heart rate), 12-lead Electrocardiograms (ECGs), collection of blood samples for safety laboratory tests and pharmacokinetics (PK), dispensing of study drug (Drug X), review of patient diaries, and assessment of adverse events (AEs).

**6.2 Schedule of Assessments (SoA)**

| Assessment                  | Screening (Day -14 to -1) | Visit 1 (Day 1) | Visit 2 (Day 14 +/- 2 days) | Visit 3 (Day 28 +/- 3 days) | Visit 4 / EOT (Day 56 +/- 5 days) |
| :-------------------------- | :-----------------------: | :-------------: | :-------------------------: | :-------------------------: | :-------------------------------: |
| Informed Consent            |             X             |                 |                             |                             |                                   |
| Demographics                |             X             |                 |                             |                             |                                   |
| Medical History             |             X             |                 |                             |                             |                                   |
| Inclusion/Exclusion Check   |             X             |        X        |                             |                             |                                   |
| Vital Signs (BP, HR)        |             X             |        X        |              X              |              X              |                 X                 |
| 12-Lead ECG                 |             X             |                 |                             |              X              |                 X                 |
| Blood Sample - Safety Labs¹ |             X             |                 |              X              |                             |                 X                 |
| Blood Sample - PK²          |                           |        X        |              X              |              X              |                                   |
| Study Drug Dispensing       |                           |        X        |              X              |              X              |                                   |
| Patient Diary Review        |                           |                 |              X              |              X              |                 X                 |
| Adverse Event Review        |             X             |        X        |              X              |              X              |                 X                 |

**Footnotes for SoA Table:**
¹ Safety Labs at Visit 2 must be collected in a fasting state (no caloric intake for at least 8 hours prior).
² PK sample at Visit 2 should be collected approximately 2 hours after the subject takes their daily dose of Drug X on the day of the visit.

**(End of Excerpt)**
```

---

**2. Content for `Scenarios_BASIC.txt`**

```text
**Scenarios for Basic CRC Exam**

**Scenario 1:**
A potential subject is undergoing screening (Day -5). Their average screening BP is 145/92 mmHg. During the medical history review, they mention they took an over-the-counter cold medication yesterday morning because they had a stuffy nose. The medication box (which they brought in) lists 'pseudoephedrine HCl 30mg' as an active ingredient. Exclusion criterion 4.2.2 prohibits ""any prescription or over-the-counter anti-hypertensive medication within 14 days"". You need to determine the next step regarding this subject's eligibility screening process.

**Scenario 2:**
Patient 002 completed Visit 2 yesterday, which was Day 15 of their study participation. Today (Day 16), they call your site coordinator line. They sound distressed because they just realized they forgot to fill out their paper patient diary for the last three days (Day 13, 14, and 15). The protocol's Schedule of Assessments (Section 6.2) indicates 'Patient Diary Review' occurs at Visit 2, Visit 3, and Visit 4. The next scheduled review is at Visit 3 (Day 28 +/- 3 days). What immediate action(s) should you take in response to this phone call?

**Scenario 3:**
You are assisting with Visit 3 procedures for Patient 003 on Day 27. As per the Schedule of Assessments (Section 6.2), an ECG is required. The certified technician performs the ECG, and the machine automatically prints the tracing with an interpretation: ""Sinus Rhythm. Borderline QT interval. Clinical correlation recommended."" The patient states they feel perfectly normal. The protocol (in a section not provided in this excerpt) generally requires reporting of 'clinically significant' findings to the Principal Investigator (PI). What is your most appropriate immediate next step?
```

","Okay, here is the specification for the candidate's answer format for the Basic Level CRC Practical Exam.

---

**Candidate Answer Format Specification**

**1. Submission File:**

*   You must submit a single file named exactly `test_submission.json`.

**2. File Content:**

*   The content of `test_submission.json` must be a single, valid JSON object.
*   This JSON object must contain two top-level keys:
    *   `candidate_id`: (String) Your unique identifier. Use the model version provided to you (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet"", etc.). Replace ""YOUR_ID_HERE"" with your actual assigned ID.
    *   `identified_issues`: (List/Array) This key holds a list (an array `[...]`) of JSON objects, where each object represents a distinct problem you identified.

**3. Structure of Each Issue Object:**

*   Each object within the `identified_issues` list must contain the following seven key-value pairs, using the exact key names specified below. All values must be strings.

    *   `issue_id`: (String)
        *   **Format:** Use the specific format ""ISSUE_XX"", where XX is a two-digit number starting from ""01"" and incrementing for each distinct issue identified (e.g., ""ISSUE_01"", ""ISSUE_02"", ""ISSUE_03"", ...).
    *   `source_description`: (String)
        *   **Format:** A brief text description indicating where the issue was found. Examples: ""Protocol Section 4.1 Inclusion Criteria"", ""Scenario 1"", ""Schedule of Assessments Table Footnote 1"".
    *   `protocol_reference`: (String)
        *   **Format:** Cite the specific protocol section(s) and/or table number(s) most relevant. Use formats like ""Section 4.1.3"", ""Section 6.2 Table 1"", ""Section 4.2.1 and 6.2"". If primarily related to a scenario without a single direct protocol section, use ""Scenario Context"" or cite the most relevant section(s) involved in the scenario analysis. Use ""N/A"" only if absolutely no protocol section is relevant (unlikely for this exam).
    *   `problem_summary`: (String)
        *   **Format:** Clear, concise text describing the specific problem (ambiguity, contradiction, missing information, feasibility concern, deviation, etc.). Focus on *what* the problem is. No specific length limit, but be concise.
    *   `potential_impact`: (String)
        *   **Format:** Brief text describing the most likely direct consequence. Examples: ""Inconsistent subject screening"", ""Risk of protocol deviation"", ""Incomplete data collection"", ""Subject safety risk"", ""Difficulty in procedure execution"".
    *   `recommended_next_step`: (String)
        *   **Format:** Text describing the single, most appropriate *immediate* action for a CRC. Focus on communication and initiating resolution. Examples: ""Inform PI and request clarification"", ""Notify PI of potential deviation"", ""Flag for discussion with PI"", ""Document issue for query to sponsor"".
    *   `communication_point_person`: (String)
        *   **Format:** Text identifying the primary individual *at the site* you would inform first. Typically this will be ""Principal Investigator"", but could be another role if appropriate for the specific issue context (though PI is usually the correct first point for protocol issues).

**4. Data Types and Conventions:**

*   All values associated with the keys inside the issue objects must be **Strings**. Enclose all text in double quotes (`""`).
*   Use standard English text.
*   There are no specific requirements for decimal places or units as the answers are descriptive text.
*   Ensure your final JSON is well-formed. You can use an online validator to check syntax if needed.

**5. Example JSON Structure Template:**

```json
{
  ""candidate_id"": ""YOUR_ID_HERE"",
  ""identified_issues"": [
    {
      ""issue_id"": ""ISSUE_01"",
      ""source_description"": ""Example: Protocol Section X.Y"",
      ""protocol_reference"": ""Example: Section X.Y.Z"",
      ""problem_summary"": ""Example: Describe the problem clearly and concisely here."",
      ""potential_impact"": ""Example: Describe the potential consequence here."",
      ""recommended_next_step"": ""Example: State the immediate next action here."",
      ""communication_point_person"": ""Example: Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_02"",
      ""source_description"": ""Example: Scenario X"",
      ""protocol_reference"": ""Example: Section A.B / Scenario Context"",
      ""problem_summary"": ""Example: Describe the problem identified from the scenario analysis."",
      ""potential_impact"": ""Example: Describe the potential consequence."",
      ""recommended_next_step"": ""Example: State the immediate next action."",
      ""communication_point_person"": ""Example: Principal Investigator""
    }
    // Add more issue objects here as needed, incrementing the issue_id
  ]
}
```

**6. Supplementary Files:**

*   No supplementary files are required. Submit only the `test_submission.json` file.

---","Okay, here are the evaluation materials for the Basic Level CRC Practical Exam, including the answer key, explanations, and passing criteria.

---

## Evaluation Materials for Basic CRC Practical Exam

**1. Complete Answer Key (`answer_key.json`)**

```json
{
  ""candidate_id"": ""EVALUATOR_KEY"",
  ""identified_issues"": [
    {
      ""issue_id"": ""ISSUE_01"",
      ""source_description"": ""Protocol Section 4.1 Inclusion Criteria"",
      ""protocol_reference"": ""Section 4.1.3"",
      ""problem_summary"": ""Inclusion criterion 4.1.3 'prior history of treatment for hypertension' is ambiguous. It lacks specifics on type, duration, timing, or required documentation of the prior treatment."",
      ""potential_impact"": ""Inconsistent subject screening and enrollment; difficulty verifying eligibility."",
      ""recommended_next_step"": ""Inform PI and request clarification or query sponsor for a more specific definition."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_02"",
      ""source_description"": ""Schedule of Assessments Table Footnotes"",
      ""protocol_reference"": ""Section 6.2 Table 1 Footnotes 1 and 2"",
      ""problem_summary"": ""There is a conflict for Visit 2 procedures. Footnote 1 requires Safety Labs in a fasting state, while Footnote 2 requires the PK sample ~2 hours post-dose. These are contradictory requirements for the same visit."",
      ""potential_impact"": ""Inability to perform Visit 2 procedures as written; risk of protocol deviation; inconsistent sample collection."",
      ""recommended_next_step"": ""Notify PI of the protocol contradiction and request clarification from the sponsor."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_03"",
      ""source_description"": ""Schedule of Assessments Table"",
      ""protocol_reference"": ""Section 6.2 Table 1"",
      ""problem_summary"": ""The SoA lists 'Patient Diary Review' but the protocol excerpt provides no details on the diary's content, completion frequency, specific review process, or handling of missing entries."",
      ""potential_impact"": ""Inconsistent data collection; difficulty assessing compliance; unclear process for handling issues like missing data."",
      ""recommended_next_step"": ""Flag the lack of detail for discussion with PI/monitor; request clarification or check study manuals (if available)."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_04"",
      ""source_description"": ""Scenario 1"",
      ""protocol_reference"": ""Section 4.2.2"",
      ""problem_summary"": ""Potential subject took pseudoephedrine. While not strictly an 'anti-hypertensive' (Exclusion 4.2.2), it can affect blood pressure. Eligibility is unclear due to potential confounding effect and ambiguity of exclusion criteria scope."",
      ""potential_impact"": ""Incorrect subject enrollment/exclusion; confounding factor for baseline BP assessment; potential protocol deviation if enrolled incorrectly."",
      ""recommended_next_step"": ""Inform PI about the medication use and request their assessment of eligibility before proceeding."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_05"",
      ""source_description"": ""Scenario 2"",
      ""protocol_reference"": ""Section 6.2 Table 1 / Scenario Context"",
      ""problem_summary"": ""Patient forgot to complete the patient diary for 3 days prior to Visit 2 review. This represents missing source data."",
      ""potential_impact"": ""Incomplete or inaccurate data collection; potential protocol deviation regarding data completion."",
      ""recommended_next_step"": ""Instruct patient on proper procedure for documenting late/missed entries (if guidance exists) or document the missing data appropriately; inform PI of the issue."",
      ""communication_point_person"": ""Principal Investigator""
    },
    {
      ""issue_id"": ""ISSUE_06"",
      ""source_description"": ""Scenario 3"",
      ""protocol_reference"": ""Section 6.2 Table 1 / Scenario Context"",
      ""problem_summary"": ""ECG at Visit 3 shows 'Borderline QT interval' flagged by the machine. Although patient is asymptomatic, this is a potential safety signal requiring medical assessment."",
      ""potential_impact"": ""Potential subject safety risk if not assessed; delay in identifying clinically significant change."",
      ""recommended_next_step"": ""Immediately notify the Principal Investigator of the ECG finding for their review and assessment of clinical significance."",
      ""communication_point_person"": ""Principal Investigator""
    }
  ]
}
```

**2. Explanation of Correct Answers and Derivations**

*   **ISSUE_01 (Protocol Ambiguity - Inclusion 4.1.3):**
    *   **Derivation:** Reading Section 4.1.3 reveals the term ""prior history of treatment for hypertension"" lacks specific definition (What kind of treatment? How long ago? How documented?).
    *   **Explanation:** This ambiguity makes it difficult for different staff members or sites to apply the criterion consistently, potentially leading to incorrect enrollments or exclusions. The correct CRC action is to identify this lack of clarity and escalate it to the PI for interpretation or to seek clarification from the sponsor.
*   **ISSUE_02 (Protocol Contradiction - SoA Footnotes):**
    *   **Derivation:** Comparing Footnote 1 (fasting for safety labs at V2) and Footnote 2 (PK sample ~2 hours post-dose at V2) in Section 6.2 shows they impose mutually exclusive conditions for sample collection at the same visit.
    *   **Explanation:** The protocol, as written, presents a logistical impossibility for Visit 2. A CRC must recognize this direct contradiction and report it to the PI so that clarification can be sought from the sponsor, likely requiring a protocol clarification or amendment.
*   **ISSUE_03 (Protocol Missing Information - SoA Diary Review):**
    *   **Derivation:** The SoA table in Section 6.2 lists ""Patient Diary Review"" as an assessment, but the provided excerpt gives no further details about the diary itself or the review process.
    *   **Explanation:** Without knowing what's in the diary, how often it should be filled, or what the ""review"" entails, the CRC cannot effectively manage this aspect of data collection or assess compliance. This lack of detail is a practical problem that should be flagged for clarification (often found in study manuals, but based *only* on the excerpt, it's missing). Scenario 2 highlights the practical impact of this missing detail.
*   **ISSUE_04 (Scenario 1 - Eligibility Ambiguity):**
    *   **Derivation:** Scenario 1 describes a subject taking pseudoephedrine. Protocol Section 4.2.2 excludes ""anti-hypertensive medication"". Pseudoephedrine is a decongestant but known to potentially increase blood pressure.
    *   **Explanation:** The core issue is the ambiguity: does this OTC medication fall under the *intent* of the exclusion criteria, even if not strictly classified as ""anti-hypertensive""? It also poses a risk of confounding the baseline BP readings. The CRC should not make the eligibility decision alone but must present the situation to the PI for medical judgment.
*   **ISSUE_05 (Scenario 2 - Data Integrity Issue):**
    *   **Derivation:** Scenario 2 describes a patient admitting to missing 3 days of diary entries reviewed at Visit 2 (per Section 6.2 SoA).
    *   **Explanation:** This is a direct report of missing source data. The CRC's immediate responsibility is data integrity. They should guide the patient on how to handle late/missed entries (if procedures exist, e.g., fill in now with notation, leave blank) and document the event. Waiting until the next scheduled review (Visit 3) is inappropriate as it delays addressing data quality. The PI should be informed. This links back to the lack of detail identified in ISSUE_03.
*   **ISSUE_06 (Scenario 3 - Potential Safety Signal):**
    *   **Derivation:** Scenario 3 describes an ECG machine flagging a ""Borderline QT interval"" during a required assessment (Visit 3, Section 6.2 SoA). The protocol generally requires reporting ""clinically significant"" findings.
    *   **Explanation:** Even though the finding is ""borderline"" and the patient is asymptomatic, any flagged potential abnormality, especially related to cardiac function (QT interval), requires immediate medical assessment by the PI to determine clinical significance. The CRC's role is not to interpret the ECG but to ensure the PI is promptly notified of the finding for review.

**3. Passing Criteria**

*   **Minimum Correct Issues:** The candidate must correctly identify and document **at least 4 out of the 6** primary issues listed in the answer key.
*   **Definition of ""Correctly Identified"":**
    *   The `issue_id` must follow the ""ISSUE_XX"" format.
    *   The `source_description` and `protocol_reference` must reasonably point to the correct location of the issue (minor variations acceptable if the location is clear).
    *   The `problem_summary` must capture the core essence of the problem (ambiguity, contradiction, missing info, deviation risk, data issue, safety signal). Wording does not need to match the key exactly but must convey the correct meaning.
    *   The `potential_impact` must be a logical consequence of the identified problem.
    *   The `recommended_next_step` must be appropriate for a basic CRC role, focusing on **immediate communication and escalation**, typically involving the PI. Proposing complex solutions (e.g., ""draft protocol amendment"") is incorrect; suggesting ""query sponsor"" *after* informing PI is acceptable.
    *   The `communication_point_person` should correctly identify the primary site contact, which is the **Principal Investigator** for all these basic-level issues.
*   **JSON Format:** The submission *must* be a valid JSON file (`test_submission.json`) adhering to the specified structure (top-level keys `candidate_id` and `identified_issues`, list of objects, correct keys within objects, all values as strings). Minor syntax errors that are easily correctable might be overlooked at the evaluator's discretion, but fundamentally broken JSON fails the format requirement.
*   **Overall Assessment:** A passing candidate demonstrates the ability to read protocol excerpts and scenarios, spot common types of basic issues, understand their potential impact, and know the crucial first step is typically to inform the PI. They must also demonstrate basic technical proficiency by structuring the answer correctly in JSON.

**Summary for Passing:**
*   Valid `test_submission.json` format.
*   At least 4 out of 6 issues correctly identified, meeting the criteria described above (especially correct problem summary, appropriate next step focusing on PI communication).","Okay, here is the Python script `task_evaluation.py` designed to evaluate the candidate's submission based on the provided answer key and evaluation criteria.

```python
# task_evaluation.py
import json
import argparse
import os
import re # Using regex for slightly more flexible keyword checking

def load_json(filepath):
    """"""Loads a JSON file with error handling.""""""
    if not os.path.exists(filepath):
        print(f""Error: File not found at {filepath}"")
        return None
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON format in {filepath}. Details: {e}"")
        return None
    except Exception as e:
        print(f""Error reading file {filepath}. Details: {e}"")
        return None

def check_keywords(text, keywords, required_count=1, case_sensitive=False):
    """"""Checks if a minimum number of keywords are present in the text.""""""
    if not text or not keywords:
        return False
    
    found_count = 0
    flags = 0 if case_sensitive else re.IGNORECASE
    
    # Ensure text is a string
    text_str = str(text)

    for keyword in keywords:
        # Use word boundaries (\b) to avoid partial matches within words
        # Escape special regex characters in keyword if necessary
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, text_str, flags):
            found_count += 1
            
    return found_count >= required_count

def evaluate_submission(submission_data, key_data):
    """"""Evaluates the candidate's submission against the answer key.""""""
    results = {
        ""candidate_id"": submission_data.get(""candidate_id"", ""ID_MISSING""),
        ""overall_score"": 0.0,
        ""evaluation_details"": [],
        ""feedback"": []
    }

    # --- Basic Structure Validation ---
    if not isinstance(submission_data, dict):
        results[""feedback""].append(""Submission is not a valid JSON object."")
        return results # Cannot proceed
    if ""identified_issues"" not in submission_data:
        results[""feedback""].append(""Submission JSON missing required 'identified_issues' key."")
        return results # Cannot proceed
    if not isinstance(submission_data[""identified_issues""], list):
         results[""feedback""].append(""'identified_issues' should be a list/array."")
         # Attempt to proceed if possible, but score might be 0
         submission_data[""identified_issues""] = []


    key_issues = key_data.get(""identified_issues"", [])
    submitted_issues = submission_data.get(""identified_issues"", [])
    
    if not key_issues:
        results[""feedback""].append(""Answer key is missing 'identified_issues'. Cannot evaluate."")
        return results

    max_score = len(key_issues)
    achieved_score = 0
    
    # Keep track of which submitted issues have been matched to avoid double counting
    matched_submission_indices = set()

    # Define keywords for matching each key issue (adjust as needed for robustness)
    # These keywords target the core concept of each expected answer.
    key_issue_match_criteria = {
        ""ISSUE_01"": {""ref"": [""4.1.3""], ""summary"": [""prior history"", ""treatment"", ""ambiguous"", ""unclear"", ""vague""], ""next_step"": [""PI"", ""Investigator"", ""clarification"", ""query sponsor""]},
        ""ISSUE_02"": {""ref"": [""6.2"", ""footnote""], ""summary"": [""fasting"", ""post-dose"", ""PK"", ""safety"", ""conflict"", ""contradiction"", ""Visit 2"", ""sample""], ""next_step"": [""PI"", ""Investigator"", ""clarification"", ""sponsor""]},
        ""ISSUE_03"": {""ref"": [""6.2"", ""diary""], ""summary"": [""diary"", ""review"", ""missing detail"", ""unclear"", ""no details"", ""lack of detail""], ""next_step"": [""PI"", ""Investigator"", ""clarification"", ""monitor""]},
        ""ISSUE_04"": {""ref"": [""Scenario 1"", ""4.2.2""], ""summary"": [""Scenario 1"", ""pseudoephedrine"", ""OTC"", ""eligibility"", ""BP"", ""anti-hypertensive"", ""ambiguity""], ""next_step"": [""PI"", ""Investigator"", ""assessment"", ""decision""]},
        ""ISSUE_05"": {""ref"": [""Scenario 2"", ""6.2""], ""summary"": [""Scenario 2"", ""diary"", ""missing"", ""forgot"", ""data"", ""entries""], ""next_step"": [""PI"", ""Investigator"", ""instruct"", ""document"", ""notify""]},
        ""ISSUE_06"": {""ref"": [""Scenario 3"", ""6.2""], ""summary"": [""Scenario 3"", ""ECG"", ""QT"", ""borderline"", ""safety"", ""finding"", ""notify""], ""next_step"": [""PI"", ""Investigator"", ""notify"", ""assessment"", ""immediately""]},
    }

    # --- Evaluate each key issue ---
    for key_issue in key_issues:
        key_id = key_issue.get(""issue_id"")
        key_desc = key_issue.get(""problem_summary"", ""No description in key"")
        
        detail = {
            ""key_issue_id"": key_id,
            ""key_issue_description"": f""{key_issue.get('source_description', '')}: {key_desc[:70]}..."", # Short description
            ""identified_by_candidate"": False,
            ""matching_candidate_issue_id"": None,
            ""evaluation_notes"": []
        }

        criteria = key_issue_match_criteria.get(key_id)
        if not criteria:
            detail[""evaluation_notes""].append(f""Warning: No matching criteria defined for key issue {key_id}."")
            results[""evaluation_details""].append(detail)
            continue # Cannot evaluate this key issue

        found_match = False
        for idx, sub_issue in enumerate(submitted_issues):
            if idx in matched_submission_indices:
                continue # Skip already matched submission issue

            # --- Check individual fields of the submitted issue ---
            # Ensure sub_issue is a dictionary and has expected keys
            if not isinstance(sub_issue, dict):
                detail[""evaluation_notes""].append(f""Candidate entry at index {idx} is not a valid object."")
                continue

            sub_id = sub_issue.get(""issue_id"", f""MISSING_ID_{idx}"")
            sub_ref = sub_issue.get(""protocol_reference"", """")
            sub_summary = sub_issue.get(""problem_summary"", """")
            sub_next_step = sub_issue.get(""recommended_next_step"", """")
            sub_comm_person = sub_issue.get(""communication_point_person"", """")

            # Apply matching logic based on keywords
            match_ref = check_keywords(sub_ref, criteria[""ref""], required_count=1)
            match_summary = check_keywords(sub_summary, criteria[""summary""], required_count=2) # Require at least 2 keywords for summary
            match_step = check_keywords(sub_next_step, criteria[""next_step""], required_count=1)
            match_person = check_keywords(sub_comm_person, [""Principal Investigator"", ""PI""], required_count=1)

            # Define conditions for a successful match (adjust strictness here)
            # Example: Must match reference context, summary concept, next step involves PI, and comms person is PI.
            is_a_match = match_ref and match_summary and match_step and match_person

            if is_a_match:
                detail[""identified_by_candidate""] = True
                detail[""matching_candidate_issue_id""] = sub_id
                detail[""evaluation_notes""].append(f""Matched with candidate issue '{sub_id}'. Criteria met."")
                achieved_score += 1
                matched_submission_indices.add(idx)
                found_match = True
                break # Stop searching for a match for this key_issue

        if not found_match:
             detail[""evaluation_notes""].append(""No matching issue found in candidate submission."")
             
        results[""evaluation_details""].append(detail)

    # --- Calculate Overall Score ---
    if max_score > 0:
        results[""overall_score""] = round((achieved_score / max_score) * 100, 2)
    else:
        results[""overall_score""] = 0.0
        results[""feedback""].append(""Max score is zero, cannot calculate percentage."")
        
    results[""feedback""].append(f""Candidate correctly identified {achieved_score} out of {max_score} key issues."")

    # --- Add check for minimum passing criteria (as per evaluation info) ---
    min_correct_issues = 4
    if achieved_score >= min_correct_issues:
         results[""feedback""].append(f""Result: PASS (Met minimum requirement of {min_correct_issues} correctly identified issues)."")
    else:
         results[""feedback""].append(f""Result: FAIL (Did not meet minimum requirement of {min_correct_issues} correctly identified issues)."")
         
    # --- Check for extra issues submitted ---
    unmatched_submission_count = len(submitted_issues) - len(matched_submission_indices)
    if unmatched_submission_count > 0:
        results[""feedback""].append(f""Candidate submitted {unmatched_submission_count} issue(s) that did not match any key issues."")


    return results

def main():
    parser = argparse.ArgumentParser(description=""Evaluate CRC Basic Practical Exam Submission."")
    parser.add_argument(""submission_file"", help=""Path to the candidate's submission JSON file."")
    parser.add_argument(""key_file"", help=""Path to the answer key JSON file."")
    args = parser.parse_args()

    print(f""Loading submission file: {args.submission_file}"")
    submission_data = load_json(args.submission_file)
    if submission_data is None:
        return # Error message already printed by load_json

    print(f""Loading answer key file: {args.key_file}"")
    key_data = load_json(args.key_file)
    if key_data is None:
        return # Error message already printed by load_json

    print(""Evaluating submission..."")
    evaluation_results = evaluate_submission(submission_data, key_data)

    output_filename = ""test_results.json""
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        print(f""Evaluation complete. Results saved to {output_filename}"")
    except Exception as e:
        print(f""Error writing results to {output_filename}. Details: {e}"")

if __name__ == ""__main__"":
    main()
```

**How to Use:**

1.  **Save the script:** Save the code above as `task_evaluation.py`.
2.  **Prepare files:** Make sure you have:
    *   The candidate's submission file (e.g., `test_submission.json`).
    *   The answer key file provided in the prompt (save its JSON content as `answer_key.json`).
    *   Place these two JSON files in the *same directory* as the `task_evaluation.py` script.
3.  **Run from command line:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the script like this:

    ```bash
    python task_evaluation.py test_submission.json answer_key.json
    ```

    *(Replace `test_submission.json` and `answer_key.json` with the actual filenames if they are different).*
4.  **Check results:** The script will print status messages to the console and create a file named `test_results.json` in the same directory. This file will contain the detailed evaluation and the overall score.

**Explanation of the Script:**

1.  **`load_json`:** Safely loads JSON data from a file, handling file not found and JSON decoding errors.
2.  **`check_keywords`:** A helper function using regular expressions (`re`) to check if a given text contains a minimum number of keywords from a list. It uses `\b` (word boundaries) for more accurate matching and is case-insensitive by default.
3.  **`evaluate_submission`:**
    *   Initializes the `results` dictionary.
    *   Performs basic validation of the submission structure.
    *   Retrieves the lists of issues from both submission and key.
    *   Defines `key_issue_match_criteria`: This dictionary holds keywords specific to each expected answer key issue, categorized by the field they should appear in (`ref`, `summary`, `next_step`). This is the core of the matching logic.
    *   Iterates through each `key_issue` from the answer key.
    *   For each `key_issue`, it iterates through the `submitted_issues` (skipping any already matched).
    *   It uses `check_keywords` to see if the submitted issue's fields (`protocol_reference`, `problem_summary`, `recommended_next_step`, `communication_point_person`) contain relevant keywords defined in `key_issue_match_criteria`.
    *   An `is_a_match` condition determines if the submitted issue sufficiently matches the key issue (e.g., requiring matches in reference, summary, next step involving PI, and comms person being PI). *This condition can be adjusted to be stricter or looser.*
    *   If a match is found, the score is incremented, the match is recorded in `evaluation_details`, and the matched submitted issue is marked to prevent reuse.
    *   Calculates the `overall_score` as a percentage.
    *   Adds feedback messages, including a PASS/FAIL assessment based on the minimum required correct issues (set to 4).
    *   Notes if the candidate submitted extra issues not matching any key issue.
4.  **`main`:**
    *   Uses `argparse` to get the submission and key file paths from the command line.
    *   Calls `load_json` to load the data.
    *   Calls `evaluate_submission` to perform the evaluation.
    *   Saves the results to `test_results.json` using `json.dump` with indentation for readability.","{'candidate_id': 'EVALUATOR_KEY', 'identified_issues': [{'issue_id': 'ISSUE_01', 'source_description': 'Protocol Section 4.1 Inclusion Criteria', 'protocol_reference': 'Section 4.1.3', 'problem_summary': ""Inclusion criterion 4.1.3 'prior history of treatment for hypertension' is ambiguous. It lacks specifics on type, duration, timing, or required documentation of the prior treatment."", 'potential_impact': 'Inconsistent subject screening and enrollment; difficulty verifying eligibility.', 'recommended_next_step': 'Inform PI and request clarification or query sponsor for a more specific definition.', 'communication_point_person': 'Principal Investigator'}, {'issue_id': 'ISSUE_02', 'source_description': 'Schedule of Assessments Table Footnotes', 'protocol_reference': 'Section 6.2 Table 1 Footnotes 1 and 2', 'problem_summary': 'There is a conflict for Visit 2 procedures. Footnote 1 requires Safety Labs in a fasting state, while Footnote 2 requires the PK sample ~2 hours post-dose. These are contradictory requirements for the same visit.', 'potential_impact': 'Inability to perform Visit 2 procedures as written; risk of protocol deviation; inconsistent sample collection.', 'recommended_next_step': 'Notify PI of the protocol contradiction and request clarification from the sponsor.', 'communication_point_person': 'Principal Investigator'}, {'issue_id': 'ISSUE_03', 'source_description': 'Schedule of Assessments Table', 'protocol_reference': 'Section 6.2 Table 1', 'problem_summary': ""The SoA lists 'Patient Diary Review' but the protocol excerpt provides no details on the diary's content, completion frequency, specific review process, or handling of missing entries."", 'potential_impact': 'Inconsistent data collection; difficulty assessing compliance; unclear process for handling issues like missing data.', 'recommended_next_step': 'Flag the lack of detail for discussion with PI/monitor; request clarification or check study manuals (if available).', 'communication_point_person': 'Principal Investigator'}, {'issue_id': 'ISSUE_04', 'source_description': 'Scenario 1', 'protocol_reference': 'Section 4.2.2', 'problem_summary': ""Potential subject took pseudoephedrine. While not strictly an 'anti-hypertensive' (Exclusion 4.2.2), it can affect blood pressure. Eligibility is unclear due to potential confounding effect and ambiguity of exclusion criteria scope."", 'potential_impact': 'Incorrect subject enrollment/exclusion; confounding factor for baseline BP assessment; potential protocol deviation if enrolled incorrectly.', 'recommended_next_step': 'Inform PI about the medication use and request their assessment of eligibility before proceeding.', 'communication_point_person': 'Principal Investigator'}, {'issue_id': 'ISSUE_05', 'source_description': 'Scenario 2', 'protocol_reference': 'Section 6.2 Table 1 / Scenario Context', 'problem_summary': 'Patient forgot to complete the patient diary for 3 days prior to Visit 2 review. This represents missing source data.', 'potential_impact': 'Incomplete or inaccurate data collection; potential protocol deviation regarding data completion.', 'recommended_next_step': 'Instruct patient on proper procedure for documenting late/missed entries (if guidance exists) or document the missing data appropriately; inform PI of the issue.', 'communication_point_person': 'Principal Investigator'}, {'issue_id': 'ISSUE_06', 'source_description': 'Scenario 3', 'protocol_reference': 'Section 6.2 Table 1 / Scenario Context', 'problem_summary': ""ECG at Visit 3 shows 'Borderline QT interval' flagged by the machine. Although patient is asymptomatic, this is a potential safety signal requiring medical assessment."", 'potential_impact': 'Potential subject safety risk if not assessed; delay in identifying clinically significant change.', 'recommended_next_step': 'Immediately notify the Principal Investigator of the ECG finding for their review and assessment of clinical significance.', 'communication_point_person': 'Principal Investigator'}]}",[],True,True,0.0,99.0,100.0,1.0,False,"Could not parse JSON. Raw LLM response:
```json
{
  ""makes_sense"": true,
  ""explanation"": ""The exam is practical, testing core CRC tasks (protocol review, scenario analysis, issue identification, reporting) relevant for 2025. The instructions, materials (with deliberately embedded issues), and submission format (JSON) are generally clear and appropriate for a basic level assessment. The grading script correctly implements the evaluation criteria (pass >= 4/6 issues) and the provided answer key should score 100% based on the script's keyword matching logic. Potential Weaknesses: 1) The submission format instruction specifies `candidate_id` as 'model version', which is confusing if the exam targets human CRCs (should be candidate name/ID). 2) The grading script's reliance on specific keyword matching across multiple fields (`protocol_reference`, `problem_summary`, `recommended_next_step`, `communication_point_person`) could potentially lead to false negatives if a candidate uses valid alternative phrasing not captured in the predefined keywords, despite correctly identifying the issue and appropriate action.""
}
```","{'overview': prompt_token_count: 247
candidates_token_count: 2311
total_token_count: 4369
, 'instructions': prompt_token_count: 2764
candidates_token_count: 1268
total_token_count: 5321
, 'materials': prompt_token_count: 4238
candidates_token_count: 3331
total_token_count: 9447
, 'check_materials': prompt_token_count: 4703
candidates_token_count: 1
total_token_count: 5069
, 'check_website': prompt_token_count: 4736
candidates_token_count: 1
total_token_count: 5194
, 'submission': prompt_token_count: 7353
candidates_token_count: 1189
total_token_count: 9147
, 'evaluation': prompt_token_count: 8463
candidates_token_count: 2437
total_token_count: 12874
, 'grading': prompt_token_count: 11089
candidates_token_count: 3379
total_token_count: 17213
, 'check_sense': prompt_token_count: 15264
candidates_token_count: 227
total_token_count: 18210
}",management_occupations
Fitness and Wellness Coordinators,15548.0,"Conduct needs assessments or surveys to determine interest in, or satisfaction with, wellness and fitness programs, events, or services.",gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor']","['Text', 'Data']","Okay, here are the clear, concise instructions for the candidate for the Basic Practical Exam.

---

### Fitness and Wellness Coordinator: Basic Practical Exam

**Welcome!**
This exam is designed to assess your practical skills in conducting needs assessments and analyzing survey data relevant to fitness and wellness programs.

**Exam Duration:** 90 minutes

**Allowed Tools:**
*   A Spreadsheet program (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A Text Editor (e.g., Notepad, TextEdit, VS Code)

**Focus:**
This is a practical exam. You will be assessed on your ability to perform the tasks successfully using the provided tools and information.

---

**Submission Requirements:**
1.  Compile all your answers into a single JSON file named `test_submission.json`.
2.  The JSON file must follow the structure provided below. Replace placeholder values with your actual answers.
3.  Ensure your JSON is valid. You can use an online JSON validator to check your file before considering it complete.

**JSON Submission File Structure (`test_submission.json`):**
```json
{
  ""candidate_name"": ""Your Name"",
  ""submission_date"": ""YYYY-MM-DD"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Placeholder Title"",
    ""target_audience_description"": ""Placeholder Description"",
    ""survey_introduction_text"": ""Placeholder Introduction"",
    ""survey_questions"": [
      {
        ""question_text"": ""Placeholder Question 1 Text"",
        ""question_type"": ""Placeholder Question Type"",
        ""options"": [""Option A"", ""Option B""]
      }
      // ... add all your questions for Task 1 here
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 0,
      ""2"": 0,
      ""3"": 0,
      ""4"": 0,
      ""5"": 0
    },
    ""top_enjoyed_aspects"": [
      ""Placeholder Enjoyed Aspect 1"",
      ""Placeholder Enjoyed Aspect 2""
    ],
    ""top_improvement_suggestions"": [
      ""Placeholder Suggestion 1"",
      ""Placeholder Suggestion 2""
    ],
    ""recommendation_for_next_year"": ""Placeholder Recommendation""
  }
}
```
*(Remember to replace `""Your Name""` and `""YYYY-MM-DD""` with your actual name and the date of submission.)*

---

**Scenario Context (Applies to both tasks):**
You are a new Fitness and Wellness Coordinator at ""Innovatech Solutions,"" a tech company with 200 employees. Management is keen to improve employee well-being.

---

**Task 1: New Program Interest Survey Design (Suggested time: 45 minutes)**

Innovatech Solutions is considering introducing a ""Mid-day Mindfulness & Stretching Break"" program. This would involve a 15-20 minute guided session offered twice a week. Before launching, they want to gauge employee interest and preferences.

**Your Task:**
Design a short survey to send to Innovatech employees. In the `task_1_interest_survey_design` section of your `test_submission.json` file, provide the following:

1.  `survey_title`: A suitable title for your survey.
2.  `target_audience_description`: A brief description of who this survey is for (e.g., ""All employees of Innovatech Solutions"").
3.  `survey_introduction_text`: A brief introductory paragraph for the survey (2-3 sentences) explaining its purpose to employees and encouraging participation.
4.  `survey_questions`: A list of **5 to 7 distinct questions**. For each question, you must specify:
    *   `question_text`: The exact wording of the question.
    *   `question_type`: Choose one type from the following list:
        *   ""Multiple Choice - Single Answer""
        *   ""Multiple Choice - Multiple Answers""
        *   ""Likert Scale (1-5)"" (where 1 is low, 5 is high)
        *   ""Open-ended Text""
    *   `options`: A list of predefined answer options. This is required for ""Multiple Choice"" and ""Likert Scale"" questions. For ""Open-ended Text"" questions, provide an empty list `[]`.

**Example Question Structure for your JSON:**
```json
// This is an example, do not include it as one of your actual questions unless appropriate.
{
    ""question_text"": ""How interested are you in participating in a mid-day mindfulness session?"",
    ""question_type"": ""Likert Scale (1-5)"",
    ""options"": [""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]
}
```
```json
// Example for an open-ended question
{
    ""question_text"": ""What other wellness activities would you be interested in?"",
    ""question_type"": ""Open-ended Text"",
    ""options"": []
}
```

---

**Task 2: Existing Program Satisfaction Data Analysis (Suggested time: 45 minutes)**

Innovatech Solutions recently concluded its annual ""Step Challenge."" 50 employees participated. A short satisfaction survey was sent out, and 30 responses were collected. The raw data from these 15 of these responses is provided below.

**Provided Data (`step_challenge_satisfaction_data`):**
The data is comma-separated (CSV format). The columns are: `ParticipantID`, `OverallSatisfaction(1-5 Likert; 5=Very Satisfied)`, `EnjoyedMost`, `SuggestionForImprovement`.

```
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Tracking daily progress,More frequent updates on leaderboard
P06,2,N/A,More prize categories
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,Better mobile app
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```

**Your Task:**
1.  Import or copy this data into your spreadsheet program.
2.  Analyze the data to identify the required information below.
3.  In the `task_2_satisfaction_data_analysis` section of your `test_submission.json` file, provide the following:
    *   `satisfaction_rating_summary`: An object where keys are the satisfaction ratings (""1"", ""2"", ""3"", ""4"", ""5"") and values are the **counts** of how many participants gave each rating.
    *   `top_enjoyed_aspects`: A list containing the **top 2** most frequently mentioned aspects employees ""Enjoyed Most"". Use the exact phrasing as it appears in the data.
    *   `top_improvement_suggestions`: A list containing the **top 2** most frequently mentioned ""Suggestions For Improvement"". Use the exact phrasing as it appears in the data. Exclude ""N/A"" from this list if it appears as a frequent suggestion.
    *   `recommendation_for_next_year`: Based on your analysis of the provided data, formulate **one concrete recommendation** (1-2 sentences) for improving next year's Step Challenge.

---

**End of Exam Instructions. Good luck!**
The following data is provided to the candidate for **Task 2: Existing Program Satisfaction Data Analysis**. This data should be presented to the candidate as `step_challenge_satisfaction_data`.

```csv
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Feeling healthier,More frequent updates on leaderboard
P06,2,N/A,Shorter duration
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,More variety in challenges
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```
To ensure your exam submission is correctly evaluated, please adhere strictly to the following JSON format for your `test_submission.json` file.

**File Name:** `test_submission.json`

**General JSON Structure:**

The entire submission must be a single JSON object. All text values must be enclosed in double quotes. Ensure your JSON is valid before submission; you can use an online JSON validator.

```json
{
  ""candidate_name"": ""YOUR_FULL_NAME"",
  ""candidate_id"": ""YOUR_ID_HERE"",
  ""submission_date"": ""YYYY-MM-DD"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    // Structure for Task 1 answers
  },
  ""task_2_satisfaction_data_analysis"": {
    // Structure for Task 2 answers
  }
}
```

**Field Explanations and Format Requirements:**

1.  **`candidate_name`**:
    *   **Type**: `String`
    *   **Format**: Your full name (e.g., ""Jane Doe"").

2.  **`candidate_id`**:
    *   **Type**: `String`
    *   **Format**: The model version that is powering your responses (e.g., ""GPT-4o"", ""Claude-3-Opus"", ""Gemini-1.5-Pro"", etc.). Replace ""YOUR_ID_HERE"" with this value.

3.  **`submission_date`**:
    *   **Type**: `String`
    *   **Format**: The date of submission in `YYYY-MM-DD` format (e.g., ""2024-07-28"").

4.  **`exam_version`**:
    *   **Type**: `String`
    *   **Format**: This field should contain the exact value: `""Basic_FWC_NeedsAssessment_v1""`.

5.  **`task_1_interest_survey_design`**:
    *   **Type**: `Object`
    *   This object contains your answers for Task 1.

    ```json
    ""task_1_interest_survey_design"": {
      ""survey_title"": ""YOUR_SURVEY_TITLE"",
      ""target_audience_description"": ""YOUR_TARGET_AUDIENCE_DESCRIPTION"",
      ""survey_introduction_text"": ""YOUR_SURVEY_INTRODUCTION_TEXT"",
      ""survey_questions"": [
        // Array of 5 to 7 question objects
      ]
    }
    ```

    *   **`survey_title`**:
        *   **Type**: `String`
        *   **Format**: A concise and descriptive title for your survey.
    *   **`target_audience_description`**:
        *   **Type**: `String`
        *   **Format**: A brief description of the intended recipients of the survey.
    *   **`survey_introduction_text`**:
        *   **Type**: `String`
        *   **Format**: The introductory text for the survey (2-3 sentences).
    *   **`survey_questions`**:
        *   **Type**: `Array` of `Object`
        *   **Format**: A list containing 5 to 7 distinct question objects. Each object represents one survey question and must have the following structure:
            ```json
            {
              ""question_text"": ""EXACT_WORDING_OF_THE_QUESTION"",
              ""question_type"": ""CHOSEN_QUESTION_TYPE"",
              ""options"": [
                // Array of strings for options, or empty array
              ]
            }
            ```
        *   **`question_text`**:
            *   **Type**: `String`
            *   **Format**: The exact wording of the survey question.
        *   **`question_type`**:
            *   **Type**: `String`
            *   **Format**: Must be one of the following exact strings:
                *   `""Multiple Choice - Single Answer""`
                *   `""Multiple Choice - Multiple Answers""`
                *   `""Likert Scale (1-5)""`
                *   `""Open-ended Text""`
        *   **`options`**:
            *   **Type**: `Array` of `String`
            *   **Format**:
                *   For `""Multiple Choice - Single Answer""` and `""Multiple Choice - Multiple Answers""`: A list of strings representing the answer choices (e.g., `[""Option 1"", ""Option 2"", ""Option 3""]`).
                *   For `""Likert Scale (1-5)""`: A list of 5 strings representing the scale points, clearly indicating the meaning of each point (e.g., `[""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]`).
                *   For `""Open-ended Text""`: An empty array `[]`.

6.  **`task_2_satisfaction_data_analysis`**:
    *   **Type**: `Object`
    *   This object contains your answers for Task 2.

    ```json
    ""task_2_satisfaction_data_analysis"": {
      ""satisfaction_rating_summary"": {
        ""1"": 0, // Integer count
        ""2"": 0, // Integer count
        ""3"": 0, // Integer count
        ""4"": 0, // Integer count
        ""5"": 0  // Integer count
      },
      ""top_enjoyed_aspects"": [
        ""ASPECT_1_TEXT"", // String
        ""ASPECT_2_TEXT""  // String
      ],
      ""top_improvement_suggestions"": [
        ""SUGGESTION_1_TEXT"", // String
        ""SUGGESTION_2_TEXT""  // String
      ],
      ""recommendation_for_next_year"": ""YOUR_RECOMMENDATION_TEXT"" // String
    }
    ```

    *   **`satisfaction_rating_summary`**:
        *   **Type**: `Object`
        *   **Format**: Keys must be strings `""1""`, `""2""`, `""3""`, `""4""`, `""5""`. Values must be `Integer` representing the count of participants for each satisfaction rating.
    *   **`top_enjoyed_aspects`**:
        *   **Type**: `Array` of `String`
        *   **Format**: A list containing exactly two (2) strings. These strings must be the exact phrasing of the top 2 most frequently mentioned aspects from the `EnjoyedMost` column in the provided data.
    *   **`top_improvement_suggestions`**:
        *   **Type**: `Array` of `String`
        *   **Format**: A list containing exactly two (2) strings. These strings must be the exact phrasing of the top 2 most frequently mentioned suggestions from the `SuggestionForImprovement` column in the provided data. Exclude ""N/A"" when determining these top suggestions.
    *   **`recommendation_for_next_year`**:
        *   **Type**: `String`
        *   **Format**: Your single, concrete recommendation (1-2 sentences) based on your analysis of the provided data.

**Example `test_submission.json` (Illustrative Structure - Do NOT use these placeholder values):**

```json
{
  ""candidate_name"": ""Your Name"",
  ""candidate_id"": ""GPT-4o"",
  ""submission_date"": ""2024-07-28"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Mid-day Wellness Break Interest"",
    ""target_audience_description"": ""All Innovatech Solutions employees"",
    ""survey_introduction_text"": ""We are exploring a new mid-day wellness break and would love your input. Please take a few minutes to complete this short survey."",
    ""survey_questions"": [
      {
        ""question_text"": ""How interested are you in a 15-20 minute guided mindfulness and stretching break during the workday?"",
        ""question_type"": ""Likert Scale (1-5)"",
        ""options"": [""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]
      },
      {
        ""question_text"": ""What time of day would be most convenient for you to participate? (Select all that apply)"",
        ""question_type"": ""Multiple Choice - Multiple Answers"",
        ""options"": [""11:00 AM - 11:30 AM"", ""12:30 PM - 1:00 PM"", ""2:00 PM - 2:30 PM"", ""Other (please specify)""]
      },
      {
        ""question_text"": ""If you selected 'Other' for the previous question, please specify your preferred time:"",
        ""question_type"": ""Open-ended Text"",
        ""options"": []
      }
      // ... up to 5-7 questions
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 1,
      ""2"": 1,
      ""3"": 3,
      ""4"": 5,
      ""5"": 5
    },
    ""top_enjoyed_aspects"": [
      ""Team competition"",
      ""Tracking daily progress""
    ],
    ""top_improvement_suggestions"": [
      ""More prize categories"",
      ""Better mobile app""
    ],
    ""recommendation_for_next_year"": ""Consider adding more prize categories and improving the mobile app for next year's Step Challenge to increase engagement.""
  }
}
```

**Supplementary Files:**
No supplementary files are required. All answers must be contained within the `test_submission.json` file.","You are an excellent examiner of Fitness and Wellness Coordinators capabilities. Design a remote, **practical** exam to verify whether a Fitness and Wellness Coordinators can Conduct needs assessments or surveys to determine interest in, or satisfaction with, wellness and fitness programs, events, or services..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets and Text editor
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here's the design for the **Basic Practical Exam** for a Fitness and Wellness Coordinator, focusing on conducting needs assessments/surveys.

## Evaluator's Guide: Basic Practical Exam

**Exam Purpose:**
This basic practical exam is designed to assess a candidate's fundamental ability to:
1.  Develop relevant questions for a needs assessment survey aimed at understanding interest in a new wellness program.
2.  Perform a simple analysis of provided satisfaction survey data for an existing program.
3.  Structure their findings and recommendations in a clear, organized manner using basic digital tools.

The exam focuses on the practical application of these skills rather than theoretical knowledge.

**Exam Structure:**
The exam consists of two distinct tasks, both contributing to a single JSON submission file (""test_submission.json"").

*   **Task 1: New Program Interest Survey Design:** The candidate will be given a scenario and asked to draft a short survey (introduction and questions) to gauge interest in a new wellness initiative. This tests their ability to formulate appropriate questions for a needs assessment.
*   **Task 2: Existing Program Satisfaction Data Analysis:** The candidate will be provided with a small, simplified dataset from a satisfaction survey for an existing program. They will need to use a spreadsheet to perform basic analysis (e.g., count responses, identify common themes) and then summarize key findings and suggest one improvement. This tests basic data interpretation and reporting skills.

**Candidate Instructions & Materials:**
(The following would be provided to the candidate)

---

### Fitness and Wellness Coordinator: Basic Practical Exam

**Welcome!** This exam will assess your practical skills in conducting needs assessments and analyzing survey data.

**Instructions:**
1.  Read each task carefully.
2.  You have a maximum of **90 minutes** to complete both tasks.
3.  You may use a **Spreadsheet program** (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc) and a **Text Editor** (e.g., Notepad, TextEdit, VS Code).
4.  All your answers must be compiled into a single JSON file named `test_submission.json`. A template for this file structure is provided below.
5.  Ensure your JSON is valid. You can use an online JSON validator to check before finalizing.

**Scenario Context (Applies to both tasks):**
You are a new Fitness and Wellness Coordinator at ""Innovatech Solutions,"" a tech company with 200 employees. Management is keen to improve employee well-being.

---

**Task 1: New Program Interest Survey Design (45 minutes)**

Innovatech Solutions is considering introducing a ""Mid-day Mindfulness & Stretching Break"" program. This would involve a 15-20 minute guided session offered twice a week. Before launching, they want to gauge employee interest and preferences.

**Your Task:**
Design a short survey to send to Innovatech employees.
In the `task_1_interest_survey_design` section of your `test_submission.json` file, provide the following:
1.  `survey_title`: A suitable title for your survey.
2.  `target_audience_description`: A brief description of who this survey is for.
3.  `survey_introduction_text`: A brief introductory paragraph for the survey, explaining its purpose to employees.
4.  `survey_questions`: A list of 5-7 distinct questions. For each question, specify:
    *   `question_text`: The exact wording of the question.
    *   `question_type`: The type of question (e.g., ""Multiple Choice - Single Answer"", ""Multiple Choice - Multiple Answers"", ""Likert Scale (1-5)"", ""Open-ended Text"").
    *   `options` (if applicable): A list of predefined answer options for multiple-choice or Likert scale questions.

**Example Question Structure in JSON:**
```json
{
    ""question_text"": ""How interested are you in participating in a mid-day mindfulness session?"",
    ""question_type"": ""Likert Scale (1-5)"",
    ""options"": [""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]
}
```

---

**Task 2: Existing Program Satisfaction Data Analysis (45 minutes)**

Innovatech Solutions recently concluded its annual ""Step Challenge."" 50 employees participated. A short satisfaction survey was sent out, and 30 responses were collected. The raw data is provided below.

**Provided Data (`step_challenge_satisfaction_data.txt`):**
(This data would be provided as a separate text file, or embedded directly. It's designed to be easily copy-pasted into a spreadsheet.)
```
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Feeling healthier,More frequent updates on leaderboard
P06,2,N/A,Shorter duration
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,More variety in challenges
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```

**Your Task:**
1.  Import or copy this data into your spreadsheet program.
2.  Analyze the data to identify:
    *   The number of participants for each ""Overall Satisfaction"" rating.
    *   The top 2 most frequently mentioned aspects employees ""Enjoyed Most.""
    *   The top 2 most frequently mentioned ""Suggestions For Improvement.""
3.  Based on your analysis, formulate one concrete recommendation for improving next year's Step Challenge.

In the `task_2_satisfaction_data_analysis` section of your `test_submission.json` file, provide the following:
1.  `satisfaction_rating_summary`: An object where keys are satisfaction ratings (e.g., ""1"", ""2"", ""3"", ""4"", ""5"") and values are the counts for each.
2.  `top_enjoyed_aspects`: A list of the top 2 most frequently mentioned positive aspects.
3.  `top_improvement_suggestions`: A list of the top 2 most frequently mentioned suggestions.
4.  `recommendation_for_next_year`: Your single, concrete recommendation (1-2 sentences).

---

**Submission File Structure (`test_submission.json`):**

Please create a JSON file named `test_submission.json` with the following structure. Replace the placeholder values with your actual answers.

```json
{
  ""candidate_name"": ""Your Name"",
  ""submission_date"": ""YYYY-MM-DD"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Placeholder Title"",
    ""target_audience_description"": ""Placeholder Description"",
    ""survey_introduction_text"": ""Placeholder Introduction"",
    ""survey_questions"": [
      {
        ""question_text"": ""Placeholder Question 1 Text"",
        ""question_type"": ""Placeholder Question Type"",
        ""options"": [""Option A"", ""Option B""]
      },
      {
        ""question_text"": ""Placeholder Question 2 Text"",
        ""question_type"": ""Placeholder Question Type (e.g., Open-ended Text)"",
        ""options"": [] // Empty if no predefined options
      }
      // ... add all your questions here
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 0, // Count for rating 1
      ""2"": 0, // Count for rating 2
      ""3"": 0, // Count for rating 3
      ""4"": 0, // Count for rating 4
      ""5"": 0  // Count for rating 5
    },
    ""top_enjoyed_aspects"": [
      ""Placeholder Enjoyed Aspect 1"",
      ""Placeholder Enjoyed Aspect 2""
    ],
    ""top_improvement_suggestions"": [
      ""Placeholder Suggestion 1"",
      ""Placeholder Suggestion 2""
    ],
    ""recommendation_for_next_year"": ""Placeholder Recommendation""
  }
}
```

**End of Exam Instructions.**

---

This structure provides a clear framework for the candidate and simplifies the evaluation process by standardizing the output. The tasks are practical, use the specified tools, and are achievable within the 90-minute timeframe for a basic level assessment.","Okay, here are the clear, concise instructions for the candidate for the Basic Practical Exam.

---

### Fitness and Wellness Coordinator: Basic Practical Exam

**Welcome!**
This exam is designed to assess your practical skills in conducting needs assessments and analyzing survey data relevant to fitness and wellness programs.

**Exam Duration:** 90 minutes

**Allowed Tools:**
*   A Spreadsheet program (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A Text Editor (e.g., Notepad, TextEdit, VS Code)

**Focus:**
This is a practical exam. You will be assessed on your ability to perform the tasks successfully using the provided tools and information.

---

**Submission Requirements:**
1.  Compile all your answers into a single JSON file named `test_submission.json`.
2.  The JSON file must follow the structure provided below. Replace placeholder values with your actual answers.
3.  Ensure your JSON is valid. You can use an online JSON validator to check your file before considering it complete.

**JSON Submission File Structure (`test_submission.json`):**
```json
{
  ""candidate_name"": ""Your Name"",
  ""submission_date"": ""YYYY-MM-DD"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Placeholder Title"",
    ""target_audience_description"": ""Placeholder Description"",
    ""survey_introduction_text"": ""Placeholder Introduction"",
    ""survey_questions"": [
      {
        ""question_text"": ""Placeholder Question 1 Text"",
        ""question_type"": ""Placeholder Question Type"",
        ""options"": [""Option A"", ""Option B""]
      }
      // ... add all your questions for Task 1 here
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 0,
      ""2"": 0,
      ""3"": 0,
      ""4"": 0,
      ""5"": 0
    },
    ""top_enjoyed_aspects"": [
      ""Placeholder Enjoyed Aspect 1"",
      ""Placeholder Enjoyed Aspect 2""
    ],
    ""top_improvement_suggestions"": [
      ""Placeholder Suggestion 1"",
      ""Placeholder Suggestion 2""
    ],
    ""recommendation_for_next_year"": ""Placeholder Recommendation""
  }
}
```
*(Remember to replace `""Your Name""` and `""YYYY-MM-DD""` with your actual name and the date of submission.)*

---

**Scenario Context (Applies to both tasks):**
You are a new Fitness and Wellness Coordinator at ""Innovatech Solutions,"" a tech company with 200 employees. Management is keen to improve employee well-being.

---

**Task 1: New Program Interest Survey Design (Suggested time: 45 minutes)**

Innovatech Solutions is considering introducing a ""Mid-day Mindfulness & Stretching Break"" program. This would involve a 15-20 minute guided session offered twice a week. Before launching, they want to gauge employee interest and preferences.

**Your Task:**
Design a short survey to send to Innovatech employees. In the `task_1_interest_survey_design` section of your `test_submission.json` file, provide the following:

1.  `survey_title`: A suitable title for your survey.
2.  `target_audience_description`: A brief description of who this survey is for (e.g., ""All employees of Innovatech Solutions"").
3.  `survey_introduction_text`: A brief introductory paragraph for the survey (2-3 sentences) explaining its purpose to employees and encouraging participation.
4.  `survey_questions`: A list of **5 to 7 distinct questions**. For each question, you must specify:
    *   `question_text`: The exact wording of the question.
    *   `question_type`: Choose one type from the following list:
        *   ""Multiple Choice - Single Answer""
        *   ""Multiple Choice - Multiple Answers""
        *   ""Likert Scale (1-5)"" (where 1 is low, 5 is high)
        *   ""Open-ended Text""
    *   `options`: A list of predefined answer options. This is required for ""Multiple Choice"" and ""Likert Scale"" questions. For ""Open-ended Text"" questions, provide an empty list `[]`.

**Example Question Structure for your JSON:**
```json
// This is an example, do not include it as one of your actual questions unless appropriate.
{
    ""question_text"": ""How interested are you in participating in a mid-day mindfulness session?"",
    ""question_type"": ""Likert Scale (1-5)"",
    ""options"": [""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]
}
```
```json
// Example for an open-ended question
{
    ""question_text"": ""What other wellness activities would you be interested in?"",
    ""question_type"": ""Open-ended Text"",
    ""options"": []
}
```

---

**Task 2: Existing Program Satisfaction Data Analysis (Suggested time: 45 minutes)**

Innovatech Solutions recently concluded its annual ""Step Challenge."" 50 employees participated. A short satisfaction survey was sent out, and 30 responses were collected. The raw data from these 15 of these responses is provided below.

**Provided Data (`step_challenge_satisfaction_data`):**
The data is comma-separated (CSV format). The columns are: `ParticipantID`, `OverallSatisfaction(1-5 Likert; 5=Very Satisfied)`, `EnjoyedMost`, `SuggestionForImprovement`.

```
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Tracking daily progress,More frequent updates on leaderboard
P06,2,N/A,More prize categories
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,Better mobile app
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```

**Your Task:**
1.  Import or copy this data into your spreadsheet program.
2.  Analyze the data to identify the required information below.
3.  In the `task_2_satisfaction_data_analysis` section of your `test_submission.json` file, provide the following:
    *   `satisfaction_rating_summary`: An object where keys are the satisfaction ratings (""1"", ""2"", ""3"", ""4"", ""5"") and values are the **counts** of how many participants gave each rating.
    *   `top_enjoyed_aspects`: A list containing the **top 2** most frequently mentioned aspects employees ""Enjoyed Most"". Use the exact phrasing as it appears in the data.
    *   `top_improvement_suggestions`: A list containing the **top 2** most frequently mentioned ""Suggestions For Improvement"". Use the exact phrasing as it appears in the data. Exclude ""N/A"" from this list if it appears as a frequent suggestion.
    *   `recommendation_for_next_year`: Based on your analysis of the provided data, formulate **one concrete recommendation** (1-2 sentences) for improving next year's Step Challenge.

---

**End of Exam Instructions. Good luck!**","The exam requires specific data for Task 2.

<MATERIALS_FOR_CANDIDATE>
The following data is provided to the candidate for **Task 2: Existing Program Satisfaction Data Analysis**. This data should be presented to the candidate as `step_challenge_satisfaction_data`.

```csv
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Feeling healthier,More frequent updates on leaderboard
P06,2,N/A,Shorter duration
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,More variety in challenges
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```
</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**Material Provided:**
The material for Task 2 is a CSV formatted dataset representing 15 responses to a satisfaction survey for an existing ""Step Challenge"" program. The columns are:
*   `ParticipantID`: A unique identifier for each respondent.
*   `OverallSatisfaction(1-5 Likert; 5=Very Satisfied)`: A numerical rating from 1 to 5.
*   `EnjoyedMost`: A text field describing what the participant enjoyed most.
*   `SuggestionForImprovement`: A text field with suggestions for improvement.

**How the Material Was Created (and Expected Analysis for Task 2):**
The data was constructed to have specific, predictable patterns to allow for objective evaluation of basic data analysis skills.

1.  **`satisfaction_rating_summary`**:
    The candidate needs to count the occurrences of each satisfaction rating.
    *   Rating 1: 1 occurrence (P12)
    *   Rating 2: 1 occurrence (P06)
    *   Rating 3: 3 occurrences (P03, P09, P15)
    *   Rating 4: 5 occurrences (P01, P05, P08, P11, P14)
    *   Rating 5: 5 occurrences (P02, P04, P07, P10, P13)
    *   **Expected JSON output:**
        ```json
        ""satisfaction_rating_summary"": {
          ""1"": 1,
          ""2"": 1,
          ""3"": 3,
          ""4"": 5,
          ""5"": 5
        }
        ```

2.  **`top_enjoyed_aspects`**:
    The candidate needs to identify the two most frequently mentioned items in the `EnjoyedMost` column. They should use the exact phrasing from the data.
    *   ""Team competition"": 5 times (P01, P04, P07, P10, P13)
    *   ""Tracking daily progress"": 3 times (P02, P08, P14)
    *   ""Feeling healthier"": 3 times (P05, P09, P15)
    *   ""N/A"": 2 times (P06, P11)
    *   ""Nothing specific"": 1 time (P03)
    *   ""Nothing"": 1 time (P12)
    *   **Handling Ties:** ""Team competition"" is clearly the most frequent. There is a tie for the second most frequent aspect between ""Tracking daily progress"" and ""Feeling healthier"" (both 3 occurrences).
    *   **Expected JSON output:** The first item must be ""Team competition"". For the second item, either ""Tracking daily progress"" OR ""Feeling healthier"" is acceptable.
        Example acceptable outputs:
        `[""Team competition"", ""Tracking daily progress""]`
        OR
        `[""Team competition"", ""Feeling healthier""]`
        The candidate should list exactly two items as per the JSON structure provided in the instructions.

3.  **`top_improvement_suggestions`**:
    The candidate needs to identify the two most frequently mentioned items in the `SuggestionForImprovement` column, **excluding ""N/A"" responses**. They should use the exact phrasing.
    *   ""More prize categories"": 3 times (P01, P07, P14)
    *   ""Better mobile app"": 2 times (P04, P10)
    *   ""Shorter duration"": 2 times (P06, P15)
    *   ""N/A"": 2 times (P08, P13) - *To be excluded from top suggestions per instructions.*
    *   ""Clearer rules on activity conversion"": 1 time (P02)
    *   ""Too long"": 1 time (P03)
    *   ""More frequent updates on leaderboard"": 1 time (P05)
    *   ""Make it optional to be on a team"": 1 time (P09)
    *   ""More variety in challenges"": 1 time (P11)
    *   ""Too competitive"": 1 time (P12)
    *   **Handling Ties:** ""More prize categories"" is clearly the most frequent suggestion (3 occurrences). There is a tie for the second most frequent suggestion between ""Better mobile app"" and ""Shorter duration"" (both 2 occurrences).
    *   **Expected JSON output:** The first item must be ""More prize categories"". For the second item, either ""Better mobile app"" OR ""Shorter duration"" is acceptable.
        Example acceptable outputs:
        `[""More prize categories"", ""Better mobile app""]`
        OR
        `[""More prize categories"", ""Shorter duration""]`

4.  **`recommendation_for_next_year`**:
    This is a qualitative answer but must be based on the analysis of the provided data, particularly the `top_improvement_suggestions`. It should be a concrete recommendation (1-2 sentences).
    *   A good recommendation would address one or both of the top improvement suggestions. For example: ""For next year's Step Challenge, consider increasing the number or variety of prize categories, as this was the most common suggestion. Additionally, exploring improvements to the mobile app or adjusting the challenge duration could enhance participant satisfaction.""
    *   Evaluation should focus on whether the recommendation is actionable, relevant to the data findings, and clearly articulated.

**Tricky Elements & Common Pitfalls:**
*   **Handling Ties:** As noted above, there are ties for the second ""top"" item in both `top_enjoyed_aspects` and `top_improvement_suggestions`. The evaluator should accept either of the tied items for the second position in the list.
*   **Exact Phrasing:** Candidates are instructed to use ""exact phrasing"". Minor deviations (e.g., capitalization changes if not consistent in source, though it is here) might be acceptable, but significant rephrasing or typos that change meaning should be marked down.
*   **Excluding ""N/A"" for Suggestions:** Candidates must remember to exclude ""N/A"" from the `top_improvement_suggestions` count, as per the instructions. If they include it, ""N/A"" (2 occurrences) would tie for the second/third spot, altering their list.
*   **Distinguishing Similar Items:** For example, ""Too long"" (1 occurrence) and ""Shorter duration"" (2 occurrences) are semantically related but textually distinct. Per instructions (""exact phrasing""), they should be counted separately.
*   **Counting Accuracy:** Simple counting errors are possible. The dataset is small to minimize this but still requires care.

This data structure and the expected outcomes are designed to test basic data handling and interpretation skills using a spreadsheet program.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
The following data is provided to the candidate for **Task 2: Existing Program Satisfaction Data Analysis**. This data should be presented to the candidate as `step_challenge_satisfaction_data`.

```csv
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Feeling healthier,More frequent updates on leaderboard
P06,2,N/A,Shorter duration
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,More variety in challenges
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```
","To ensure your exam submission is correctly evaluated, please adhere strictly to the following JSON format for your `test_submission.json` file.

**File Name:** `test_submission.json`

**General JSON Structure:**

The entire submission must be a single JSON object. All text values must be enclosed in double quotes. Ensure your JSON is valid before submission; you can use an online JSON validator.

```json
{
  ""candidate_name"": ""YOUR_FULL_NAME"",
  ""candidate_id"": ""YOUR_ID_HERE"",
  ""submission_date"": ""YYYY-MM-DD"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    // Structure for Task 1 answers
  },
  ""task_2_satisfaction_data_analysis"": {
    // Structure for Task 2 answers
  }
}
```

**Field Explanations and Format Requirements:**

1.  **`candidate_name`**:
    *   **Type**: `String`
    *   **Format**: Your full name (e.g., ""Jane Doe"").

2.  **`candidate_id`**:
    *   **Type**: `String`
    *   **Format**: The model version that is powering your responses (e.g., ""GPT-4o"", ""Claude-3-Opus"", ""Gemini-1.5-Pro"", etc.). Replace ""YOUR_ID_HERE"" with this value.

3.  **`submission_date`**:
    *   **Type**: `String`
    *   **Format**: The date of submission in `YYYY-MM-DD` format (e.g., ""2024-07-28"").

4.  **`exam_version`**:
    *   **Type**: `String`
    *   **Format**: This field should contain the exact value: `""Basic_FWC_NeedsAssessment_v1""`.

5.  **`task_1_interest_survey_design`**:
    *   **Type**: `Object`
    *   This object contains your answers for Task 1.

    ```json
    ""task_1_interest_survey_design"": {
      ""survey_title"": ""YOUR_SURVEY_TITLE"",
      ""target_audience_description"": ""YOUR_TARGET_AUDIENCE_DESCRIPTION"",
      ""survey_introduction_text"": ""YOUR_SURVEY_INTRODUCTION_TEXT"",
      ""survey_questions"": [
        // Array of 5 to 7 question objects
      ]
    }
    ```

    *   **`survey_title`**:
        *   **Type**: `String`
        *   **Format**: A concise and descriptive title for your survey.
    *   **`target_audience_description`**:
        *   **Type**: `String`
        *   **Format**: A brief description of the intended recipients of the survey.
    *   **`survey_introduction_text`**:
        *   **Type**: `String`
        *   **Format**: The introductory text for the survey (2-3 sentences).
    *   **`survey_questions`**:
        *   **Type**: `Array` of `Object`
        *   **Format**: A list containing 5 to 7 distinct question objects. Each object represents one survey question and must have the following structure:
            ```json
            {
              ""question_text"": ""EXACT_WORDING_OF_THE_QUESTION"",
              ""question_type"": ""CHOSEN_QUESTION_TYPE"",
              ""options"": [
                // Array of strings for options, or empty array
              ]
            }
            ```
        *   **`question_text`**:
            *   **Type**: `String`
            *   **Format**: The exact wording of the survey question.
        *   **`question_type`**:
            *   **Type**: `String`
            *   **Format**: Must be one of the following exact strings:
                *   `""Multiple Choice - Single Answer""`
                *   `""Multiple Choice - Multiple Answers""`
                *   `""Likert Scale (1-5)""`
                *   `""Open-ended Text""`
        *   **`options`**:
            *   **Type**: `Array` of `String`
            *   **Format**:
                *   For `""Multiple Choice - Single Answer""` and `""Multiple Choice - Multiple Answers""`: A list of strings representing the answer choices (e.g., `[""Option 1"", ""Option 2"", ""Option 3""]`).
                *   For `""Likert Scale (1-5)""`: A list of 5 strings representing the scale points, clearly indicating the meaning of each point (e.g., `[""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]`).
                *   For `""Open-ended Text""`: An empty array `[]`.

6.  **`task_2_satisfaction_data_analysis`**:
    *   **Type**: `Object`
    *   This object contains your answers for Task 2.

    ```json
    ""task_2_satisfaction_data_analysis"": {
      ""satisfaction_rating_summary"": {
        ""1"": 0, // Integer count
        ""2"": 0, // Integer count
        ""3"": 0, // Integer count
        ""4"": 0, // Integer count
        ""5"": 0  // Integer count
      },
      ""top_enjoyed_aspects"": [
        ""ASPECT_1_TEXT"", // String
        ""ASPECT_2_TEXT""  // String
      ],
      ""top_improvement_suggestions"": [
        ""SUGGESTION_1_TEXT"", // String
        ""SUGGESTION_2_TEXT""  // String
      ],
      ""recommendation_for_next_year"": ""YOUR_RECOMMENDATION_TEXT"" // String
    }
    ```

    *   **`satisfaction_rating_summary`**:
        *   **Type**: `Object`
        *   **Format**: Keys must be strings `""1""`, `""2""`, `""3""`, `""4""`, `""5""`. Values must be `Integer` representing the count of participants for each satisfaction rating.
    *   **`top_enjoyed_aspects`**:
        *   **Type**: `Array` of `String`
        *   **Format**: A list containing exactly two (2) strings. These strings must be the exact phrasing of the top 2 most frequently mentioned aspects from the `EnjoyedMost` column in the provided data.
    *   **`top_improvement_suggestions`**:
        *   **Type**: `Array` of `String`
        *   **Format**: A list containing exactly two (2) strings. These strings must be the exact phrasing of the top 2 most frequently mentioned suggestions from the `SuggestionForImprovement` column in the provided data. Exclude ""N/A"" when determining these top suggestions.
    *   **`recommendation_for_next_year`**:
        *   **Type**: `String`
        *   **Format**: Your single, concrete recommendation (1-2 sentences) based on your analysis of the provided data.

**Example `test_submission.json` (Illustrative Structure - Do NOT use these placeholder values):**

```json
{
  ""candidate_name"": ""Your Name"",
  ""candidate_id"": ""GPT-4o"",
  ""submission_date"": ""2024-07-28"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Mid-day Wellness Break Interest"",
    ""target_audience_description"": ""All Innovatech Solutions employees"",
    ""survey_introduction_text"": ""We are exploring a new mid-day wellness break and would love your input. Please take a few minutes to complete this short survey."",
    ""survey_questions"": [
      {
        ""question_text"": ""How interested are you in a 15-20 minute guided mindfulness and stretching break during the workday?"",
        ""question_type"": ""Likert Scale (1-5)"",
        ""options"": [""1 - Not at all interested"", ""2 - Slightly interested"", ""3 - Moderately interested"", ""4 - Very interested"", ""5 - Extremely interested""]
      },
      {
        ""question_text"": ""What time of day would be most convenient for you to participate? (Select all that apply)"",
        ""question_type"": ""Multiple Choice - Multiple Answers"",
        ""options"": [""11:00 AM - 11:30 AM"", ""12:30 PM - 1:00 PM"", ""2:00 PM - 2:30 PM"", ""Other (please specify)""]
      },
      {
        ""question_text"": ""If you selected 'Other' for the previous question, please specify your preferred time:"",
        ""question_type"": ""Open-ended Text"",
        ""options"": []
      }
      // ... up to 5-7 questions
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 1,
      ""2"": 1,
      ""3"": 3,
      ""4"": 5,
      ""5"": 5
    },
    ""top_enjoyed_aspects"": [
      ""Team competition"",
      ""Tracking daily progress""
    ],
    ""top_improvement_suggestions"": [
      ""More prize categories"",
      ""Better mobile app""
    ],
    ""recommendation_for_next_year"": ""Consider adding more prize categories and improving the mobile app for next year's Step Challenge to increase engagement.""
  }
}
```

**Supplementary Files:**
No supplementary files are required. All answers must be contained within the `test_submission.json` file.","Okay, here are the evaluation materials for the Basic Practical Exam, including the JSON answer key, explanation of correct answers, and passing criteria.

## 1. Complete Answer Key (JSON Format)

Filename: `basic_exam_answer_key.json`

```json
{
  ""candidate_name"": ""Official Answer Key"",
  ""candidate_id"": ""Official_Answer_Key_v1.0"",
  ""submission_date"": ""2024-07-29"",
  ""exam_version"": ""Basic_FWC_NeedsAssessment_v1"",
  ""task_1_interest_survey_design"": {
    ""survey_title"": ""Interest in Mid-day Mindfulness & Stretching Breaks at Innovatech"",
    ""target_audience_description"": ""All employees of Innovatech Solutions."",
    ""survey_introduction_text"": ""Innovatech Solutions is exploring a new 'Mid-day Mindfulness & Stretching Break' program to support employee well-being. These 15-20 minute guided sessions would be offered twice a week. Your anonymous feedback is valuable in helping us understand interest and preferences. Please take a few minutes to complete this short survey."",
    ""survey_questions"": [
      {
        ""question_text"": ""How interested are you in participating in a 15-20 minute guided 'Mid-day Mindfulness & Stretching Break' offered twice a week at Innovatech Solutions?"",
        ""question_type"": ""Likert Scale (1-5)"",
        ""options"": [
          ""1 - Not at all interested"",
          ""2 - Slightly interested"",
          ""3 - Moderately interested"",
          ""4 - Very interested"",
          ""5 - Extremely interested""
        ]
      },
      {
        ""question_text"": ""Which days of the week would you most likely be able to attend such a session? (Select all that apply)"",
        ""question_type"": ""Multiple Choice - Multiple Answers"",
        ""options"": [""Monday"", ""Tuesday"", ""Wednesday"", ""Thursday"", ""Friday""]
      },
      {
        ""question_text"": ""What time slot(s) during the mid-day (approx. 11 AM - 2 PM) would best suit you for these sessions? (Select all that apply)"",
        ""question_type"": ""Multiple Choice - Multiple Answers"",
        ""options"": [""11:30 AM - 11:50 AM"", ""12:30 PM - 12:50 PM"", ""1:30 PM - 1:50 PM"", ""Other (please specify if different)""]
      },
      {
        ""question_text"": ""To what extent do you feel a mid-day break for mindfulness and stretching would help improve your personal well-being or productivity?"",
        ""question_type"": ""Likert Scale (1-5)"",
        ""options"": [
          ""1 - Not at all helpful"",
          ""2 - Slightly helpful"",
          ""3 - Moderately helpful"",
          ""4 - Very helpful"",
          ""5 - Extremely helpful""
        ]
      },
      {
        ""question_text"": ""What, if any, are your primary concerns or potential barriers to participating in such a program? (e.g., workload, meeting schedules, not interested, physical limitations)"",
        ""question_type"": ""Open-ended Text"",
        ""options"": []
      },
      {
        ""question_text"": ""If such sessions were offered, would you prefer more emphasis on mindfulness, stretching, or an equal balance of both?"",
        ""question_type"": ""Multiple Choice - Single Answer"",
        ""options"": [""Primarily Mindfulness"", ""Primarily Stretching"", ""Equal Balance of Both"", ""Unsure""]
      }
    ]
  },
  ""task_2_satisfaction_data_analysis"": {
    ""satisfaction_rating_summary"": {
      ""1"": 1,
      ""2"": 1,
      ""3"": 3,
      ""4"": 5,
      ""5"": 5
    },
    ""top_enjoyed_aspects"": [
      ""Team competition"",
      ""Tracking daily progress""
    ],
    ""top_improvement_suggestions"": [
      ""More prize categories"",
      ""Better mobile app""
    ],
    ""recommendation_for_next_year"": ""For next year's Step Challenge, Innovatech Solutions should consider increasing the number or variety of prize categories, as this was a prominent suggestion. Additionally, exploring improvements to the mobile app or offering a shorter challenge duration could enhance participant satisfaction and address other common feedback points.""
  }
}
```

## 2. Explanation of Correct Answers and How They Were Derived

### Task 1: New Program Interest Survey Design

The goal of Task 1 is to assess the candidate's ability to create a relevant, clear, and well-structured short survey to gauge interest in a new wellness program. The provided answer key offers an *exemplar* solution.

*   **`survey_title`**: Should be concise, descriptive, and relevant to the ""Mid-day Mindfulness & Stretching Break"" and the company context (Innovatech).
    *   *Exemplar Rationale*: ""Interest in Mid-day Mindfulness & Stretching Breaks at Innovatech"" is clear and informative.
*   **`target_audience_description`**: Should accurately identify the survey recipients.
    *   *Exemplar Rationale*: ""All employees of Innovatech Solutions."" is direct and correct given the scenario.
*   **`survey_introduction_text`**: Should be welcoming, state the survey's purpose (gauging interest for the new program), mention anonymity if applicable (good practice), and encourage participation. It should be 2-3 sentences.
    *   *Exemplar Rationale*: The exemplar text explains the program, its purpose, assures anonymity (implied by ""Your anonymous feedback""), and encourages participation.
*   **`survey_questions`**: The candidate must provide 5-7 distinct questions. Each question is evaluated on:
    *   **Relevance**: Does it help gauge interest or preferences for the ""Mid-day Mindfulness & Stretching Break""?
    *   **Clarity**: Is the question easy to understand?
    *   **Question Type**: Is the chosen type appropriate for the information sought?
    *   **Options (if applicable)**: Are the options comprehensive and well-phrased?
    *   **JSON Structure**: Is it correctly formatted?

    **Rationale for Exemplar Questions:**
    1.  **Interest Level (Likert Scale)**: Directly measures interest, which is the primary goal. Likert scale provides nuanced data.
    2.  **Preferred Days (Multiple Choice - Multiple Answers)**: Practical for scheduling; allows multiple selections as preferences may vary.
    3.  **Preferred Times (Multiple Choice - Multiple Answers)**: Crucial for logistics; allows multiple selections. ""Other"" option captures unlisted preferences.
    4.  **Current Perceived Need/Benefit (Likert Scale)**: Helps understand the perceived value proposition from the employees' perspective.
    5.  **Potential Barriers (Open-ended Text)**: Identifies potential obstacles to participation, allowing for proactive planning. Open-ended allows for diverse, unprompted feedback.
    6.  **Focus Preference (Multiple Choice - Single Answer)**: Gathers input on program content, helping tailor sessions to employee preferences.

    *Evaluation Notes for Task 1:*
    *   The number of questions must be between 5 and 7.
    *   A good mix of question types is desirable.
    *   Questions should logically flow and cover key aspects like interest, logistics (timing/days), perceived benefits, and potential barriers.

### Task 2: Existing Program Satisfaction Data Analysis

The goal of Task 2 is to assess the candidate's ability to perform basic data analysis on a small dataset using a spreadsheet and report findings in the specified JSON format.

**Provided Data for Analysis:**
```csv
ParticipantID,OverallSatisfaction(1-5 Likert; 5=Very Satisfied),EnjoyedMost,SuggestionForImprovement
P01,4,Team competition,More prize categories
P02,5,Tracking daily progress,Clearer rules on activity conversion
P03,3,Nothing specific,Too long
P04,5,Team competition,Better mobile app
P05,4,Feeling healthier,More frequent updates on leaderboard
P06,2,N/A,Shorter duration
P07,5,Team competition,More prize categories
P08,4,Tracking daily progress,N/A
P09,3,Feeling healthier,Make it optional to be on a team
P10,5,Team competition,Better mobile app
P11,4,N/A,More variety in challenges
P12,1,Nothing,Too competitive
P13,5,Team competition,N/A
P14,4,Tracking daily progress,More prize categories
P15,3,Feeling healthier,Shorter duration
```

1.  **`satisfaction_rating_summary`**:
    *   Count occurrences of each rating in the `OverallSatisfaction` column:
        *   Rating ""1"": 1 (P12)
        *   Rating ""2"": 1 (P06)
        *   Rating ""3"": 3 (P03, P09, P15)
        *   Rating ""4"": 5 (P01, P05, P08, P11, P14)
        *   Rating ""5"": 5 (P02, P04, P07, P10, P13)
    *   **Correct JSON**: `{""1"": 1, ""2"": 1, ""3"": 3, ""4"": 5, ""5"": 5}`

2.  **`top_enjoyed_aspects`**:
    *   Count frequencies in the `EnjoyedMost` column (using exact phrasing):
        *   ""Team competition"": 5 times
        *   ""Tracking daily progress"": 3 times
        *   ""Feeling healthier"": 3 times
        *   ""N/A"": 2 times
        *   ""Nothing specific"": 1 time
        *   ""Nothing"": 1 time
    *   The top aspect is ""Team competition"" (5 occurrences).
    *   There is a tie for the second top aspect: ""Tracking daily progress"" (3 occurrences) and ""Feeling healthier"" (3 occurrences).
    *   **Correct JSON**: The list must contain ""Team competition"" and one of the tied second-place items.
        *   Example 1 (used in key): `[""Team competition"", ""Tracking daily progress""]`
        *   Example 2 (also correct): `[""Team competition"", ""Feeling healthier""]`
        *   The order of the two items in the list does not strictly matter as long as both are correct top 2. However, listing the most frequent first is good practice.

3.  **`top_improvement_suggestions`**:
    *   Count frequencies in the `SuggestionForImprovement` column (using exact phrasing, **excluding ""N/A""**):
        *   ""More prize categories"": 3 times
        *   ""Better mobile app"": 2 times
        *   ""Shorter duration"": 2 times
        *   ""Clearer rules on activity conversion"": 1 time
        *   ""Too long"": 1 time
        *   ""More frequent updates on leaderboard"": 1 time
        *   ""Make it optional to be on a team"": 1 time
        *   ""More variety in challenges"": 1 time
        *   ""Too competitive"": 1 time
        *   (""N/A"" appeared 2 times but is excluded from this analysis per instructions).
    *   The top suggestion is ""More prize categories"" (3 occurrences).
    *   There is a tie for the second top suggestion: ""Better mobile app"" (2 occurrences) and ""Shorter duration"" (2 occurrences).
    *   **Correct JSON**: The list must contain ""More prize categories"" and one of the tied second-place items.
        *   Example 1 (used in key): `[""More prize categories"", ""Better mobile app""]`
        *   Example 2 (also correct): `[""More prize categories"", ""Shorter duration""]`

4.  **`recommendation_for_next_year`**:
    *   Must be a concrete recommendation (1-2 sentences) based on the analysis, particularly the `top_improvement_suggestions`.
    *   *Exemplar Rationale*: The key's recommendation directly addresses the top identified suggestions (""More prize categories,"" ""Better mobile app,"" and implicitly ""Shorter duration"" by mentioning it as an alternative). It is actionable and clearly linked to the data.
    *   *Evaluation*: Should be logical, derived from the data, and practical.

## 3. Passing Criteria

**Total Possible Points: 100**

**Task 1: New Program Interest Survey Design (Total 40 points)**
*   `survey_title` (2 points): Relevant and clear.
*   `target_audience_description` (2 points): Accurate.
*   `survey_introduction_text` (6 points):
    *   Clarity and purpose (3 points)
    *   Tone and encouragement (3 points)
*   `survey_questions` (30 points total):
    *   **Quantity (Fulfilled if 5-7 questions)**: This is a prerequisite for full scoring of questions. If not met, deduct proportionally or cap points.
    *   **For each question (e.g., 6 questions x 5 points each = 30 points; if 5 questions, 6 pts each; if 7 questions, approx 4.3 pts each):**
        *   Relevance to scenario (2 points per question)
        *   Appropriateness of question type & options (if applicable) (2 points per question)
        *   Clarity of question text (1 point per question)
    *   Correct JSON structure for all Task 1 elements is expected. Minor syntax errors might be penalized slightly, but focus is on content.

**Task 2: Existing Program Satisfaction Data Analysis (Total 60 points)**
*   `satisfaction_rating_summary` (20 points):
    *   4 points for each correctly counted rating (""1"" through ""5""). (5 ratings x 4 points = 20 points)
*   `top_enjoyed_aspects` (15 points):
    *   Correct identification of ""Team competition"" as one of the top 2 (7.5 points).
    *   Correct identification of either ""Tracking daily progress"" OR ""Feeling healthier"" as the other top 2 aspect (7.5 points).
    *   Exact phrasing from data required.
*   `top_improvement_suggestions` (15 points):
    *   Correct identification of ""More prize categories"" as one of the top 2 (7.5 points).
    *   Correct identification of either ""Better mobile app"" OR ""Shorter duration"" as the other top 2 aspect (7.5 points).
    *   Exact phrasing from data required, and ""N/A"" must be correctly excluded from consideration.
*   `recommendation_for_next_year` (10 points):
    *   Directly based on and supported by the analyzed data (especially `top_improvement_suggestions`) (5 points).
    *   Clear, concrete, and actionable (1-2 sentences) (5 points).
*   Correct JSON structure for all Task 2 elements is expected.

**Overall Passing Score:**
*   **Pass:** 70 points or higher (out of 100).
*   **Pass with Merit/Exceeds Expectations (Optional):** 85 points or higher.

**General Evaluation Notes:**
*   **JSON Validity:** The submitted `test_submission.json` file must be valid JSON. If it's not, the candidate may fail or receive a significant penalty, as this is a core requirement.
*   **Adherence to Instructions:** Points may be deducted for not following specific instructions (e.g., number of questions, format of options, exact phrasing where required).
*   **Practical Application:** The focus is on the practical ability to perform the tasks. Theoretical knowledge is not directly assessed but underpins practical execution.
*   **Time Limit:** While not directly scored, the 90-minute time limit is a factor in assessing if the candidate can perform these basic tasks efficiently.","```python
import json
import sys
import re
import argparse

# --- Configuration for Scoring ---
TASK1_MAX_SCORE = 40.0
TASK1_TITLE_MAX = 2.0
TASK1_AUDIENCE_MAX = 2.0
TASK1_INTRO_MAX = 6.0
TASK1_QUESTIONS_MAX = 30.0

TASK2_MAX_SCORE = 60.0
TASK2_SATISFACTION_SUMMARY_MAX = 20.0
TASK2_TOP_ENJOYED_MAX = 15.0
TASK2_TOP_IMPROVEMENT_MAX = 15.0
TASK2_RECOMMENDATION_MAX = 10.0

TOTAL_EXAM_MAX_SCORE = TASK1_MAX_SCORE + TASK2_MAX_SCORE

# Keywords for Task 1 (case-insensitive)
TITLE_KEYWORDS = [""survey"", ""interest"", ""mindfulness"", ""stretching"", ""innovatech"", ""wellness"", ""feedback""]
AUDIENCE_KEYWORDS = [""employee"", ""innovatech"", ""staff"", ""personnel"", ""all""]
INTRO_PURPOSE_KEYWORDS = [""feedback"", ""purpose"", ""survey"", ""input"", ""opinion"", ""understand"", ""gauge"", ""assess"", ""mindfulness"", ""stretching"", ""wellness"", ""program"", ""initiative""]
INTRO_TONE_KEYWORDS = [""please"", ""thank you"", ""valuable"", ""appreciate"", ""confidential"", ""anonymous"", ""help"", ""invite"", ""encourage""]
QUESTION_RELEVANCE_KEYWORDS = [""mindfulness"", ""stretching"", ""break"", ""interest"", ""schedule"", ""barrier"", ""preference"", ""well-being"", ""time"", ""day"", ""frequency"", ""duration"", ""participate"", ""benefit"", ""concern"", ""content"", ""format""]
ALLOWED_QUESTION_TYPES = [
    ""Multiple Choice - Single Answer"",
    ""Multiple Choice - Multiple Answers"",
    ""Likert Scale (1-5)"",
    ""Open-ended Text""
]

# Keywords for Task 2 Recommendation (case-insensitive)
RECOMMENDATION_ACTION_KEYWORDS = [""consider"", ""improve"", ""add"", ""change"", ""explore"", ""recommend"", ""suggest"", ""implement"", ""offer"", ""provide"", ""ensure"", ""review"", ""adjust""]


def get_nested_value(data_dict, path, default=None):
    """"""Safely access nested dictionary values.""""""
    current = data_dict
    for key in path:
        if isinstance(current, dict) and key in current:
            current = current[key]
        elif isinstance(current, list) and isinstance(key, int) and 0 <= key < len(current):
            current = current[key]
        else:
            return default
    return current

def contains_keywords(text, keywords, N=1):
    """"""Check if text contains at least N keywords (case-insensitive).""""""
    if not isinstance(text, str) or not text:
        return False
    text_lower = text.lower()
    found_count = 0
    for kw in keywords:
        if kw.lower() in text_lower:
            found_count += 1
            if found_count >= N:
                return True
    return False

def score_task1_survey_title(candidate_title):
    score = 0
    comment = ""Title: ""
    if candidate_title and isinstance(candidate_title, str) and len(candidate_title.strip()) > 0:
        if contains_keywords(candidate_title, TITLE_KEYWORDS):
            score = TASK1_TITLE_MAX
            comment += ""Relevant and clear title.""
        else:
            score = TASK1_TITLE_MAX / 2
            comment += ""Title present but could be more relevant or descriptive.""
    else:
        comment += ""Missing or empty.""
    return score, comment

def score_task1_target_audience(candidate_desc):
    score = 0
    comment = ""Target Audience: ""
    if candidate_desc and isinstance(candidate_desc, str) and len(candidate_desc.strip()) > 0:
        if contains_keywords(candidate_desc, AUDIENCE_KEYWORDS):
            score = TASK1_AUDIENCE_MAX
            comment += ""Accurate description.""
        else:
            score = TASK1_AUDIENCE_MAX / 2
            comment += ""Description present but could be more specific or accurate.""
    else:
        comment += ""Missing or empty.""
    return score, comment

def score_task1_survey_introduction(candidate_intro):
    score = 0
    comments = []
    # Clarity and purpose (3 points)
    purpose_score = 0
    if candidate_intro and isinstance(candidate_intro, str) and len(candidate_intro.strip()) > 15: # Min length
        if contains_keywords(candidate_intro, INTRO_PURPOSE_KEYWORDS, N=2):
            purpose_score = 3.0
            comments.append(""Clear purpose."")
        elif contains_keywords(candidate_intro, INTRO_PURPOSE_KEYWORDS, N=1):
            purpose_score = 1.5
            comments.append(""Purpose somewhat clear."")
        else:
            comments.append(""Purpose not clearly stated."")
    else:
        comments.append(""Introduction missing, too short, or not a string."")
    score += purpose_score

    # Tone and encouragement (3 points)
    tone_score = 0
    if candidate_intro and isinstance(candidate_intro, str): # Already checked for presence
        if contains_keywords(candidate_intro, INTRO_TONE_KEYWORDS, N=1):
            tone_score = 3.0
            comments.append(""Good tone and encouragement."")
        elif purpose_score > 0 : # If intro is somewhat valid
            tone_score = 1.5
            comments.append(""Tone could be more encouraging."")
        else:
            comments.append(""Tone and encouragement lacking."")
    score += tone_score
    
    return score, ""Introduction: "" + "" "".join(comments)

def score_task1_survey_questions(candidate_questions_list):
    total_score = 0
    question_details_report = []
    num_q_submitted = 0
    quantity_comment = """"

    if not isinstance(candidate_questions_list, list):
        quantity_comment = ""Survey questions not provided as a list.""
        return 0, quantity_comment, question_details_report, num_q_submitted

    num_q_submitted = len(candidate_questions_list)

    if num_q_submitted == 0:
        quantity_comment = ""No survey questions provided.""
        return 0, quantity_comment, question_details_report, num_q_submitted

    # Points per sub-criterion if 6 questions were submitted (ideal for 5 points per question)
    # These are the max points for each sub-criterion for *each* question
    base_relevance_max = 2.0
    base_type_options_max = 2.0
    base_clarity_max = 1.0

    raw_achieved_score = 0

    for i, q_data in enumerate(candidate_questions_list):
        q_text = get_nested_value(q_data, (""question_text"",))
        q_type = get_nested_value(q_data, (""question_type"",))
        q_options = get_nested_value(q_data, (""options"",)) # Default is None, handle if not list

        current_q_relevance_score = 0
        current_q_type_options_score = 0
        current_q_clarity_score = 0
        
        q_report = {
            ""question_index"": i + 1,
            ""text"": q_text if q_text else ""N/A"",
            ""relevance_score"": 0, ""relevance_max"": base_relevance_max, ""relevance_comment"": """",
            ""type_options_score"": 0, ""type_options_max"": base_type_options_max, ""type_options_comment"": """",
            ""clarity_score"": 0, ""clarity_max"": base_clarity_max, ""clarity_comment"": """"
        }

        # Score Relevance
        if q_text and isinstance(q_text, str) and contains_keywords(q_text, QUESTION_RELEVANCE_KEYWORDS):
            current_q_relevance_score = base_relevance_max
            q_report[""relevance_comment""] = ""Relevant.""
        elif q_text and isinstance(q_text, str):
            current_q_relevance_score = base_relevance_max / 2
            q_report[""relevance_comment""] = ""Partially relevant or relevance unclear.""
        else:
            q_report[""relevance_comment""] = ""Question text missing or not relevant.""
        q_report[""relevance_score""] = current_q_relevance_score
        raw_achieved_score += current_q_relevance_score

        # Score Type & Options
        type_options_comments = []
        if q_type and isinstance(q_type, str) and q_type in ALLOWED_QUESTION_TYPES:
            valid_type = True
            if q_type == ""Likert Scale (1-5)"":
                if isinstance(q_options, list) and len(q_options) == 5:
                    current_q_type_options_score = base_type_options_max
                    type_options_comments.append(""Correct Likert scale format."")
                else:
                    current_q_type_options_score = base_type_options_max / 2
                    type_options_comments.append(""Likert scale type, but options incorrect (expected 5)."")
            elif ""Multiple Choice"" in q_type:
                if isinstance(q_options, list) and len(q_options) > 0:
                    current_q_type_options_score = base_type_options_max
                    type_options_comments.append(""Correct Multiple Choice format."")
                else:
                    current_q_type_options_score = base_type_options_max / 2
                    type_options_comments.append(""Multiple Choice type, but options missing or empty."")
            elif q_type == ""Open-ended Text"":
                if isinstance(q_options, list) and len(q_options) == 0:
                    current_q_type_options_score = base_type_options_max
                    type_options_comments.append(""Correct Open-ended format."")
                else: # options should be an empty list
                    current_q_type_options_score = base_type_options_max / 2
                    type_options_comments.append(""Open-ended type, but options field not an empty list."")
        else:
            type_options_comments.append(f""Invalid or missing question type: '{q_type}'."")
        
        q_report[""type_options_score""] = current_q_type_options_score
        q_report[""type_options_comment""] = "" "".join(type_options_comments)
        raw_achieved_score += current_q_type_options_score

        # Score Clarity
        if q_text and isinstance(q_text, str) and len(q_text.split()) >= 5: # At least 5 words
            current_q_clarity_score = base_clarity_max
            q_report[""clarity_comment""] = ""Clear.""
        elif q_text and isinstance(q_text, str) and len(q_text.strip()) > 0:
            current_q_clarity_score = base_clarity_max / 2
            q_report[""clarity_comment""] = ""Somewhat clear or too brief.""
        else:
            q_report[""clarity_comment""] = ""Question text missing or unclear.""
        q_report[""clarity_score""] = current_q_clarity_score
        raw_achieved_score += current_q_clarity_score
        
        question_details_report.append(q_report)

    # Apply scaling based on number of questions
    # The raw_achieved_score is sum of (up to) 5 points per question.
    # Max possible raw score is num_q_submitted * 5.
    # We need to scale this to the 30 point max for the question section.
    
    # If num_q_submitted is ideal (5, 6, or 7), the total score is capped at 30.
    # Each question contributes proportionally.
    if 5 <= num_q_submitted <= 7:
        # Max possible score for these questions is num_q_submitted * 5.
        # We want to scale this to 30.
        # Example: 5 questions, max raw = 25. If they get 20 raw, final = (20/25)*30 = 24.
        # Example: 7 questions, max raw = 35. If they get 28 raw, final = (28/35)*30 = 24.
        if num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max) > 0: # Avoid division by zero
             total_score = (raw_achieved_score / (num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max))) * TASK1_QUESTIONS_MAX
        else:
            total_score = 0
        quantity_comment = f""Correct number of questions ({num_q_submitted}). Score scaled to fit {TASK1_QUESTIONS_MAX} points.""
    elif num_q_submitted < 5 and num_q_submitted > 0:
        # Proportional penalty for too few questions
        # Calculate score as if they submitted 5 questions, then penalize.
        # Max raw score if they had 5 questions = 5 * 5 = 25.
        # Their score relative to this: raw_achieved_score / (num_q_submitted * 5)
        # This proportion applied to 30 points, then penalized.
        base_score_if_ideal_count = 0
        if num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max) > 0:
            base_score_if_ideal_count = (raw_achieved_score / (num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max))) * TASK1_QUESTIONS_MAX
        
        penalty_factor = num_q_submitted / 5.0
        total_score = base_score_if_ideal_count * penalty_factor
        quantity_comment = f""Too few questions ({num_q_submitted} vs 5-7 expected). Base score adjusted by factor {penalty_factor:.2f}.""
    elif num_q_submitted > 7:
        # Proportional penalty for too many questions
        base_score_if_ideal_count = 0
        if num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max) > 0:
            base_score_if_ideal_count = (raw_achieved_score / (num_q_submitted * (base_relevance_max + base_type_options_max + base_clarity_max))) * TASK1_QUESTIONS_MAX

        penalty_factor = 7.0 / num_q_submitted
        total_score = base_score_if_ideal_count * penalty_factor
        quantity_comment = f""Too many questions ({num_q_submitted} vs 5-7 expected). Base score adjusted by factor {penalty_factor:.2f}.""

    total_score = min(total_score, TASK1_QUESTIONS_MAX) # Cap at max
    total_score = max(total_score, 0) # Ensure non-negative

    return total_score, quantity_comment, question_details_report, num_q_submitted


def score_task2_satisfaction_summary(candidate_summary, key_summary):
    score = 0
    comments = []
    details = {}

    if not isinstance(candidate_summary, dict):
        return 0, ""Satisfaction summary not provided as a dictionary."", details

    points_per_rating = TASK2_SATISFACTION_SUMMARY_MAX / 5.0 # 4 points per rating

    for rating in [""1"", ""2"", ""3"", ""4"", ""5""]:
        candidate_count = get_nested_value(candidate_summary, (rating,))
        key_count = get_nested_value(key_summary, (rating,))
        
        details[f""rating_{rating}""] = {""candidate"": candidate_count, ""key"": key_count, ""score"": 0}

        if isinstance(candidate_count, int) and candidate_count == key_count:
            score += points_per_rating
            details[f""rating_{rating}""][""score""] = points_per_rating
            comments.append(f""Rating '{rating}': Correct count ({candidate_count})."")
        elif candidate_count is not None:
            comments.append(f""Rating '{rating}': Incorrect count (Candidate: {candidate_count}, Key: {key_count})."")
            details[f""rating_{rating}""][""score""] = 0
        else:
            comments.append(f""Rating '{rating}': Missing in submission."")
            details[f""rating_{rating}""][""score""] = 0
            
    return score, ""Satisfaction Summary: "" + "" "".join(comments) if comments else ""No valid ratings found."", details

def score_task2_top_list(candidate_list, key_primary, key_secondary_options, list_name):
    score = 0
    comment = f""{list_name}: ""

    if not isinstance(candidate_list, list) or len(candidate_list) != 2:
        comment += ""Not a list of 2 items.""
        return 0, comment

    candidate_set = set(item.strip() if isinstance(item, str) else """" for item in candidate_list)
    
    # Remove empty strings that might result from stripping non-string items
    candidate_set = {s for s in candidate_set if s}
    if len(candidate_set) != 2 : # check if two distinct non-empty strings were provided
        comment += ""Does not contain two distinct non-empty string items.""
        return 0, comment


    valid_sets = [{key_primary.strip(), opt.strip()} for opt in key_secondary_options]

    if candidate_set in valid_sets:
        score = TASK2_TOP_ENJOYED_MAX # Max score for this type of list (15 pts)
        comment += ""Correctly identified top 2 items.""
    else:
        # Partial credit attempt: check if primary is there
        partial_match = False
        if key_primary.strip() in candidate_set:
            score = TASK2_TOP_ENJOYED_MAX / 2
            comment += f""Partially correct: '{key_primary}' identified, but the second item is incorrect or missing from valid options.""
            partial_match = True
        
        # Check if one of the secondary options is there (if primary wasn't)
        if not partial_match:
            for opt in key_secondary_options:
                if opt.strip() in candidate_set:
                    score = TASK2_TOP_ENJOYED_MAX / 2 # Or a smaller partial credit
                    comment += f""Partially correct: One valid item ('{opt}') identified, but the other is incorrect or missing.""
                    partial_match = True
                    break
        if not partial_match:
             comment += ""Incorrect items.""
             
    return score, comment

def score_task2_recommendation(candidate_rec, key_top_improvement_suggestions):
    score = 0
    comments = []

    if not isinstance(candidate_rec, str) or not candidate_rec.strip():
        return 0, ""Recommendation: Missing or empty.""

    # Data-driven (5 pts)
    data_driven_score = 0
    rec_lower = candidate_rec.lower()
    
    # Themes from top improvement suggestions (example themes)
    # key_top_improvement_suggestions is a list like [""More prize categories"", ""Better mobile app""]
    # We need to extract themes from these.
    themes_found = 0
    # Theme 1 (e.g., from ""More prize categories"")
    if any(kw in rec_lower for kw in [""prize"", ""reward"", ""category"", ""categories""]):
        data_driven_score += 2.5
        themes_found +=1
    
    # Theme 2 (e.g., from ""Better mobile app"" or ""Shorter duration"")
    if any(kw in rec_lower for kw in [""app"", ""mobile"", ""duration"", ""shorter"", ""length"", ""time""]):
        if themes_found == 0: # First theme found
             data_driven_score += 2.5
        elif data_driven_score < 5.0: # Second distinct theme
             data_driven_score = 5.0 
        themes_found +=1

    if data_driven_score > 0:
        comments.append(f""Data-driven aspects identified (Score: {data_driven_score}/5)."")
    else:
        comments.append(""Recommendation does not clearly reflect data findings (Score: 0/5)."")
    score += data_driven_score

    # Clarity/Actionability (5 pts)
    clarity_score = 0
    word_count = len(candidate_rec.split())
    if 10 <= word_count <= 70:
        clarity_score += 2.5
        comments.append(f""Appropriate length ({word_count} words)."")
    else:
        comments.append(f""Length not ideal ({word_count} words, expected 10-70)."")

    if contains_keywords(candidate_rec, RECOMMENDATION_ACTION_KEYWORDS):
        clarity_score += 2.5
        comments.append(""Contains actionable language."")
    else:
        comments.append(""Lacks clear actionable language."")
    score += clarity_score
    
    return score, ""Recommendation: "" + "" "".join(comments)


def evaluate_submission(submission_data, key_data):
    results = {
        ""candidate_name"": get_nested_value(submission_data, (""candidate_name"",), ""N/A""),
        ""candidate_id"": get_nested_value(submission_data, (""candidate_id"",), ""N/A""),
        ""overall_score"": 0.0,
        ""task_1_scores"": {
            ""total_task_1_score"": 0,
            ""max_score"": TASK1_MAX_SCORE
        },
        ""task_2_scores"": {
            ""total_task_2_score"": 0,
            ""max_score"": TASK2_MAX_SCORE
        },
        ""total_exam_score"": 0,
        ""max_exam_score"": TOTAL_EXAM_MAX_SCORE
    }

    # --- Task 1 Evaluation ---
    task1_submission = get_nested_value(submission_data, (""task_1_interest_survey_design"",), {})
    
    s, c = score_task1_survey_title(get_nested_value(task1_submission, (""survey_title"",)))
    results[""task_1_scores""][""survey_title""] = {""score"": s, ""max_score"": TASK1_TITLE_MAX, ""comment"": c}
    results[""task_1_scores""][""total_task_1_score""] += s

    s, c = score_task1_target_audience(get_nested_value(task1_submission, (""target_audience_description"",)))
    results[""task_1_scores""][""target_audience_description""] = {""score"": s, ""max_score"": TASK1_AUDIENCE_MAX, ""comment"": c}
    results[""task_1_scores""][""total_task_1_score""] += s

    s, c = score_task1_survey_introduction(get_nested_value(task1_submission, (""survey_introduction_text"",)))
    results[""task_1_scores""][""survey_introduction_text""] = {""score"": s, ""max_score"": TASK1_INTRO_MAX, ""comment"": c}
    results[""task_1_scores""][""total_task_1_score""] += s

    s, c, q_details, num_q = score_task1_survey_questions(get_nested_value(task1_submission, (""survey_questions"",)))
    results[""task_1_scores""][""survey_questions""] = {
        ""score"": s, ""max_score"": TASK1_QUESTIONS_MAX, ""comment"": c,
        ""num_questions_provided"": num_q, ""num_questions_expected"": ""5-7"",
        ""question_details"": q_details
    }
    results[""task_1_scores""][""total_task_1_score""] += s

    # --- Task 2 Evaluation ---
    task2_submission = get_nested_value(submission_data, (""task_2_satisfaction_data_analysis"",), {})
    task2_key = get_nested_value(key_data, (""task_2_satisfaction_data_analysis"",), {})

    s, c, summary_details = score_task2_satisfaction_summary(
        get_nested_value(task2_submission, (""satisfaction_rating_summary"",)),
        get_nested_value(task2_key, (""satisfaction_rating_summary"",))
    )
    results[""task_2_scores""][""satisfaction_rating_summary""] = {""score"": s, ""max_score"": TASK2_SATISFACTION_SUMMARY_MAX, ""comment"": c, ""details"": summary_details}
    results[""task_2_scores""][""total_task_2_score""] += s

    # Top Enjoyed Aspects
    # Key provides one valid pair, e.g., [""Team competition"", ""Tracking daily progress""]
    # Alternative for 2nd item: ""Feeling healthier""
    key_enjoyed_primary = get_nested_value(task2_key, (""top_enjoyed_aspects"", 0), """") # ""Team competition""
    key_enjoyed_secondary_in_key = get_nested_value(task2_key, (""top_enjoyed_aspects"", 1), """") # ""Tracking daily progress""
    # Based on evaluation guide, the alternative secondary is ""Feeling healthier""
    enjoyed_alt_secondary = ""Feeling healthier"" 
    key_enjoyed_secondary_options = [key_enjoyed_secondary_in_key, enjoyed_alt_secondary]
    
    s, c = score_task2_top_list(
        get_nested_value(task2_submission, (""top_enjoyed_aspects"",)),
        key_enjoyed_primary,
        key_enjoyed_secondary_options,
        ""Top Enjoyed Aspects""
    )
    results[""task_2_scores""][""top_enjoyed_aspects""] = {""score"": s, ""max_score"": TASK2_TOP_ENJOYED_MAX, ""comment"": c}
    results[""task_2_scores""][""total_task_2_score""] += s
    
    # Top Improvement Suggestions
    # Key provides one valid pair, e.g., [""More prize categories"", ""Better mobile app""]
    # Alternative for 2nd item: ""Shorter duration""
    key_improvement_primary = get_nested_value(task2_key, (""top_improvement_suggestions"", 0), """") # ""More prize categories""
    key_improvement_secondary_in_key = get_nested_value(task2_key, (""top_improvement_suggestions"", 1), """") # ""Better mobile app""
    improvement_alt_secondary = ""Shorter duration""
    key_improvement_secondary_options = [key_improvement_secondary_in_key, improvement_alt_secondary]

    s, c = score_task2_top_list(
        get_nested_value(task2_submission, (""top_improvement_suggestions"",)),
        key_improvement_primary,
        key_improvement_secondary_options,
        ""Top Improvement Suggestions""
    )
    results[""task_2_scores""][""top_improvement_suggestions""] = {""score"": s, ""max_score"": TASK2_TOP_IMPROVEMENT_MAX, ""comment"": c}
    results[""task_2_scores""][""total_task_2_score""] += s

    s, c = score_task2_recommendation(
        get_nested_value(task2_submission, (""recommendation_for_next_year"",)),
        # Pass the actual top suggestions from the key for keyword checking
        [key_improvement_primary, key_improvement_secondary_in_key, improvement_alt_secondary] 
    )
    results[""task_2_scores""][""recommendation_for_next_year""] = {""score"": s, ""max_score"": TASK2_RECOMMENDATION_MAX, ""comment"": c}
    results[""task_2_scores""][""total_task_2_score""] += s

    # --- Final Scores ---
    results[""total_exam_score""] = results[""task_1_scores""][""total_task_1_score""] + results[""task_2_scores""][""total_task_2_score""]
    if TOTAL_EXAM_MAX_SCORE > 0:
        results[""overall_score""] = round((results[""total_exam_score""] / TOTAL_EXAM_MAX_SCORE) * 100, 2)
    else:
        results[""overall_score""] = 0.0
        
    return results

def main():
    parser = argparse.ArgumentParser(description=""Evaluate Fitness and Wellness Coordinator exam submission."")
    parser.add_argument(""submission_file"", help=""Path to the candidate's submission JSON file."")
    parser.add_argument(""key_file"", help=""Path to the answer key JSON file."")
    args = parser.parse_args()

    try:
        with open(args.submission_file, 'r') as f:
            submission_data = json.load(f)
    except FileNotFoundError:
        print(f""Error: Submission file '{args.submission_file}' not found."")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f""Error: Could not decode JSON from submission file '{args.submission_file}'."")
        sys.exit(1)

    try:
        with open(args.key_file, 'r') as f:
            key_data = json.load(f)
    except FileNotFoundError:
        print(f""Error: Answer key file '{args.key_file}' not found."")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f""Error: Could not decode JSON from answer key file '{args.key_file}'."")
        sys.exit(1)

    evaluation_results = evaluate_submission(submission_data, key_data)

    try:
        with open(""test_results.json"", 'w') as f:
            json.dump(evaluation_results, f, indent=2)
        print(""Evaluation complete. Results saved to 'test_results.json'"")
    except IOError:
        print(""Error: Could not write results to 'test_results.json'."")
        sys.exit(1)

if __name__ == ""__main__"":
    main()
```","{'candidate_name': 'Official Answer Key', 'candidate_id': 'Official_Answer_Key_v1.0', 'submission_date': '2024-07-29', 'exam_version': 'Basic_FWC_NeedsAssessment_v1', 'task_1_interest_survey_design': {'survey_title': 'Interest in Mid-day Mindfulness & Stretching Breaks at Innovatech', 'target_audience_description': 'All employees of Innovatech Solutions.', 'survey_introduction_text': ""Innovatech Solutions is exploring a new 'Mid-day Mindfulness & Stretching Break' program to support employee well-being. These 15-20 minute guided sessions would be offered twice a week. Your anonymous feedback is valuable in helping us understand interest and preferences. Please take a few minutes to complete this short survey."", 'survey_questions': [{'question_text': ""How interested are you in participating in a 15-20 minute guided 'Mid-day Mindfulness & Stretching Break' offered twice a week at Innovatech Solutions?"", 'question_type': 'Likert Scale (1-5)', 'options': ['1 - Not at all interested', '2 - Slightly interested', '3 - Moderately interested', '4 - Very interested', '5 - Extremely interested']}, {'question_text': 'Which days of the week would you most likely be able to attend such a session? (Select all that apply)', 'question_type': 'Multiple Choice - Multiple Answers', 'options': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']}, {'question_text': 'What time slot(s) during the mid-day (approx. 11 AM - 2 PM) would best suit you for these sessions? (Select all that apply)', 'question_type': 'Multiple Choice - Multiple Answers', 'options': ['11:30 AM - 11:50 AM', '12:30 PM - 12:50 PM', '1:30 PM - 1:50 PM', 'Other (please specify if different)']}, {'question_text': 'To what extent do you feel a mid-day break for mindfulness and stretching would help improve your personal well-being or productivity?', 'question_type': 'Likert Scale (1-5)', 'options': ['1 - Not at all helpful', '2 - Slightly helpful', '3 - Moderately helpful', '4 - Very helpful', '5 - Extremely helpful']}, {'question_text': 'What, if any, are your primary concerns or potential barriers to participating in such a program? (e.g., workload, meeting schedules, not interested, physical limitations)', 'question_type': 'Open-ended Text', 'options': []}, {'question_text': 'If such sessions were offered, would you prefer more emphasis on mindfulness, stretching, or an equal balance of both?', 'question_type': 'Multiple Choice - Single Answer', 'options': ['Primarily Mindfulness', 'Primarily Stretching', 'Equal Balance of Both', 'Unsure']}]}, 'task_2_satisfaction_data_analysis': {'satisfaction_rating_summary': {'1': 1, '2': 1, '3': 3, '4': 5, '5': 5}, 'top_enjoyed_aspects': ['Team competition', 'Tracking daily progress'], 'top_improvement_suggestions': ['More prize categories', 'Better mobile app'], 'recommendation_for_next_year': ""For next year's Step Challenge, Innovatech Solutions should consider increasing the number or variety of prize categories, as this was a prominent suggestion. Additionally, exploring improvements to the mobile app or offering a shorter challenge duration could enhance participant satisfaction and address other common feedback points.""}}","[[None, 'loop of ufunc does not support argument 0 of type NoneType which has no callable rint method']]",True,True,0.0,99.0,100.0,2.0,True,"The exam is practical, testing real job tasks like survey design and basic data analysis, which are realistic for a Fitness and Wellness Coordinator in 2025. The instructions, materials, and submission requirements are generally unambiguous and well-detailed, especially the JSON submission format. 

The grading script and answer key appear to correctly reflect the exam's intent. The answer key, when manually checked against the grading script's logic, should score 100%. The scoring weights and passing threshold (70/100) seem reasonable for a basic practical exam, and it's unlikely a candidate could pass by failing a major component entirely (e.g., getting zero on Task 1 or Task 2).

Potential Weaknesses/Ambiguities:
1.  **`candidate_id` Field**: The instruction for `candidate_id` in the `Submission Format` section asks for ""The model version that is powering your responses (e.g., 'GPT-4o', ...)"". This is confusing for a human candidate, for whom this exam seems intended. It should likely be a standard candidate identifier (e.g., employee ID, exam ID) or removed if not applicable to human test-takers.
2.  **Order of Top Items in Task 2 Lists**: There's a slight internal inconsistency. The `MATERIALS_EXPLANATION_FOR_EVALUATOR` states for `top_enjoyed_aspects` and `top_improvement_suggestions` that ""The first item must be [the most frequent]"". However, the `Evaluation Info & Answer Key Explanation` (which likely dictates grading policy) says, ""The order of the two items in the list does not strictly matter"". The grading script correctly aligns with the latter (order doesn't matter) by using set comparisons. This internal documentation discrepancy should be resolved, but the current grading implementation is consistent with the stated grading policy.
3.  **Task 2 Data Context**: A minor inconsistency exists where instructions mention ""30 responses were collected"" but then state ""raw data from these 15 of these responses is provided"". This doesn't affect the task itself as only 15 are provided and analyzed, but could be tidied up.

Overall, the exam design is robust for its stated purpose.","{'overview': prompt_token_count: 252
candidates_token_count: 2033
total_token_count: 3820
, 'instructions': prompt_token_count: 2491
candidates_token_count: 1827
total_token_count: 8416
, 'materials': prompt_token_count: 4523
candidates_token_count: 1866
total_token_count: 13446
, 'check_materials': prompt_token_count: 3797
candidates_token_count: 1
total_token_count: 4721
, 'check_website': prompt_token_count: 3830
candidates_token_count: 1
total_token_count: 4211
, 'submission': prompt_token_count: 6173
candidates_token_count: 2286
total_token_count: 9790
, 'evaluation': prompt_token_count: 8380
candidates_token_count: 3683
total_token_count: 15705
, 'grading': prompt_token_count: 12252
candidates_token_count: 7395
total_token_count: 27396
, 'check_sense': prompt_token_count: 20365
candidates_token_count: 509
total_token_count: 28401
}",
Fitness and Wellness Coordinators,15558.0,"Maintain wellness- and fitness-related schedules, records, or reports.",gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor', 'PDF viewer']","['Text', 'Data']","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

**Fitness and Wellness Coordinator - Basic Practical Exam: Maintaining Schedules, Records, and Reports**

**Welcome Candidate,**

This practical exam assesses your ability to perform fundamental tasks related to maintaining wellness and fitness schedules, records, and reports using common digital tools. You will be working with provided data files to complete specific tasks.

**Exam Duration:** 90 Minutes

**Your Goal:**

Accurately process scheduling updates, record attendance data, and generate a simple summary report based on the provided information. Your final answers must be submitted in a specific JSON format.

**Tools Allowed:**

*   Spreadsheet Software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   Text Editor (e.g., Notepad, TextEdit, VS Code)
*   PDF Viewer (for reading these instructions)

**Provided Materials:**

You will be provided with the following digital files:

1.  `instructions.pdf`: This document, containing the exam scenario, tasks, and submission format.
2.  `current_schedule.txt`: A text file listing the current weekly fitness class schedule, including class name, instructor, day, time, and room.
3.  `update_requests.txt`: A text file containing:
    *   A request to add one new class to the schedule.
    *   Attendance data (list of attendees) for specific classes on specific dates.

**Tasks:**

You must complete the following three tasks. Use your spreadsheet software to organize and process the data as needed before constructing your final JSON submission.

**Task 1: Update Schedule & Identify Conflict**

1.  Examine the `current_schedule.txt` file to understand the existing class schedule.
2.  Review the ""New Class Request"" details provided in the `update_requests.txt` file.
3.  Determine if adding the requested new class creates a scheduling conflict with the existing schedule (i.e., occupies the same room at the same time on the same day as another class).
4.  Prepare the data for the `updated_schedule` section of your JSON submission. This list should include:
    *   All classes from the original `current_schedule.txt`.
    *   The requested new class *only if it does not create a conflict*. If it conflicts, do *not* include it in this list.
5.  Note your findings regarding the conflict (whether one exists and a brief, factual description) for the JSON submission.

**Task 2: Record Attendance**

1.  Review the ""Attendance Data"" section within the `update_requests.txt` file.
2.  Accurately transcribe this attendance information, ensuring correct class names, dates, and attendee names are associated. Prepare this data for the `attendance_records` section of your JSON submission.

**Task 3: Generate Simple Report**

1.  Using the attendance data provided in `update_requests.txt` (and structured in Task 2), calculate the *total number of attendees* for each class instance listed in the attendance data.
2.  Prepare this summary data for the `attendance_summary` section of your JSON submission, listing each class name from the attendance logs and its corresponding total attendee count.

**Submission Instructions:**

You must submit your answers as a single JSON file named `test_submission.json`.

*   Use a text editor to create or verify the final JSON file.
*   Ensure the file strictly adheres to the structure specified below, including all keys, data types (string, boolean, integer, array), and formatting (curly braces `{}`, square brackets `[]`, commas `,`, double quotes `""""`).
*   Replace `""REPLACE_WITH_YOUR_ID""` with your assigned Candidate ID if provided; otherwise, leave it as is.

**Required JSON Format:**

```json
{
  ""exam_type"": ""basic_maintenance"",
  ""candidate_id"": ""REPLACE_WITH_YOUR_ID"",
  ""task_1_scheduling"": {
    ""conflict_identified"": true, // Boolean: true if a conflict exists, false otherwise
    ""conflict_description"": ""Provide a brief, factual description of the conflict (e.g., 'Requested 'Advanced Spin' on Wed 10:00 in Studio B conflicts with existing 'Yoga Flow' at the same time and location.') OR 'No conflict identified.' if none exists."", // String
    ""updated_schedule"": [
      // Array of objects. Include all original classes. Only include the NEW class if it did NOT conflict.
      // Example object structure:
      {
        ""class_name"": ""Example Class Name"", // String
        ""instructor"": ""Instructor Name"", // String
        ""day"": ""DayOfWeek"", // String (e.g., ""Monday"")
        ""time"": ""HH:MM"", // String (e.g., ""09:00"")
        ""room"": ""Room Name"" // String (e.g., ""Studio A"")
      }
      // Add one object for each class in the final schedule list
    ]
  },
  ""task_2_record_keeping"": {
    ""attendance_records"": [
      // Array of objects, one for each class instance attendance was recorded for.
      // Example object structure:
      {
        ""class_name"": ""Class Name from Log"", // String
        ""date"": ""YYYY-MM-DD"", // String
        ""attendees"": [
          ""Attendee Full Name 1"", // String
          ""Attendee Full Name 2""  // String
          // Add all attendee names for this class/date instance
        ]
      }
      // Add one object for each recorded attendance log entry
    ]
  },
  ""task_3_reporting"": {
    ""attendance_summary"": [
      // Array of objects, one for each class summarized from the attendance logs.
      // Example object structure:
      {
        ""class_name"": ""Class Name from Log"", // String
        ""total_attendees"": 0 // Integer: Total count of attendees for this class from the logs
      }
      // Add one object for each class listed in the attendance logs
    ]
  }
}
```

**Begin the exam when you are ready. Ensure you manage your time effectively to complete all tasks and format the submission file within the 90-minute limit.**

---
Here is the content for the two text files (`.txt`) that will be provided to the candidate.

**File 1: `current_schedule.txt`**

```text
Class: Morning Yoga, Instructor: Anya Sharma, Day: Monday, Time: 09:00, Room: Studio A
Class: Cycle Fit, Instructor: Ben Carter, Day: Tuesday, Time: 17:30, Room: Cycle Studio
Class: Pilates Core, Instructor: Anya Sharma, Day: Thursday, Time: 10:00, Room: Studio B
Class: Aqua Aerobics, Instructor: Chloe Davis, Day: Friday, Time: 11:00, Room: Pool
Class: Strength Circuit, Instructor: Ben Carter, Day: Thursday, Time: 18:00, Room: Gym Floor
```

**File 2: `update_requests.txt`**

```text
**New Class Request:**
Class Name: Gentle Stretching
Instructor: David Lee
Requested Day: Thursday
Requested Time: 10:00
Requested Room: Studio B

**Attendance Data:**
Class: Cycle Fit, Date: 2024-08-06, Attendees: John Smith, Emily White, Omar Khan
Class: Aqua Aerobics, Date: 2024-08-09, Attendees: Sarah Jones, David Chen
Class: Morning Yoga, Date: 2024-08-05, Attendees: Lisa Green, Mike Brown, Emily White, Anna Bell
```

Okay, here is the specification for the candidate's submission format.

**Submission File:**

You must submit a single file named `test_submission.json`. No other files are required.

**Candidate ID:**

Within the JSON file, locate the `candidate_id` field. Replace the placeholder `""YOUR_ID_HERE""` with the specific model version identifier provided to you (e.g., `""GPT-4o""`, `""Claude-3_7-Sonnet""`, etc.).

**JSON Format Specification:**

Your `test_submission.json` file must strictly adhere to the following structure and data types. Ensure all strings are enclosed in double quotes (`""""`), commas separate elements in objects and arrays correctly, and boolean values are `true` or `false` (lowercase, without quotes).

```json
{
  ""exam_type"": ""basic_maintenance"", // Keep this value exactly as ""basic_maintenance"" (String)
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace with your assigned model ID (String)

  ""task_1_scheduling"": {
    // Task 1: Schedule Update and Conflict Identification
    ""conflict_identified"": null, // Replace null with either true or false (Boolean)
    ""conflict_description"": """", // Provide a descriptive sentence explaining the conflict OR the exact phrase ""No conflict identified."" (String)
    ""updated_schedule"": [
      // Array of Objects. Each object represents a class in the final schedule.
      // Include all original classes. Only include the NEW class if it did NOT conflict.
      // The order of classes within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided (String)
        ""instructor"": """", // Instructor name exactly as provided (String)
        ""day"": """", // Day of the week, capitalized (e.g., ""Monday"") (String)
        ""time"": """", // Time in HH:MM format (e.g., ""09:00"", ""17:30"") (String)
        ""room"": """" // Room name exactly as provided (String)
      }
      // Add more class objects as needed, following the structure above.
    ]
  },

  ""task_2_record_keeping"": {
    // Task 2: Attendance Recording
    ""attendance_records"": [
      // Array of Objects. Each object represents one attendance log entry.
      // Include one object for each class/date instance listed in the ""Attendance Data"".
      // The order of entries within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided in the attendance log (String)
        ""date"": """", // Date in YYYY-MM-DD format (e.g., ""2024-08-06"") (String)
        ""attendees"": [
          // Array of Strings. List all attendee full names exactly as provided for this specific class/date.
          // The order of names within the array does not matter.
          """" // Attendee Full Name (String)
          // Add more attendee name strings as needed.
        ]
      }
      // Add more attendance record objects as needed, following the structure above.
    ]
  },

  ""task_3_reporting"": {
    // Task 3: Simple Attendance Summary Report
    ""attendance_summary"": [
      // Array of Objects. Each object represents the summary for one class from the attendance logs.
      // Include one object for each unique class name present in the ""Attendance Data"".
      // The order of class summaries within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided in the attendance log (String)
        ""total_attendees"": 0 // The total count of attendees for this class across all provided logs (Integer)
      }
      // Add more summary objects as needed, following the structure above.
    ]
  }
}
```

**Key Formatting Rules:**

*   **File Name:** Must be exactly `test_submission.json`.
*   **JSON Validity:** The entire file must be valid JSON. Use an online validator if unsure.
*   **Data Types:** Adhere strictly to the specified types (String, Boolean, Integer, Array, Object).
*   **Strings:** Enclose all keys and string values in double quotes (`""""`).
*   **Booleans:** Use `true` or `false` (lowercase, no quotes).
*   **Integers:** Use whole numbers without quotes (e.g., `3`, not `""3""`).
*   **Dates:** Use `YYYY-MM-DD` format for dates where specified.
*   **Times:** Use `HH:MM` (24-hour format) for times where specified.
*   **Exactness:** Transcribe names, class titles, rooms, etc., exactly as they appear in the provided `.txt` files, preserving capitalization and spacing.
*   **Completeness:** Ensure all required fields are present in your JSON structure. Do not add extra fields not specified here.","You are an excellent examiner of Fitness and Wellness Coordinators capabilities. Design a remote, **practical** exam to verify whether a Fitness and Wellness Coordinators can Maintain wellness- and fitness-related schedules, records, or reports..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets, Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the design for the **Basic Level Practical Exam** focusing on maintaining wellness- and fitness-related schedules, records, or reports.

**Examiner's Briefing:**

*   **Exam Purpose:** This practical exam assesses the candidate's fundamental ability to perform routine tasks related to maintaining basic fitness/wellness schedules, recording simple data accurately, and generating straightforward reports from provided information. It focuses on *doing* the tasks, not theoretical knowledge.
*   **Exam Structure:** The exam consists of three sequential tasks simulating a workflow:
    1.  **Scheduling:** Updating a simple class schedule and identifying a basic conflict.
    2.  **Record Keeping:** Entering attendance data accurately into a structured format.
    3.  **Reporting:** Compiling a simple summary report based on the recorded data.
*   **Candidate Tools:** Spreadsheet software (e.g., Excel, Google Sheets, LibreOffice Calc), a basic Text Editor (e.g., Notepad, TextEdit), and a PDF Viewer.
*   **Provided Materials:**
    *   `instructions.pdf`: Contains the scenario description, task details, and required JSON output format.
    *   `current_schedule.txt`: A simple text file listing current fitness classes, times, instructors, and locations.
    *   `update_requests.txt`: Contains requests for adding one new class (which creates a simple conflict) and recording attendance for a few specific classes/dates.
*   **Assessment Focus:** Accuracy in data entry, ability to follow instructions, correct identification of a simple scheduling conflict, basic data aggregation (counting), and adherence to the specified JSON output format.
*   **Time Limit:** 90 minutes.
*   **Submission:** A single JSON file named `test_submission.json`.

---

**Exam Details (To be provided to the candidate within `instructions.pdf`)**

**Scenario:**

You are assisting the lead Fitness and Wellness Coordinator at a community center. Your task is to update the weekly class schedule based on a new request, record recent class attendance, and provide a quick summary report.

**Tools Allowed:**

*   Spreadsheet Software (for organizing data, calculations)
*   Text Editor (for viewing input files, drafting/checking JSON)
*   PDF Viewer (for these instructions)

**Provided Files:**

*   `instructions.pdf` (This file)
*   `current_schedule.txt`: Shows the existing class schedule for the week.
*   `update_requests.txt`: Contains information for the tasks below.

**Time Limit:** 90 Minutes

**Tasks:**

**Task 1: Update Schedule & Identify Conflict**

1.  Review the `current_schedule.txt` file to understand the existing schedule.
2.  Review the ""New Class Request"" section within `update_requests.txt`.
3.  Identify if adding the new class creates a scheduling conflict (e.g., same time and room as an existing class).
4.  Using your spreadsheet software, create an updated schedule.
    *   Include all original classes from `current_schedule.txt`.
    *   Add the *new class* from `update_requests.txt` **only if it does not conflict**. If it conflicts, do *not* add it to the schedule table itself, but note the conflict as described below.
5.  Prepare the JSON output for this task (see ""Submission Format"" below).
    *   Clearly state whether a conflict was identified.
    *   Describe the conflict (which classes, times, rooms are involved).
    *   Represent the final, updated schedule (including the new class if successfully added, or just the original schedule if the new one conflicted).

**Task 2: Record Attendance**

1.  Review the ""Attendance Data"" section within `update_requests.txt`. This section lists attendees for specific classes on specific dates.
2.  Using your spreadsheet software (you can use a new sheet or area), accurately record this attendance information in a structured way.
3.  Prepare the JSON output for this task, representing the recorded attendance data clearly (see ""Submission Format"" below). Ensure names, class, and date are accurately transcribed.

**Task 3: Generate Simple Report**

1.  Using the attendance data you structured in Task 2 (or by directly referencing `update_requests.txt`), calculate the *total number of attendees* for each class listed in the attendance data.
2.  Prepare the JSON output for this task, creating a simple summary report showing each class name and its total attendance count (see ""Submission Format"" below).

**Submission Format:**

Create a single JSON file named `test_submission.json` with the following structure. Use your spreadsheet to organize the data first, then carefully construct the JSON output using a text editor. Ensure correct JSON syntax (curly braces, square brackets, commas, quotes).

```json
{
  ""exam_type"": ""basic_maintenance"",
  ""candidate_id"": ""REPLACE_WITH_YOUR_ID"", // Or leave as placeholder if instructed
  ""task_1_scheduling"": {
    ""conflict_identified"": true, // boolean: true or false
    ""conflict_description"": ""Describe the conflict here, e.g., 'Requested 'Advanced Spin' on Wed 10:00 in Studio B conflicts with existing 'Yoga Flow' at the same time and location.' or 'No conflict identified.'"",
    ""updated_schedule"": [
      // Array of objects, one for each class in the final schedule
      // Example: Only include the new class if it DID NOT conflict.
      {
        ""class_name"": ""Example Class 1"",
        ""instructor"": ""Instructor A"",
        ""day"": ""Monday"",
        ""time"": ""09:00"",
        ""room"": ""Studio A""
      },
      {
        ""class_name"": ""Example Class 2"",
        ""instructor"": ""Instructor B"",
        ""day"": ""Tuesday"",
        ""time"": ""18:00"",
        ""room"": ""Studio B""
      }
      // ... include all classes from the final schedule
    ]
  },
  ""task_2_record_keeping"": {
    ""attendance_records"": [
      // Array of objects, one for each class instance attendance was recorded for
      // Example:
      {
        ""class_name"": ""Recorded Class A"",
        ""date"": ""YYYY-MM-DD"",
        ""attendees"": [
          ""Attendee Name 1"",
          ""Attendee Name 2"",
          ""Attendee Name 3""
        ]
      },
      {
        ""class_name"": ""Recorded Class B"",
        ""date"": ""YYYY-MM-DD"",
        ""attendees"": [
          ""Attendee Name 4"",
          ""Attendee Name 5""
        ]
      }
      // ... include all recorded attendance logs
    ]
  },
  ""task_3_reporting"": {
    ""attendance_summary"": [
      // Array of objects, one for each class summarized
      // Example:
      {
        ""class_name"": ""Recorded Class A"",
        ""total_attendees"": 3 // Integer: Total count for this class from the logs
      },
      {
        ""class_name"": ""Recorded Class B"",
        ""total_attendees"": 2 // Integer: Total count for this class from the logs
      }
      // ... include summary for all classes in the attendance logs
    ]
  }
}

```

**End of Instructions**

---

**(For the Evaluator - Content of Input Files - Example)**

*   **`current_schedule.txt`:**
    ```
    Class: Yoga Flow, Instructor: Sarah Chen, Day: Wednesday, Time: 10:00, Room: Studio B
    Class: HIIT Blast, Instructor: Mike Davis, Day: Tuesday, Time: 18:00, Room: Gym Floor
    Class: Senior Stretch, Instructor: Sarah Chen, Day: Wednesday, Time: 11:00, Room: Studio B
    Class: Zumba Gold, Instructor: Maria Garcia, Day: Friday, Time: 09:00, Room: Studio A
    ```
*   **`update_requests.txt`:**
    ```
    **New Class Request:**
    Class Name: Advanced Spin
    Instructor: Mike Davis
    Requested Day: Wednesday
    Requested Time: 10:00
    Requested Room: Studio B

    **Attendance Data:**
    Class: HIIT Blast, Date: 2024-07-16, Attendees: Alice W, Bob T, Charlie M
    Class: Zumba Gold, Date: 2024-07-19, Attendees: Eve F, Frank G
    Class: Yoga Flow, Date: 2024-07-17, Attendees: Grace H, Heidi I, Bob T
    ```

This setup provides a concrete, practical test of basic scheduling (with conflict identification), data entry, and simple reporting skills using standard office tools, suitable for remote administration and evaluation via the structured JSON output.","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

**Fitness and Wellness Coordinator - Basic Practical Exam: Maintaining Schedules, Records, and Reports**

**Welcome Candidate,**

This practical exam assesses your ability to perform fundamental tasks related to maintaining wellness and fitness schedules, records, and reports using common digital tools. You will be working with provided data files to complete specific tasks.

**Exam Duration:** 90 Minutes

**Your Goal:**

Accurately process scheduling updates, record attendance data, and generate a simple summary report based on the provided information. Your final answers must be submitted in a specific JSON format.

**Tools Allowed:**

*   Spreadsheet Software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   Text Editor (e.g., Notepad, TextEdit, VS Code)
*   PDF Viewer (for reading these instructions)

**Provided Materials:**

You will be provided with the following digital files:

1.  `instructions.pdf`: This document, containing the exam scenario, tasks, and submission format.
2.  `current_schedule.txt`: A text file listing the current weekly fitness class schedule, including class name, instructor, day, time, and room.
3.  `update_requests.txt`: A text file containing:
    *   A request to add one new class to the schedule.
    *   Attendance data (list of attendees) for specific classes on specific dates.

**Tasks:**

You must complete the following three tasks. Use your spreadsheet software to organize and process the data as needed before constructing your final JSON submission.

**Task 1: Update Schedule & Identify Conflict**

1.  Examine the `current_schedule.txt` file to understand the existing class schedule.
2.  Review the ""New Class Request"" details provided in the `update_requests.txt` file.
3.  Determine if adding the requested new class creates a scheduling conflict with the existing schedule (i.e., occupies the same room at the same time on the same day as another class).
4.  Prepare the data for the `updated_schedule` section of your JSON submission. This list should include:
    *   All classes from the original `current_schedule.txt`.
    *   The requested new class *only if it does not create a conflict*. If it conflicts, do *not* include it in this list.
5.  Note your findings regarding the conflict (whether one exists and a brief, factual description) for the JSON submission.

**Task 2: Record Attendance**

1.  Review the ""Attendance Data"" section within the `update_requests.txt` file.
2.  Accurately transcribe this attendance information, ensuring correct class names, dates, and attendee names are associated. Prepare this data for the `attendance_records` section of your JSON submission.

**Task 3: Generate Simple Report**

1.  Using the attendance data provided in `update_requests.txt` (and structured in Task 2), calculate the *total number of attendees* for each class instance listed in the attendance data.
2.  Prepare this summary data for the `attendance_summary` section of your JSON submission, listing each class name from the attendance logs and its corresponding total attendee count.

**Submission Instructions:**

You must submit your answers as a single JSON file named `test_submission.json`.

*   Use a text editor to create or verify the final JSON file.
*   Ensure the file strictly adheres to the structure specified below, including all keys, data types (string, boolean, integer, array), and formatting (curly braces `{}`, square brackets `[]`, commas `,`, double quotes `""""`).
*   Replace `""REPLACE_WITH_YOUR_ID""` with your assigned Candidate ID if provided; otherwise, leave it as is.

**Required JSON Format:**

```json
{
  ""exam_type"": ""basic_maintenance"",
  ""candidate_id"": ""REPLACE_WITH_YOUR_ID"",
  ""task_1_scheduling"": {
    ""conflict_identified"": true, // Boolean: true if a conflict exists, false otherwise
    ""conflict_description"": ""Provide a brief, factual description of the conflict (e.g., 'Requested 'Advanced Spin' on Wed 10:00 in Studio B conflicts with existing 'Yoga Flow' at the same time and location.') OR 'No conflict identified.' if none exists."", // String
    ""updated_schedule"": [
      // Array of objects. Include all original classes. Only include the NEW class if it did NOT conflict.
      // Example object structure:
      {
        ""class_name"": ""Example Class Name"", // String
        ""instructor"": ""Instructor Name"", // String
        ""day"": ""DayOfWeek"", // String (e.g., ""Monday"")
        ""time"": ""HH:MM"", // String (e.g., ""09:00"")
        ""room"": ""Room Name"" // String (e.g., ""Studio A"")
      }
      // Add one object for each class in the final schedule list
    ]
  },
  ""task_2_record_keeping"": {
    ""attendance_records"": [
      // Array of objects, one for each class instance attendance was recorded for.
      // Example object structure:
      {
        ""class_name"": ""Class Name from Log"", // String
        ""date"": ""YYYY-MM-DD"", // String
        ""attendees"": [
          ""Attendee Full Name 1"", // String
          ""Attendee Full Name 2""  // String
          // Add all attendee names for this class/date instance
        ]
      }
      // Add one object for each recorded attendance log entry
    ]
  },
  ""task_3_reporting"": {
    ""attendance_summary"": [
      // Array of objects, one for each class summarized from the attendance logs.
      // Example object structure:
      {
        ""class_name"": ""Class Name from Log"", // String
        ""total_attendees"": 0 // Integer: Total count of attendees for this class from the logs
      }
      // Add one object for each class listed in the attendance logs
    ]
  }
}
```

**Begin the exam when you are ready. Ensure you manage your time effectively to complete all tasks and format the submission file within the 90-minute limit.**

---","<MATERIALS_FOR_CANDIDATE>
Here is the content for the two text files (`.txt`) that will be provided to the candidate.

**File 1: `current_schedule.txt`**

```text
Class: Morning Yoga, Instructor: Anya Sharma, Day: Monday, Time: 09:00, Room: Studio A
Class: Cycle Fit, Instructor: Ben Carter, Day: Tuesday, Time: 17:30, Room: Cycle Studio
Class: Pilates Core, Instructor: Anya Sharma, Day: Thursday, Time: 10:00, Room: Studio B
Class: Aqua Aerobics, Instructor: Chloe Davis, Day: Friday, Time: 11:00, Room: Pool
Class: Strength Circuit, Instructor: Ben Carter, Day: Thursday, Time: 18:00, Room: Gym Floor
```

**File 2: `update_requests.txt`**

```text
**New Class Request:**
Class Name: Gentle Stretching
Instructor: David Lee
Requested Day: Thursday
Requested Time: 10:00
Requested Room: Studio B

**Attendance Data:**
Class: Cycle Fit, Date: 2024-08-06, Attendees: John Smith, Emily White, Omar Khan
Class: Aqua Aerobics, Date: 2024-08-09, Attendees: Sarah Jones, David Chen
Class: Morning Yoga, Date: 2024-08-05, Attendees: Lisa Green, Mike Brown, Emily White, Anna Bell
```

</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. How Materials Were Created:**

*   The materials were synthetically generated to create a specific, predictable scenario for a basic-level assessment.
*   **`current_schedule.txt`**: Contains a small, easily parsable list of fitness classes with standard details (Name, Instructor, Day, Time, Room). One specific class (`Pilates Core` on Thursday at 10:00 in Studio B) was included intentionally to create a conflict with the new request.
*   **`update_requests.txt`**:
    *   **New Class Request**: This request was designed to directly conflict with the `Pilates Core` class in `current_schedule.txt` by requesting the exact same Day, Time, and Room. This tests the candidate's ability to identify a simple, direct scheduling conflict.
    *   **Attendance Data**: This section provides straightforward attendance logs for three distinct class instances (using classes from the *original* schedule). Dates are in a consistent `YYYY-MM-DD` format. Attendee names are simple and require accurate transcription. One attendee (`Emily White`) appears in two different class logs (`Cycle Fit` and `Morning Yoga`) to ensure the candidate records attendance per specific class instance and doesn't mistakenly merge or omit data.

**2. Cross-References and Connections:**

*   The ""New Class Request"" in `update_requests.txt` must be compared against all entries in `current_schedule.txt` to identify the conflict (specifically with `Pilates Core`).
*   The ""Attendance Data"" in `update_requests.txt` uses class names (`Cycle Fit`, `Aqua Aerobics`, `Morning Yoga`) that exist in the `current_schedule.txt`, reinforcing the context. However, the attendance task itself only requires processing the data within the ""Attendance Data"" section.
*   Task 3 (Reporting) directly relies on the accurate processing and aggregation of the ""Attendance Data"" provided in `update_requests.txt` (and likely structured by the candidate during Task 2).

**3. Tricky Elements or Common Pitfalls:**

*   **Task 1 - Conflict Identification:** The primary point is identifying the conflict. A candidate might incorrectly add the new class despite the conflict or fail to describe the conflict accurately (e.g., missing which class it conflicts with, or the specific time/location). They might also incorrectly state there is no conflict.
*   **Task 1 - Updated Schedule:** If the conflict is correctly identified, the `updated_schedule` in the JSON output should *only* contain the original 5 classes from `current_schedule.txt`. Adding the conflicting class (""Gentle Stretching"") here would be an error.
*   **Task 2 - Data Entry Accuracy:** Simple transcription errors (misspelling names, incorrect dates, assigning attendees to the wrong class) are possible. The candidate needs to be meticulous. Ensure `Emily White` is listed under *both* `Cycle Fit` and `Morning Yoga` in the respective records.
*   **Task 3 - Accurate Counting:** The candidate must count the attendees for *each* class instance listed in the attendance data. Errors could involve miscounting or summarizing based on the overall schedule rather than just the provided attendance logs. The expected counts are: Cycle Fit = 3, Aqua Aerobics = 2, Morning Yoga = 4.
*   **JSON Formatting:** Adherence to the strict JSON format (quotes, commas, brackets, data types) is crucial for evaluation. Syntax errors would indicate a failure to follow instructions precisely.

**4. ""Hidden"" Information:**

*   There is no intentionally ""hidden"" information. The test relies on careful reading and direct application of the instructions to the provided data. The presence of `Emily White` in two lists is a minor detail requiring careful reading but isn't hidden. The core challenge lies in accurately performing the requested actions (compare, record, count) and formatting the output correctly.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
Here is the content for the two text files (`.txt`) that will be provided to the candidate.

**File 1: `current_schedule.txt`**

```text
Class: Morning Yoga, Instructor: Anya Sharma, Day: Monday, Time: 09:00, Room: Studio A
Class: Cycle Fit, Instructor: Ben Carter, Day: Tuesday, Time: 17:30, Room: Cycle Studio
Class: Pilates Core, Instructor: Anya Sharma, Day: Thursday, Time: 10:00, Room: Studio B
Class: Aqua Aerobics, Instructor: Chloe Davis, Day: Friday, Time: 11:00, Room: Pool
Class: Strength Circuit, Instructor: Ben Carter, Day: Thursday, Time: 18:00, Room: Gym Floor
```

**File 2: `update_requests.txt`**

```text
**New Class Request:**
Class Name: Gentle Stretching
Instructor: David Lee
Requested Day: Thursday
Requested Time: 10:00
Requested Room: Studio B

**Attendance Data:**
Class: Cycle Fit, Date: 2024-08-06, Attendees: John Smith, Emily White, Omar Khan
Class: Aqua Aerobics, Date: 2024-08-09, Attendees: Sarah Jones, David Chen
Class: Morning Yoga, Date: 2024-08-05, Attendees: Lisa Green, Mike Brown, Emily White, Anna Bell
```

","Okay, here is the specification for the candidate's submission format.

**Submission File:**

You must submit a single file named `test_submission.json`. No other files are required.

**Candidate ID:**

Within the JSON file, locate the `candidate_id` field. Replace the placeholder `""YOUR_ID_HERE""` with the specific model version identifier provided to you (e.g., `""GPT-4o""`, `""Claude-3_7-Sonnet""`, etc.).

**JSON Format Specification:**

Your `test_submission.json` file must strictly adhere to the following structure and data types. Ensure all strings are enclosed in double quotes (`""""`), commas separate elements in objects and arrays correctly, and boolean values are `true` or `false` (lowercase, without quotes).

```json
{
  ""exam_type"": ""basic_maintenance"", // Keep this value exactly as ""basic_maintenance"" (String)
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace with your assigned model ID (String)

  ""task_1_scheduling"": {
    // Task 1: Schedule Update and Conflict Identification
    ""conflict_identified"": null, // Replace null with either true or false (Boolean)
    ""conflict_description"": """", // Provide a descriptive sentence explaining the conflict OR the exact phrase ""No conflict identified."" (String)
    ""updated_schedule"": [
      // Array of Objects. Each object represents a class in the final schedule.
      // Include all original classes. Only include the NEW class if it did NOT conflict.
      // The order of classes within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided (String)
        ""instructor"": """", // Instructor name exactly as provided (String)
        ""day"": """", // Day of the week, capitalized (e.g., ""Monday"") (String)
        ""time"": """", // Time in HH:MM format (e.g., ""09:00"", ""17:30"") (String)
        ""room"": """" // Room name exactly as provided (String)
      }
      // Add more class objects as needed, following the structure above.
    ]
  },

  ""task_2_record_keeping"": {
    // Task 2: Attendance Recording
    ""attendance_records"": [
      // Array of Objects. Each object represents one attendance log entry.
      // Include one object for each class/date instance listed in the ""Attendance Data"".
      // The order of entries within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided in the attendance log (String)
        ""date"": """", // Date in YYYY-MM-DD format (e.g., ""2024-08-06"") (String)
        ""attendees"": [
          // Array of Strings. List all attendee full names exactly as provided for this specific class/date.
          // The order of names within the array does not matter.
          """" // Attendee Full Name (String)
          // Add more attendee name strings as needed.
        ]
      }
      // Add more attendance record objects as needed, following the structure above.
    ]
  },

  ""task_3_reporting"": {
    // Task 3: Simple Attendance Summary Report
    ""attendance_summary"": [
      // Array of Objects. Each object represents the summary for one class from the attendance logs.
      // Include one object for each unique class name present in the ""Attendance Data"".
      // The order of class summaries within the array does not matter.
      {
        ""class_name"": """", // Class name exactly as provided in the attendance log (String)
        ""total_attendees"": 0 // The total count of attendees for this class across all provided logs (Integer)
      }
      // Add more summary objects as needed, following the structure above.
    ]
  }
}
```

**Key Formatting Rules:**

*   **File Name:** Must be exactly `test_submission.json`.
*   **JSON Validity:** The entire file must be valid JSON. Use an online validator if unsure.
*   **Data Types:** Adhere strictly to the specified types (String, Boolean, Integer, Array, Object).
*   **Strings:** Enclose all keys and string values in double quotes (`""""`).
*   **Booleans:** Use `true` or `false` (lowercase, no quotes).
*   **Integers:** Use whole numbers without quotes (e.g., `3`, not `""3""`).
*   **Dates:** Use `YYYY-MM-DD` format for dates where specified.
*   **Times:** Use `HH:MM` (24-hour format) for times where specified.
*   **Exactness:** Transcribe names, class titles, rooms, etc., exactly as they appear in the provided `.txt` files, preserving capitalization and spacing.
*   **Completeness:** Ensure all required fields are present in your JSON structure. Do not add extra fields not specified here.","Okay, here are the evaluation materials for the Basic Level Practical Exam.

---

**1. Complete Answer Key (`test_submission.json`)**

```json
{
  ""exam_type"": ""basic_maintenance"",
  ""candidate_id"": ""ANSWER_KEY_DO_NOT_MODIFY"",
  ""task_1_scheduling"": {
    ""conflict_identified"": true,
    ""conflict_description"": ""Requested 'Gentle Stretching' on Thursday 10:00 in Studio B conflicts with existing 'Pilates Core' at the same time and location."",
    ""updated_schedule"": [
      {
        ""class_name"": ""Morning Yoga"",
        ""instructor"": ""Anya Sharma"",
        ""day"": ""Monday"",
        ""time"": ""09:00"",
        ""room"": ""Studio A""
      },
      {
        ""class_name"": ""Cycle Fit"",
        ""instructor"": ""Ben Carter"",
        ""day"": ""Tuesday"",
        ""time"": ""17:30"",
        ""room"": ""Cycle Studio""
      },
      {
        ""class_name"": ""Pilates Core"",
        ""instructor"": ""Anya Sharma"",
        ""day"": ""Thursday"",
        ""time"": ""10:00"",
        ""room"": ""Studio B""
      },
      {
        ""class_name"": ""Aqua Aerobics"",
        ""instructor"": ""Chloe Davis"",
        ""day"": ""Friday"",
        ""time"": ""11:00"",
        ""room"": ""Pool""
      },
      {
        ""class_name"": ""Strength Circuit"",
        ""instructor"": ""Ben Carter"",
        ""day"": ""Thursday"",
        ""time"": ""18:00"",
        ""room"": ""Gym Floor""
      }
    ]
  },
  ""task_2_record_keeping"": {
    ""attendance_records"": [
      {
        ""class_name"": ""Cycle Fit"",
        ""date"": ""2024-08-06"",
        ""attendees"": [
          ""John Smith"",
          ""Emily White"",
          ""Omar Khan""
        ]
      },
      {
        ""class_name"": ""Aqua Aerobics"",
        ""date"": ""2024-08-09"",
        ""attendees"": [
          ""Sarah Jones"",
          ""David Chen""
        ]
      },
      {
        ""class_name"": ""Morning Yoga"",
        ""date"": ""2024-08-05"",
        ""attendees"": [
          ""Lisa Green"",
          ""Mike Brown"",
          ""Emily White"",
          ""Anna Bell""
        ]
      }
    ]
  },
  ""task_3_reporting"": {
    ""attendance_summary"": [
      {
        ""class_name"": ""Cycle Fit"",
        ""total_attendees"": 3
      },
      {
        ""class_name"": ""Aqua Aerobics"",
        ""total_attendees"": 2
      },
      {
        ""class_name"": ""Morning Yoga"",
        ""total_attendees"": 4
      }
    ]
  }
}
```

---

**2. Explanation of Correct Answers**

*   **`candidate_id`**: This should be replaced by the candidate's specific ID during the actual test run. The key uses a placeholder.
*   **`task_1_scheduling`**:
    *   `conflict_identified`: Set to `true`. The ""New Class Request"" (`Gentle Stretching`, Thursday, 10:00, Studio B) directly conflicts with the existing `Pilates Core` class (Thursday, 10:00, Studio B) listed in `current_schedule.txt`.
    *   `conflict_description`: Must accurately state the conflict, mentioning the requested class, the existing class, and the shared time/location resource (Day, Time, Room). The example text provides the necessary detail. Minor variations in wording are acceptable if the core information (conflicting classes, time, location) is present. Stating ""No conflict identified."" would be incorrect.
    *   `updated_schedule`: Because a conflict *was* identified, the requested ""Gentle Stretching"" class should *not* be added to the schedule. Therefore, the `updated_schedule` array must contain only the original five classes listed in `current_schedule.txt`. The order of classes in the array does not matter, but all five original classes must be present with accurately transcribed details (name, instructor, day, time, room). Including ""Gentle Stretching"" here is a critical error.
*   **`task_2_record_keeping`**:
    *   `attendance_records`: This array must contain exactly three objects, one for each entry in the ""Attendance Data"" section of `update_requests.txt`.
    *   Each object must accurately reflect the `class_name`, `date` (in YYYY-MM-DD format), and the list of `attendees` provided for that specific entry.
    *   Transcription must be precise: ""Cycle Fit"" (2024-08-06) must list ""John Smith"", ""Emily White"", ""Omar Khan"". ""Aqua Aerobics"" (2024-08-09) must list ""Sarah Jones"", ""David Chen"". ""Morning Yoga"" (2024-08-05) must list ""Lisa Green"", ""Mike Brown"", ""Emily White"", ""Anna Bell"".
    *   The order of objects within the `attendance_records` array does not matter. The order of names within the `attendees` array does not matter. Missing/extra attendees, incorrect dates, or incorrect class names are errors. Note that ""Emily White"" correctly appears in two different lists.
*   **`task_3_reporting`**:
    *   `attendance_summary`: This array must contain exactly three objects, one for each unique class name found in the ""Attendance Data"".
    *   Each object must have the correct `class_name` and the accurate `total_attendees` count (as an integer) based *only* on the provided attendance logs.
    *   Counts must be: ""Cycle Fit"": 3, ""Aqua Aerobics"": 2, ""Morning Yoga"": 4.
    *   The order of objects within the `attendance_summary` array does not matter. Incorrect counts are errors.

---

**3. Passing Criteria**

This is a basic-level practical exam focusing on accuracy and the ability to follow instructions using standard tools. A passing candidate demonstrates fundamental competence in these specific tasks.

**Minimum Requirements for Passing:**

1.  **Valid JSON Submission:** The submitted `test_submission.json` file must be syntactically valid JSON and adhere strictly to the specified structure (all required keys present, correct data types). (Pass/Fail)
2.  **Task 1 - Conflict Handling:**
    *   The `conflict_identified` value must be correct (`true`). (Essential)
    *   The `updated_schedule` array must correctly reflect the conflict decision (i.e., it must contain *only* the 5 original classes and *not* the requested ""Gentle Stretching"" class). (Essential)
    *   The `conflict_description` must reasonably identify the conflicting classes and the reason (time/location). Minor wording differences are acceptable, but identifying the wrong classes or reason is a failure.
3.  **Task 2 - Record Keeping Accuracy:**
    *   All three attendance records must be present in the `attendance_records` array.
    *   Class names and dates for each record must be 100% accurate.
    *   Attendee lists must be highly accurate. Allow for a maximum of 1 minor transcription error (e.g., slight misspelling like ""Jon Smith"" instead of ""John Smith"") across *all* attendee names combined. Missing attendees, extra attendees, or assigning attendees to the wrong class/date constitute failure on this task.
4.  **Task 3 - Reporting Accuracy:**
    *   All three class summaries must be present in the `attendance_summary` array.
    *   The `total_attendees` count for *each* class must be exactly correct (3, 2, 4). (Essential)

**Summary:**

To pass, the candidate must submit valid JSON meeting all structural requirements AND satisfy all 'Essential' criteria listed above for Tasks 1 and 3, plus meet the high accuracy standard for Task 2. Failure on any 'Essential' criterion or significant errors in Task 2 data entry results in a fail for this basic assessment. This strictness reflects the need for accuracy in fundamental record-keeping and scheduling tasks.","```python
import json
import sys
import os
from typing import List, Dict, Any, Tuple, Set

# --- Configuration ---
OUTPUT_FILENAME = ""test_results.json""

# Define points for each check
POINTS_CONFIG = {
    ""task1"": {
        ""conflict_identified"": 1,
        ""conflict_description"": 1,
        ""updated_schedule_match"": 3,
        ""max"": 5
    },
    ""task2"": {
        ""record_structure"": 1, # Points for having the correct number of records
        ""record_content_match_per_record"": 1, # Points per fully correct record
        ""max"": 4 # 1 for structure + 3 records * 1 point/record
    },
    ""task3"": {
        ""summary_structure"": 1, # Points for having the correct number of summaries
        ""summary_content_match_per_item"": 1, # Points per fully correct summary item
        ""max"": 4 # 1 for structure + 3 summaries * 1 point/item
    }
}
# Calculate total possible points dynamically
TOTAL_POSSIBLE_POINTS = sum(v['max'] for v in POINTS_CONFIG.values())

# --- Helper Functions ---

def load_json(filepath: str) -> Dict[str, Any]:
    """"""Loads a JSON file and returns its content.""""""
    if not os.path.exists(filepath):
        print(f""Error: File not found at {filepath}"", file=sys.stderr)
        sys.exit(1)
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON format in {filepath}. Details: {e}"", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f""Error: Could not read file {filepath}. Details: {e}"", file=sys.stderr)
        sys.exit(1)

def create_set_from_list_of_dicts(data_list: List[Dict[str, Any]]) -> Set[frozenset]:
    """"""Converts a list of dictionaries into a set of frozensets for order-agnostic comparison.""""""
    try:
        return {frozenset(item.items()) for item in data_list}
    except Exception:
        # Handle cases where items might not be hashable (e.g., lists within dict values)
        # For this specific exam structure, this shouldn't happen if format is followed.
        return set() # Return empty set to indicate comparison failure

def create_set_from_attendance_records(records: List[Dict[str, Any]]) -> Set[Tuple[str, str, frozenset]]:
    """"""Converts attendance records into a set of tuples with frozenset for attendees.""""""
    record_set = set()
    for record in records:
        try:
            # Ensure attendees list exists and is a list before creating frozenset
            attendees = record.get(""attendees"", [])
            if not isinstance(attendees, list):
                 attendees = [] # Treat non-list attendees as empty for comparison

            # Ensure required keys exist
            class_name = record.get(""class_name"", ""MISSING_CLASS_NAME"")
            date = record.get(""date"", ""MISSING_DATE"")

            record_set.add(
                (
                    class_name,
                    date,
                    frozenset(attendees) # Order of attendees doesn't matter
                )
            )
        except Exception:
            # Handle potential errors if record structure is severely broken
            continue # Skip malformed records
    return record_set

# --- Evaluation Functions ---

def evaluate_task1(candidate_task1: Dict, key_task1: Dict) -> Tuple[int, Dict]:
    """"""Evaluates Task 1: Scheduling.""""""
    score = 0
    details = {}
    max_points = POINTS_CONFIG[""task1""][""max""]

    # Check 1: conflict_identified
    candidate_conflict_id = candidate_task1.get(""conflict_identified"")
    key_conflict_id = key_task1.get(""conflict_identified"")
    if candidate_conflict_id == key_conflict_id and isinstance(candidate_conflict_id, bool):
        score += POINTS_CONFIG[""task1""][""conflict_identified""]
        details[""conflict_identified""] = f""Correct ({key_conflict_id}). +{POINTS_CONFIG['task1']['conflict_identified']} points.""
    else:
        details[""conflict_identified""] = f""Incorrect. Expected '{key_conflict_id}', Got '{candidate_conflict_id}'. +0 points.""

    # Check 2: conflict_description
    # Basic check: if conflict is true, description should not be the 'no conflict' message.
    # If conflict is false, description should be the 'no conflict' message.
    # More robust: Compare against key description (allowing for minor variations is hard, so we'll be strict or check presence)
    candidate_desc = candidate_task1.get(""conflict_description"", """")
    key_desc = key_task1.get(""conflict_description"", """")
    no_conflict_msg = ""No conflict identified."" # Assuming this exact phrase is used

    description_correct = False
    if key_conflict_id is True:
        # Check if candidate description is present and not the 'no conflict' message.
        # A simple check: is it reasonably similar or at least not the negative case?
        # For simplicity, we check if it's not empty and not the 'no conflict' message.
        # A stricter check could compare content, but let's be lenient for basic.
        # Let's check if it's *exactly* the key description for simplicity here.
        if candidate_desc == key_desc:
             description_correct = True
        # Lenient alternative: if candidate_desc and candidate_desc != no_conflict_msg: description_correct = True
    elif key_conflict_id is False:
        if candidate_desc == no_conflict_msg:
            description_correct = True

    if description_correct:
        score += POINTS_CONFIG[""task1""][""conflict_description""]
        details[""conflict_description""] = f""Correct. +{POINTS_CONFIG['task1']['conflict_description']} points.""
    else:
        details[""conflict_description""] = f""Incorrect. Description does not match expectation based on conflict status. Expected logic based on '{key_desc}', Got '{candidate_desc}'. +0 points.""


    # Check 3: updated_schedule
    candidate_schedule = candidate_task1.get(""updated_schedule"", [])
    key_schedule = key_task1.get(""updated_schedule"", [])

    if not isinstance(candidate_schedule, list):
        details[""updated_schedule""] = ""Incorrect format. Expected a list. +0 points.""
    else:
        candidate_schedule_set = create_set_from_list_of_dicts(candidate_schedule)
        key_schedule_set = create_set_from_list_of_dicts(key_schedule)

        if candidate_schedule_set == key_schedule_set:
            score += POINTS_CONFIG[""task1""][""updated_schedule_match""]
            details[""updated_schedule""] = f""Correct. Schedule matches expected content. +{POINTS_CONFIG['task1']['updated_schedule_match']} points.""
        else:
            details[""updated_schedule""] = f""Incorrect. Schedule content does not match expected. Expected {len(key_schedule)} items, Got {len(candidate_schedule)}. Content mismatch. +0 points.""
            # Provide more detail for debugging/feedback
            details[""updated_schedule_expected_items""] = [dict(fs) for fs in key_schedule_set]
            details[""updated_schedule_candidate_items""] = [dict(fs) for fs in candidate_schedule_set]


    return score, {""score"": score, ""max_points"": max_points, ""details"": details}

def evaluate_task2(candidate_task2: Dict, key_task2: Dict) -> Tuple[int, Dict]:
    """"""Evaluates Task 2: Record Keeping.""""""
    score = 0
    details = {}
    max_points = POINTS_CONFIG[""task2""][""max""]

    candidate_records = candidate_task2.get(""attendance_records"", [])
    key_records = key_task2.get(""attendance_records"", [])

    if not isinstance(candidate_records, list):
        details[""attendance_records_structure""] = ""Incorrect format. Expected a list. +0 points.""
        details[""attendance_records_content""] = ""Cannot evaluate content due to incorrect structure.""
        return 0, {""score"": 0, ""max_points"": max_points, ""details"": details}

    # Check 1: Structure (correct number of records)
    if len(candidate_records) == len(key_records):
        score += POINTS_CONFIG[""task2""][""record_structure""]
        details[""attendance_records_structure""] = f""Correct number of records ({len(key_records)}). +{POINTS_CONFIG['task2']['record_structure']} points.""
    else:
        details[""attendance_records_structure""] = f""Incorrect number of records. Expected {len(key_records)}, Got {len(candidate_records)}. +0 points.""
        # Stop content evaluation if structure is wrong? Or try partial matching? Let's try partial.

    # Check 2: Content (match records individually)
    candidate_records_set = create_set_from_attendance_records(candidate_records)
    key_records_set = create_set_from_attendance_records(key_records)

    correct_records_count = len(candidate_records_set.intersection(key_records_set))
    content_score = correct_records_count * POINTS_CONFIG[""task2""][""record_content_match_per_record""]
    score += content_score

    details[""attendance_records_content""] = f""Found {correct_records_count} correctly matching records out of {len(key_records)}. +{content_score} points.""

    if correct_records_count != len(key_records):
         details[""attendance_records_content""] += "" Mismatches detected.""
         # Optional: Add details on missing/extra records if needed for feedback
         missing_records = key_records_set - candidate_records_set
         extra_records = candidate_records_set - key_records_set
         if missing_records:
             details[""missing_records""] = [f""Class: {r[0]}, Date: {r[1]}, Attendees: {sorted(list(r[2]))}"" for r in missing_records]
         if extra_records:
             details[""extra_or_incorrect_records""] = [f""Class: {r[0]}, Date: {r[1]}, Attendees: {sorted(list(r[2]))}"" for r in extra_records]


    # Ensure score doesn't exceed max points for the task
    score = min(score, max_points)

    return score, {""score"": score, ""max_points"": max_points, ""details"": details}


def evaluate_task3(candidate_task3: Dict, key_task3: Dict) -> Tuple[int, Dict]:
    """"""Evaluates Task 3: Reporting.""""""
    score = 0
    details = {}
    max_points = POINTS_CONFIG[""task3""][""max""]

    candidate_summary = candidate_task3.get(""attendance_summary"", [])
    key_summary = key_task3.get(""attendance_summary"", [])

    if not isinstance(candidate_summary, list):
        details[""attendance_summary_structure""] = ""Incorrect format. Expected a list. +0 points.""
        details[""attendance_summary_content""] = ""Cannot evaluate content due to incorrect structure.""
        return 0, {""score"": 0, ""max_points"": max_points, ""details"": details}

    # Check 1: Structure (correct number of summary items)
    if len(candidate_summary) == len(key_summary):
        score += POINTS_CONFIG[""task3""][""summary_structure""]
        details[""attendance_summary_structure""] = f""Correct number of summary items ({len(key_summary)}). +{POINTS_CONFIG['task3']['summary_structure']} points.""
    else:
        details[""attendance_summary_structure""] = f""Incorrect number of summary items. Expected {len(key_summary)}, Got {len(candidate_summary)}. +0 points.""

    # Check 2: Content (match summary items individually)
    candidate_summary_set = create_set_from_list_of_dicts(candidate_summary)
    key_summary_set = create_set_from_list_of_dicts(key_summary)

    correct_items_count = len(candidate_summary_set.intersection(key_summary_set))
    content_score = correct_items_count * POINTS_CONFIG[""task3""][""summary_content_match_per_item""]
    score += content_score

    details[""attendance_summary_content""] = f""Found {correct_items_count} correctly matching summary items out of {len(key_summary)}. +{content_score} points.""

    if correct_items_count != len(key_summary):
         details[""attendance_summary_content""] += "" Mismatches detected.""
         # Optional: Add details on missing/extra items
         missing_items = key_summary_set - candidate_summary_set
         extra_items = candidate_summary_set - key_summary_set
         if missing_items:
             details[""missing_summary_items""] = [dict(fs) for fs in missing_items]
         if extra_items:
             details[""extra_or_incorrect_summary_items""] = [dict(fs) for fs in extra_items]

    # Ensure score doesn't exceed max points for the task
    score = min(score, max_points)

    return score, {""score"": score, ""max_points"": max_points, ""details"": details}

# --- Main Execution ---

def main():
    """"""Main function to orchestrate the evaluation.""""""
    if len(sys.argv) != 3:
        print(""Usage: python task_evaluation.py <candidate_submission.json> <answer_key.json>"", file=sys.stderr)
        sys.exit(1)

    candidate_file = sys.argv[1]
    key_file = sys.argv[2]

    print(f""Loading candidate submission from: {candidate_file}"")
    candidate_data = load_json(candidate_file)
    print(f""Loading answer key from: {key_file}"")
    key_data = load_json(key_file)

    results = {
        ""candidate_file"": candidate_file,
        ""answer_key_file"": key_file,
        ""candidate_id"": candidate_data.get(""candidate_id"", ""ID_MISSING""),
        ""evaluation_details"": {},
        ""overall_score"": 0.0,
        ""total_score_achieved"": 0,
        ""total_possible_points"": TOTAL_POSSIBLE_POINTS
    }

    total_score_achieved = 0

    # --- Evaluate Task 1 ---
    print(""Evaluating Task 1: Scheduling..."")
    try:
        task1_score, task1_results = evaluate_task1(
            candidate_data.get(""task_1_scheduling"", {}),
            key_data.get(""task_1_scheduling"", {})
        )
        results[""evaluation_details""][""task_1_scheduling""] = task1_results
        total_score_achieved += task1_score
        print(f""Task 1 Score: {task1_score}/{task1_results['max_points']}"")
    except Exception as e:
        print(f""Error evaluating Task 1: {e}"", file=sys.stderr)
        results[""evaluation_details""][""task_1_scheduling""] = {""error"": str(e), ""score"": 0, ""max_points"": POINTS_CONFIG[""task1""][""max""]}

    # --- Evaluate Task 2 ---
    print(""Evaluating Task 2: Record Keeping..."")
    try:
        task2_score, task2_results = evaluate_task2(
            candidate_data.get(""task_2_record_keeping"", {}),
            key_data.get(""task_2_record_keeping"", {})
        )
        results[""evaluation_details""][""task_2_record_keeping""] = task2_results
        total_score_achieved += task2_score
        print(f""Task 2 Score: {task2_score}/{task2_results['max_points']}"")
    except Exception as e:
        print(f""Error evaluating Task 2: {e}"", file=sys.stderr)
        results[""evaluation_details""][""task_2_record_keeping""] = {""error"": str(e), ""score"": 0, ""max_points"": POINTS_CONFIG[""task2""][""max""]}

    # --- Evaluate Task 3 ---
    print(""Evaluating Task 3: Reporting..."")
    try:
        task3_score, task3_results = evaluate_task3(
            candidate_data.get(""task_3_reporting"", {}),
            key_data.get(""task_3_reporting"", {})
        )
        results[""evaluation_details""][""task_3_reporting""] = task3_results
        total_score_achieved += task3_score
        print(f""Task 3 Score: {task3_score}/{task3_results['max_points']}"")
    except Exception as e:
        print(f""Error evaluating Task 3: {e}"", file=sys.stderr)
        results[""evaluation_details""][""task_3_reporting""] = {""error"": str(e), ""score"": 0, ""max_points"": POINTS_CONFIG[""task3""][""max""]}

    # --- Calculate Final Score ---
    results[""total_score_achieved""] = total_score_achieved
    if TOTAL_POSSIBLE_POINTS > 0:
        overall_percentage = round((total_score_achieved / TOTAL_POSSIBLE_POINTS) * 100, 2)
    else:
        overall_percentage = 0.0
    results[""overall_score""] = overall_percentage

    print(f""\nTotal Score: {total_score_achieved}/{TOTAL_POSSIBLE_POINTS}"")
    print(f""Overall Percentage: {overall_percentage}%"")

    # --- Save Results ---
    try:
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4)
        print(f""Evaluation results saved to: {OUTPUT_FILENAME}"")
    except Exception as e:
        print(f""Error: Could not write results to {OUTPUT_FILENAME}. Details: {e}"", file=sys.stderr)

if __name__ == ""__main__"":
    main()
```

**Explanation:**

1.  **Imports:** Imports `json` for handling JSON data, `sys` for command-line arguments, `os` for file path checks, and typing hints for clarity.
2.  **Configuration:**
    *   `OUTPUT_FILENAME`: Sets the name for the results file.
    *   `POINTS_CONFIG`: A dictionary defining the points awarded for each specific check within each task and the maximum points per task. This makes the scoring logic easy to adjust.
    *   `TOTAL_POSSIBLE_POINTS`: Calculated automatically by summing the `max` points from `POINTS_CONFIG`.
3.  **Helper Functions:**
    *   `load_json(filepath)`: Safely loads a JSON file, handling `FileNotFoundError` and `json.JSONDecodeError`. Exits if errors occur.
    *   `create_set_from_list_of_dicts(data_list)`: Converts a list of dictionaries into a `set` of `frozenset`s. Each dictionary's items are turned into a `frozenset` (which is hashable), allowing the list of dictionaries to be compared irrespective of the order of dictionaries in the list.
    *   `create_set_from_attendance_records(records)`: A specialized version for Task 2. It converts each record dictionary into a tuple containing `(class_name, date, frozenset(attendees))`. This makes the comparison order-agnostic for both the records list *and* the `attendees` list within each record. It includes basic error handling for missing keys or incorrect attendee list types.
4.  **Evaluation Functions (`evaluate_task1`, `evaluate_task2`, `evaluate_task3`):**
    *   Each function takes the candidate's data and the key's data for that specific task.
    *   They initialize `score` and `details` dictionary.
    *   They safely `.get()` data from the dictionaries to avoid `KeyError` if the candidate's structure is incomplete.
    *   **Task 1:** Compares `conflict_identified` directly, checks `conflict_description` based on the conflict status, and uses the `create_set_from_list_of_dicts` helper to compare the `updated_schedule` lists order-agnostically.
    *   **Task 2:** Checks if the number of records matches the key (structure points). Uses `create_set_from_attendance_records` to compare the content of records order-agnostically (both records and attendees). Points are awarded per correctly matched record.
    *   **Task 3:** Checks if the number of summary items matches (structure points). Uses `create_set_from_list_of_dicts` to compare the summary items order-agnostically. Points are awarded per correctly matched item.
    *   Each function returns the calculated score for the task and a dictionary containing detailed results (score, max points, and specific feedback messages).
5.  **Main Execution (`main` function):**
    *   Checks for the correct number of command-line arguments.
    *   Loads the candidate and key JSON files using `load_json`.
    *   Initializes a `results` dictionary to store all evaluation output.
    *   Calls each `evaluate_taskX` function within a `try...except` block to handle potential errors during evaluation gracefully.
    *   Accumulates the total score.
    *   Calculates the `overall_score` as a percentage.
    *   Prints summary information to the console.
    *   Saves the detailed `results` dictionary to `test_results.json` using `json.dump` with indentation for readability.
6.  **`if __name__ == ""__main__"":`:** Ensures the `main()` function runs only when the script is executed directly.

**To Use:**

1.  Save the code as `task_evaluation.py`.
2.  Make sure you have the candidate's submission file (e.g., `test_submission.json`) and the answer key file (you'll need to save the provided answer key JSON into a file, e.g., `answer_key.json`).
3.  Run from your terminal:
    ```bash
    python task_evaluation.py test_submission.json answer_key.json
    ```
4.  The script will print summary scores to the console and create a `test_results.json` file in the same directory with the detailed evaluation.","{'exam_type': 'basic_maintenance', 'candidate_id': 'ANSWER_KEY_DO_NOT_MODIFY', 'task_1_scheduling': {'conflict_identified': True, 'conflict_description': ""Requested 'Gentle Stretching' on Thursday 10:00 in Studio B conflicts with existing 'Pilates Core' at the same time and location."", 'updated_schedule': [{'class_name': 'Morning Yoga', 'instructor': 'Anya Sharma', 'day': 'Monday', 'time': '09:00', 'room': 'Studio A'}, {'class_name': 'Cycle Fit', 'instructor': 'Ben Carter', 'day': 'Tuesday', 'time': '17:30', 'room': 'Cycle Studio'}, {'class_name': 'Pilates Core', 'instructor': 'Anya Sharma', 'day': 'Thursday', 'time': '10:00', 'room': 'Studio B'}, {'class_name': 'Aqua Aerobics', 'instructor': 'Chloe Davis', 'day': 'Friday', 'time': '11:00', 'room': 'Pool'}, {'class_name': 'Strength Circuit', 'instructor': 'Ben Carter', 'day': 'Thursday', 'time': '18:00', 'room': 'Gym Floor'}]}, 'task_2_record_keeping': {'attendance_records': [{'class_name': 'Cycle Fit', 'date': '2024-08-06', 'attendees': ['John Smith', 'Emily White', 'Omar Khan']}, {'class_name': 'Aqua Aerobics', 'date': '2024-08-09', 'attendees': ['Sarah Jones', 'David Chen']}, {'class_name': 'Morning Yoga', 'date': '2024-08-05', 'attendees': ['Lisa Green', 'Mike Brown', 'Emily White', 'Anna Bell']}]}, 'task_3_reporting': {'attendance_summary': [{'class_name': 'Cycle Fit', 'total_attendees': 3}, {'class_name': 'Aqua Aerobics', 'total_attendees': 2}, {'class_name': 'Morning Yoga', 'total_attendees': 4}]}}",[],True,True,0.0,99.0,100.0,1.0,False,"Could not parse JSON. Raw LLM response:
```json
{
  ""makes_sense"": true,
  ""explanation"": ""The exam is practical, testing core tasks (scheduling, record keeping, basic reporting) relevant to a Fitness and Wellness Coordinator using realistic tools (spreadsheet, text editor) and data formats (text files, JSON). The tasks are appropriate for a basic level in 2025. Instructions, materials, and submission format are generally clear. The answer key correctly reflects the tasks and should score 100% on the provided grading script due to the script's reliance on exact or set-based matching.\n\nPotential Weaknesses:\n1.  **Grading Script vs. Criteria Mismatch:** The written 'Passing Criteria' mention flexibility (e.g., 'reasonably identify' conflict description, allowance for 1 minor transcription error), but the Python grading script implements stricter checks (exact match for description, exact set comparison for attendees which penalizes any misspelling). This could lead to false negatives.\n2.  **Implicit Pass/Fail:** The script calculates a score, but the 'Passing Criteria' define certain elements as 'Essential' for passing (e.g., correct conflict identification). The script doesn't explicitly enforce these as pass/fail gates; an evaluator must manually interpret the score breakdown against the criteria. A candidate could fail an essential part but still achieve a non-zero score.\n3.  **Conflict Description Strictness:** The script's current exact match for the conflict description might unfairly penalize correct descriptions phrased slightly differently.""
}
```","{'overview': prompt_token_count: 245
candidates_token_count: 1997
total_token_count: 4299
, 'instructions': prompt_token_count: 2448
candidates_token_count: 1395
total_token_count: 4782
, 'materials': prompt_token_count: 4048
candidates_token_count: 1222
total_token_count: 6373
, 'check_materials': prompt_token_count: 2721
candidates_token_count: 1
total_token_count: 3390
, 'check_website': prompt_token_count: 2754
candidates_token_count: 1
total_token_count: 3265
, 'submission': prompt_token_count: 5054
candidates_token_count: 1116
total_token_count: 6900
, 'evaluation': prompt_token_count: 6091
candidates_token_count: 1943
total_token_count: 9332
, 'grading': prompt_token_count: 8223
candidates_token_count: 5374
total_token_count: 15817
, 'check_sense': prompt_token_count: 13995
candidates_token_count: 322
total_token_count: 16620
}",management_occupations
Regulatory Affairs Managers,18027.0,Establish regulatory priorities or budgets and allocate resources and workloads.,gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor', 'PDF viewer']","['Text', 'Data']","Okay, here are the candidate instructions for the Basic Level practical exam.

---

## Practical Exam: Regulatory Prioritization and Workload Allocation (Basic Level)

**Welcome, Candidate.**

This practical exam assesses your ability to establish basic regulatory priorities and allocate resources and workloads for a short-term period using provided data and standard office tools.

**Time Limit:** 90 minutes

**Available Tools:**
*   Spreadsheet software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   Text editor (e.g., Notepad, VS Code, TextEdit)
*   PDF viewer

### Your Goal

Analyze a list of regulatory tasks and available team resources to create a prioritized work plan for the **next 4 weeks**. You need to allocate tasks to team members based on defined rules and identify any potential issues. Your final output must be a structured JSON file.

### Provided Materials

You will be provided with the following digital files:

1.  **`Regulatory_Tasks.csv`**: A comma-separated value file containing a list of regulatory tasks. Key columns include:
    *   `Task_ID`: Unique identifier for each task.
    *   `Task_Description`: Brief description of the task.
    *   `Region_Agency`: Relevant region or agency.
    *   `Due_Date`: Deadline for the task (YYYY-MM-DD or ""N/A""). Assume the planning period starts **2024-07-01**.
    *   `Estimated_Effort_Person_Days`: Estimated work effort in person-days.
    *   `Initial_Notes`: Additional context (e.g., business impact, risk).

2.  **`Team_Resources.csv`**: A comma-separated value file listing the available team members. Key columns include:
    *   `Resource_ID`: Unique identifier for each team member.
    *   `Name`: Name of the team member.
    *   `Primary_Expertise`: Main area of regulatory expertise (e.g., EU Submissions, US CMC, General Regulatory Ops). This directly relates to the types of tasks they can perform.
    *   `Available_Person_Days`: Total available workdays for this resource over the 4-week planning period.

### Your Tasks

1.  **Analyze Data:** Open and review the `Regulatory_Tasks.csv` and `Team_Resources.csv` files using your spreadsheet software. Understand the tasks, their requirements, deadlines, effort, and the available resources' expertise and capacity.
2.  **Prioritize Tasks:** Assign a priority rank to **all** tasks listed in `Regulatory_Tasks.csv`. Use the following strict rules in the order presented:
    *   **Rule 1 (Deadline):** Tasks with a `Due_Date` between **2024-07-01 and 2024-07-28** (inclusive) have the highest priority. Rank these tasks with the earliest due date highest.
    *   **Rule 2 (Business Impact):** For tasks not prioritized by Rule 1, prioritize those where `Initial_Notes` contains the exact phrase ""**High business impact**"".
    *   **Rule 3 (Compliance Risk):** For tasks not prioritized by Rule 1 or 2, prioritize those where `Initial_Notes` contains the exact phrase ""**Compliance risk**"".
    *   **Rule 4 (Tie-breaker):** If tasks have the same priority level after applying Rules 1-3, rank the task with the lower `Task_ID` (e.g., T01 before T02) as higher priority.
    *   Assign a sequential numerical priority rank (`assigned_priority`), starting with `1` for the highest priority task. Every task must receive a unique rank.
3.  **Allocate Tasks:** Assign the prioritized tasks to team members (`Resource_ID`) based on the following rules:
    *   **Expertise Match:** Assign tasks only to resources whose `Primary_Expertise` matches the task requirement (you will need to infer the required expertise from the `Task_Description` and/or `Region_Agency` - e.g., an ""EU"" submission task requires ""EU Submissions"" expertise; a ""CMC"" task requires ""US CMC"" or potentially ""General Regulatory Ops"" if applicable).
    *   **Capacity:** Do not assign tasks to a resource if the task's `Estimated_Effort_Person_Days` exceeds the resource's *remaining* `Available_Person_Days` at the point of allocation.
    *   **Priority Order:** Allocate tasks strictly in the order of their `assigned_priority` (from highest priority rank `1` downwards).
    *   **Resource Selection Tie-breaker:** If multiple available resources have the matching expertise and sufficient remaining capacity for a task, assign it to the resource who currently has *more* remaining `Available_Person_Days`. If there is still a tie, assign it to the resource with the lower `Resource_ID` (e.g., R01 before R02).
    *   **No Splitting:** Each task must be allocated entirely to a single resource. Do not split tasks.
4.  **Identify Unallocated Tasks:** List any tasks that could not be assigned to any resource according to the rules. For each unallocated task, select the most appropriate reason from the predefined list below:
    *   `INSUFFICIENT_CAPACITY`: Not enough total available days across *all* potentially suitable resources.
    *   `NO_MATCHING_EXPERTISE`: No available resource possesses the required expertise.
    *   `RESOURCE_OVERLOADED`: Suitable resource(s) exist but are already fully allocated with higher-priority tasks.
5.  **Summarize Resource Workload:** For each resource, calculate the total `Estimated_Effort_Person_Days` allocated to them. Determine their workload status using these definitions:
    *   `total_allocated_days` > `total_available_days`: Status is ""**Over-allocated**""
    *   `total_allocated_days` < (0.8 * `total_available_days`): Status is ""**Under-allocated**""
    *   Otherwise (allocated days are >= 80% and <= 100% of available days): Status is ""**Within Capacity**""
6.  **Format Output:** Compile your results into a single JSON file named `test_submission.json` using a text editor. Follow the exact structure specified below.

### Submission Format

Create a JSON file named `test_submission.json` with the following structure. Ensure all keys and expected value types match precisely. Replace placeholder text (like `""CANDIDATE_UNIQUE_IDENTIFIER""`) with your identifier if provided, otherwise leave it as is. Use the exact predefined strings for `status`, `reason`, and `workload_status`.

```json
{
  ""exam_level"": ""basic"",
  ""candidate_id"": ""CANDIDATE_UNIQUE_IDENTIFIER"",
  ""planning_period_weeks"": 4,
  ""prioritized_allocation"": [
    // Add one object here for EACH task that WAS successfully allocated
    {
      ""task_id"": ""TXX"", // e.g., ""T01"" (from Regulatory_Tasks.csv)
      ""assigned_priority"": 1, // Numerical rank based on prioritization rules
      ""assigned_resource_id"": ""RXX"", // e.g., ""R02"" (from Team_Resources.csv)
      ""status"": ""Allocated"" // Must be this exact string
    }
    // ... more allocated tasks, ordered by assigned_priority
  ],
  ""unallocated_tasks"": [
    // Add one object here for EACH task that could NOT be allocated
    {
      ""task_id"": ""TXX"", // e.g., ""T07""
      ""assigned_priority"": 5, // The priority rank it received before allocation failed
      ""reason"": ""RESOURCE_OVERLOADED"" // Must be one of: INSUFFICIENT_CAPACITY, NO_MATCHING_EXPERTISE, RESOURCE_OVERLOADED
    }
    // ... more unallocated tasks, ordered by assigned_priority
  ],
  ""resource_summary"": [
    // Add one object here for EACH resource listed in Team_Resources.csv
    {
      ""resource_id"": ""RXX"", // e.g., ""R01""
      ""total_available_days"": 20, // From Team_Resources.csv
      ""total_allocated_days"": 18, // Sum of effort for tasks assigned to this resource
      ""workload_status"": ""Within Capacity"" // Must be one of: Over-allocated, Under-allocated, Within Capacity
    }
    // ... summary for all other resources, ordered by resource_id
  ],
  ""overall_comments"": ""Optional: Add any brief, objective comments here if necessary. This field is not the primary focus of evaluation.""
}
```

**Important Notes:**

*   Adhere strictly to the prioritization and allocation rules provided.
*   Ensure your JSON is valid and follows the specified structure exactly. Use a JSON validator if needed before finalizing.
*   Base all your calculations and decisions solely on the data provided in the `.csv` files and the rules in these instructions.
*   Save your final output as `test_submission.json`.

**Begin the exam when you are ready.** Good luck!

---
**1. `Regulatory_Tasks.csv`**

```csv
Task_ID,Task_Description,Region_Agency,Due_Date,Estimated_Effort_Person_Days,Initial_Notes
T01,Respond to EMA CHMP LoQ - Product A,EU EMA,2024-07-15,8,Compliance risk
T02,Compile Module 3 Update - Product B,US FDA,2024-08-10,10,High business impact
T03,Submit Annual Report - Product C,HC Canada,2024-07-25,5,Standard procedure
T04,Prepare Briefing Book - Product D,US FDA,2024-07-20,7,High business impact
T05,Initiate new MAA preparation - Prod E,EU EMA,N/A,15,High business impact
T06,Update Core Data Sheet - Product A,Global,N/A,6,Compliance risk
T07,Prepare response to HA query - Prod B,US FDA,2024-07-18,9,Urgent request
T08,Review Promotional Material - Prod C,EU National,N/A,4,Standard procedure
T09,Submit Variation Type IB - Product A,EU EMA,N/A,3,Standard procedure
T10,Prepare Orphan Drug App - Product F,US FDA,N/A,12,High business impact
T11,Address TGA Questions - Product G,AU TGA,2024-07-22,6,Compliance risk
T12,Finalize SOP Update - Reg Processes,Internal,2024-07-26,4,Compliance risk
```

**2. `Team_Resources.csv`**

```csv
Resource_ID,Name,Primary_Expertise,Available_Person_Days
R01,Alex Smith,EU Submissions,20
R02,Brenda Lee,US CMC,18
R03,Chris Wong,General Regulatory Ops,15
```

**3. Expertise Inference Guidelines (Included in Candidate Instructions)**

*   Tasks involving `EU EMA` or `EU National` require **`EU Submissions`** expertise.
*   Tasks involving `US FDA` and descriptions like `Module 3 Update`, `Briefing Book`, or `Orphan Drug App` require **`US CMC`** expertise.
*   Tasks involving `HC Canada` Annual Reports, `Global` Core Data Sheet updates, or `Internal` SOP updates require **`General Regulatory Ops`** expertise.
*   Tasks involving other agencies (e.g., `AU TGA`) require expertise specific to that region (e.g., `APAC Submissions`), which may or may not be available in the team.

Okay, here is the specification for the candidate's submission format.

## Submission Format Specification

Your submission must be a single JSON file named `test_submission.json`. This file must contain a single JSON object adhering strictly to the structure, keys, data types, and formats described below.

**File Name:** `test_submission.json`

**Content:** A single JSON object.

**JSON Structure and Field Formats:**

```json
{
  ""exam_level"": ""basic"", // Must be the exact string ""basic""
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace ""YOUR_ID_HERE"" with the specific model version you are using (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet"", etc.)
  ""planning_period_weeks"": 4, // Must be the integer 4
  ""prioritized_allocation"": [
    // This is an array of objects. Include one object for EACH task that was successfully allocated.
    // Order these objects by ""assigned_priority"" in ascending order (1, 2, 3...).
    {
      ""task_id"": ""TXX"", // String. Must match a Task_ID from Regulatory_Tasks.csv (e.g., ""T01"")
      ""assigned_priority"": 1, // Integer. The calculated priority rank (e.g., 1, 2, 3...)
      ""assigned_resource_id"": ""RXX"", // String. Must match a Resource_ID from Team_Resources.csv (e.g., ""R02"")
      ""status"": ""Allocated"" // String. Must be the exact string ""Allocated""
    }
    // ... potentially more objects for other allocated tasks
  ],
  ""unallocated_tasks"": [
    // This is an array of objects. Include one object for EACH task that could NOT be allocated.
    // Order these objects by ""assigned_priority"" in ascending order.
    {
      ""task_id"": ""TXX"", // String. Must match a Task_ID from Regulatory_Tasks.csv (e.g., ""T07"")
      ""assigned_priority"": 5, // Integer. The calculated priority rank the task received before allocation failed.
      ""reason"": ""PREDEFINED_REASON"" // String. Must be one of the exact strings: ""INSUFFICIENT_CAPACITY"", ""NO_MATCHING_EXPERTISE"", ""RESOURCE_OVERLOADED""
    }
    // ... potentially more objects for other unallocated tasks
  ],
  ""resource_summary"": [
    // This is an array of objects. Include one object for EACH resource listed in Team_Resources.csv.
    // Order these objects by ""resource_id"" in alphanumeric order (e.g., R01, R02, R03...).
    {
      ""resource_id"": ""RXX"", // String. Must match a Resource_ID from Team_Resources.csv (e.g., ""R01"")
      ""total_available_days"": 20, // Integer. The value from Team_Resources.csv for this resource.
      ""total_allocated_days"": 18, // Integer. The calculated sum of Estimated_Effort_Person_Days for tasks assigned to this resource. Must be a whole number.
      ""workload_status"": ""PREDEFINED_STATUS"" // String. Must be one of the exact strings: ""Over-allocated"", ""Under-allocated"", ""Within Capacity""
    }
    // ... objects for all other resources
  ],
  ""overall_comments"": ""Optional string."" // String. Can be an empty string """" or contain brief text comments.
}
```

**Key Formatting Requirements:**

*   **JSON Validity:** The file must be a valid JSON object. Use a JSON validator if unsure.
*   **Keys:** Use the exact key names as specified (e.g., `""task_id""`, `""assigned_priority""`).
*   **String Values:** Where specific string values are required (e.g., `""Allocated""`, `""basic""`, `""INSUFFICIENT_CAPACITY""`), use them exactly as shown, including capitalization.
*   **Data Types:** Ensure all values match the specified data types (string, integer, array, object).
*   **Numbers:** All numerical values (`planning_period_weeks`, `assigned_priority`, `total_available_days`, `total_allocated_days`) must be integers (whole numbers). Do not use decimals.
*   **Order:** Objects within the `prioritized_allocation` and `unallocated_tasks` arrays must be ordered by `assigned_priority` (ascending). Objects within the `resource_summary` array must be ordered by `resource_id` (alphanumeric).
*   **Candidate ID:** Replace `""YOUR_ID_HERE""` in the `candidate_id` field with the specific model version identifier provided to you or inherent to your execution environment (e.g., `""GPT-4o""`).
*   **Completeness:** Ensure all required sections (`prioritized_allocation`, `unallocated_tasks`, `resource_summary`) are present, even if an array is empty (e.g., `[]` if no tasks were unallocated). Include entries for *all* tasks (either allocated or unallocated) and *all* resources.

**Supplementary Files:**

*   No supplementary files are required. Submit only the `test_submission.json` file.","You are an excellent examiner of Regulatory Affairs Managers capabilities. Design a remote, **practical** exam to verify whether a Regulatory Affairs Managers can Establish regulatory priorities or budgets and allocate resources and workloads..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets, Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the design for the **Basic Level** practical exam focusing on establishing regulatory priorities and allocating resources/workloads.

## Basic Exam: Regulatory Prioritization and Workload Allocation

**Exam Purpose for Evaluator:**

This basic-level practical exam assesses the candidate's fundamental ability to analyze a set of regulatory tasks, evaluate available resources, establish priorities based on given criteria, and allocate workload effectively. It simulates a common scenario faced by RA Managers where they need to create a short-term operational plan. The focus is on logical prioritization, resource matching, workload balancing (at a simple level), and clear documentation of the plan and rationale within a defined structure (JSON). It tests practical application rather than theoretical knowledge. Budgeting aspects are simplified to focus on time/effort allocation.

**Exam Structure for Evaluator:**

1.  **Scenario & Input Data (Provided to Candidate):** The candidate receives:
    *   `Instructions.pdf`: Explains the task, the timeframe (e.g., ""plan for the next 4 weeks""), the goal (prioritize tasks, allocate resources, identify issues), prioritization guidelines (e.g., compliance deadlines > major submissions > minor variations), and the required JSON output format. Specifies the 90-minute time limit.
    *   `Regulatory_Tasks.csv`: A spreadsheet-compatible file listing potential regulatory activities. Columns include:
        *   `Task_ID` (Unique identifier, e.g., T01, T02)
        *   `Task_Description` (e.g., ""Prepare response to HA query - Product A EU"", ""Compile Module 3 update - Product B US"", ""Submit Annual Report - Product C Canada"", ""Initiate new MAA preparation - Product D EU"")
        *   `Region_Agency` (e.g., EU EMA, US FDA, HC Canada)
        *   `Due_Date` (YYYY-MM-DD or ""N/A"")
        *   `Estimated_Effort_Person_Days` (e.g., 5, 10, 3, 15)
        *   `Initial_Notes` (e.g., ""High business impact"", ""Standard procedure"", ""Requires CMC expertise"", ""Potential delay risk"")
    *   `Team_Resources.csv`: A spreadsheet-compatible file listing available team members. Columns include:
        *   `Resource_ID` (Unique identifier, e.g., R01, R02, R03)
        *   `Name` (e.g., Alex Smith, Brenda Lee, Chris Wong)
        *   `Primary_Expertise` (e.g., EU Submissions, US CMC, General Regulatory Ops)
        *   `Available_Person_Days` (Total available days for the planning period, e.g., 20, 20, 15 - assuming a 4-week period, potentially accounting for partial availability)

2.  **Candidate Task:**
    *   Analyze the `Regulatory_Tasks.csv` and `Team_Resources.csv` data using a spreadsheet tool.
    *   Based on the `Instructions.pdf` guidelines (especially prioritization rules) and the task data (due dates, effort, notes), prioritize the tasks.
    *   Allocate the prioritized tasks to appropriate team members (`Resource_ID`) considering their expertise, available days, and the task's estimated effort.
    *   Identify any tasks that cannot be allocated due to resource constraints or conflicts.
    *   Identify any resources that appear significantly over or under-allocated.
    *   Document the plan and findings in the specified JSON format using a text editor.

3.  **Candidate Submission:**
    *   A single file named `test_submission.json`.

**JSON Output Structure (`test_submission.json`):**

```json
{
  ""exam_level"": ""basic"",
  ""candidate_id"": ""CANDIDATE_UNIQUE_IDENTIFIER"", // To be filled by candidate or pre-filled
  ""planning_period_weeks"": 4, // Example, should match instructions
  ""prioritized_allocation"": [
    {
      ""task_id"": ""T01"", // From Regulatory_Tasks.csv
      ""assigned_priority"": 1, // Numerical rank, 1 = highest
      ""assigned_resource_id"": ""R02"", // From Team_Resources.csv
      ""justification_priority"": ""Critical compliance deadline."", // Brief rationale for priority
      ""justification_allocation"": ""Matches R02 expertise (US CMC) and fits availability."", // Brief rationale for resource choice
      ""status"": ""Allocated"" // ""Allocated"" or ""Partially Allocated""
    },
    {
      ""task_id"": ""T04"",
      ""assigned_priority"": 2,
      ""assigned_resource_id"": ""R01"",
      ""justification_priority"": ""High business impact (new MAA)."",
      ""justification_allocation"": ""Matches R01 expertise (EU Submissions). Effort requires significant portion of R01 time."",
      ""status"": ""Allocated""
    },
    // ... other allocated tasks
  ],
  ""unallocated_tasks"": [
    {
      ""task_id"": ""T07"",
      ""reason"": ""Insufficient resource availability with required expertise (e.g., EU Submissions specialist fully booked).""
    }
    // ... other tasks that could not be allocated
  ],
  ""resource_summary"": [
    {
      ""resource_id"": ""R01"",
      ""total_available_days"": 20,
      ""total_allocated_days"": 18, // Sum of 'Estimated_Effort_Person_Days' for tasks assigned to R01
      ""workload_comment"": ""Near full allocation."" // e.g., ""Under-allocated"", ""Fully allocated"", ""Over-allocated risk""
    },
    {
      ""resource_id"": ""R02"",
      ""total_available_days"": 20,
      ""total_allocated_days"": 22,
      ""workload_comment"": ""Slightly over-allocated. Requires monitoring or task adjustment.""
    },
    {
      ""resource_id"": ""R03"",
      ""total_available_days"": 15,
      ""total_allocated_days"": 8,
      ""workload_comment"": ""Significantly under-allocated. Potential for additional tasks.""
    }
    // ... summary for each resource
  ],
  ""overall_comments"": ""Optional field for candidate to add any overarching comments or identified risks not captured elsewhere.""
}
```

**Evaluation Criteria (for the Evaluator):**

*   **Prioritization Logic:** Does the `assigned_priority` ranking logically follow the guidelines in `Instructions.pdf` and consider factors like deadlines and stated importance? Is the `justification_priority` clear and relevant?
*   **Resource Allocation Appropriateness:** Are tasks assigned to resources with matching `Primary_Expertise`? Is the allocation reasonable given the `Estimated_Effort_Person_Days` and `Available_Person_Days`? Is the `justification_allocation` sound?
*   **Workload Balancing Awareness:** Does the `resource_summary` accurately reflect the calculated allocation? Does the `workload_comment` correctly identify potential over/under-allocation issues?
*   **Problem Identification:** Are `unallocated_tasks` correctly identified with valid reasons?
*   **Completeness and Accuracy:** Is the JSON structure correct? Are all necessary fields populated accurately based on the candidate's analysis?
*   **Efficiency:** While not directly timed during evaluation, the plan should be achievable within the 90-minute timeframe, indicating efficient analysis and decision-making.

This basic exam provides a concrete task allowing the candidate to demonstrate practical skills in prioritization and resource allocation using standard office tools in a remote setting.","Okay, here are the candidate instructions for the Basic Level practical exam.

---

## Practical Exam: Regulatory Prioritization and Workload Allocation (Basic Level)

**Welcome, Candidate.**

This practical exam assesses your ability to establish basic regulatory priorities and allocate resources and workloads for a short-term period using provided data and standard office tools.

**Time Limit:** 90 minutes

**Available Tools:**
*   Spreadsheet software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   Text editor (e.g., Notepad, VS Code, TextEdit)
*   PDF viewer

### Your Goal

Analyze a list of regulatory tasks and available team resources to create a prioritized work plan for the **next 4 weeks**. You need to allocate tasks to team members based on defined rules and identify any potential issues. Your final output must be a structured JSON file.

### Provided Materials

You will be provided with the following digital files:

1.  **`Regulatory_Tasks.csv`**: A comma-separated value file containing a list of regulatory tasks. Key columns include:
    *   `Task_ID`: Unique identifier for each task.
    *   `Task_Description`: Brief description of the task.
    *   `Region_Agency`: Relevant region or agency.
    *   `Due_Date`: Deadline for the task (YYYY-MM-DD or ""N/A""). Assume the planning period starts **2024-07-01**.
    *   `Estimated_Effort_Person_Days`: Estimated work effort in person-days.
    *   `Initial_Notes`: Additional context (e.g., business impact, risk).

2.  **`Team_Resources.csv`**: A comma-separated value file listing the available team members. Key columns include:
    *   `Resource_ID`: Unique identifier for each team member.
    *   `Name`: Name of the team member.
    *   `Primary_Expertise`: Main area of regulatory expertise (e.g., EU Submissions, US CMC, General Regulatory Ops). This directly relates to the types of tasks they can perform.
    *   `Available_Person_Days`: Total available workdays for this resource over the 4-week planning period.

### Your Tasks

1.  **Analyze Data:** Open and review the `Regulatory_Tasks.csv` and `Team_Resources.csv` files using your spreadsheet software. Understand the tasks, their requirements, deadlines, effort, and the available resources' expertise and capacity.
2.  **Prioritize Tasks:** Assign a priority rank to **all** tasks listed in `Regulatory_Tasks.csv`. Use the following strict rules in the order presented:
    *   **Rule 1 (Deadline):** Tasks with a `Due_Date` between **2024-07-01 and 2024-07-28** (inclusive) have the highest priority. Rank these tasks with the earliest due date highest.
    *   **Rule 2 (Business Impact):** For tasks not prioritized by Rule 1, prioritize those where `Initial_Notes` contains the exact phrase ""**High business impact**"".
    *   **Rule 3 (Compliance Risk):** For tasks not prioritized by Rule 1 or 2, prioritize those where `Initial_Notes` contains the exact phrase ""**Compliance risk**"".
    *   **Rule 4 (Tie-breaker):** If tasks have the same priority level after applying Rules 1-3, rank the task with the lower `Task_ID` (e.g., T01 before T02) as higher priority.
    *   Assign a sequential numerical priority rank (`assigned_priority`), starting with `1` for the highest priority task. Every task must receive a unique rank.
3.  **Allocate Tasks:** Assign the prioritized tasks to team members (`Resource_ID`) based on the following rules:
    *   **Expertise Match:** Assign tasks only to resources whose `Primary_Expertise` matches the task requirement (you will need to infer the required expertise from the `Task_Description` and/or `Region_Agency` - e.g., an ""EU"" submission task requires ""EU Submissions"" expertise; a ""CMC"" task requires ""US CMC"" or potentially ""General Regulatory Ops"" if applicable).
    *   **Capacity:** Do not assign tasks to a resource if the task's `Estimated_Effort_Person_Days` exceeds the resource's *remaining* `Available_Person_Days` at the point of allocation.
    *   **Priority Order:** Allocate tasks strictly in the order of their `assigned_priority` (from highest priority rank `1` downwards).
    *   **Resource Selection Tie-breaker:** If multiple available resources have the matching expertise and sufficient remaining capacity for a task, assign it to the resource who currently has *more* remaining `Available_Person_Days`. If there is still a tie, assign it to the resource with the lower `Resource_ID` (e.g., R01 before R02).
    *   **No Splitting:** Each task must be allocated entirely to a single resource. Do not split tasks.
4.  **Identify Unallocated Tasks:** List any tasks that could not be assigned to any resource according to the rules. For each unallocated task, select the most appropriate reason from the predefined list below:
    *   `INSUFFICIENT_CAPACITY`: Not enough total available days across *all* potentially suitable resources.
    *   `NO_MATCHING_EXPERTISE`: No available resource possesses the required expertise.
    *   `RESOURCE_OVERLOADED`: Suitable resource(s) exist but are already fully allocated with higher-priority tasks.
5.  **Summarize Resource Workload:** For each resource, calculate the total `Estimated_Effort_Person_Days` allocated to them. Determine their workload status using these definitions:
    *   `total_allocated_days` > `total_available_days`: Status is ""**Over-allocated**""
    *   `total_allocated_days` < (0.8 * `total_available_days`): Status is ""**Under-allocated**""
    *   Otherwise (allocated days are >= 80% and <= 100% of available days): Status is ""**Within Capacity**""
6.  **Format Output:** Compile your results into a single JSON file named `test_submission.json` using a text editor. Follow the exact structure specified below.

### Submission Format

Create a JSON file named `test_submission.json` with the following structure. Ensure all keys and expected value types match precisely. Replace placeholder text (like `""CANDIDATE_UNIQUE_IDENTIFIER""`) with your identifier if provided, otherwise leave it as is. Use the exact predefined strings for `status`, `reason`, and `workload_status`.

```json
{
  ""exam_level"": ""basic"",
  ""candidate_id"": ""CANDIDATE_UNIQUE_IDENTIFIER"",
  ""planning_period_weeks"": 4,
  ""prioritized_allocation"": [
    // Add one object here for EACH task that WAS successfully allocated
    {
      ""task_id"": ""TXX"", // e.g., ""T01"" (from Regulatory_Tasks.csv)
      ""assigned_priority"": 1, // Numerical rank based on prioritization rules
      ""assigned_resource_id"": ""RXX"", // e.g., ""R02"" (from Team_Resources.csv)
      ""status"": ""Allocated"" // Must be this exact string
    }
    // ... more allocated tasks, ordered by assigned_priority
  ],
  ""unallocated_tasks"": [
    // Add one object here for EACH task that could NOT be allocated
    {
      ""task_id"": ""TXX"", // e.g., ""T07""
      ""assigned_priority"": 5, // The priority rank it received before allocation failed
      ""reason"": ""RESOURCE_OVERLOADED"" // Must be one of: INSUFFICIENT_CAPACITY, NO_MATCHING_EXPERTISE, RESOURCE_OVERLOADED
    }
    // ... more unallocated tasks, ordered by assigned_priority
  ],
  ""resource_summary"": [
    // Add one object here for EACH resource listed in Team_Resources.csv
    {
      ""resource_id"": ""RXX"", // e.g., ""R01""
      ""total_available_days"": 20, // From Team_Resources.csv
      ""total_allocated_days"": 18, // Sum of effort for tasks assigned to this resource
      ""workload_status"": ""Within Capacity"" // Must be one of: Over-allocated, Under-allocated, Within Capacity
    }
    // ... summary for all other resources, ordered by resource_id
  ],
  ""overall_comments"": ""Optional: Add any brief, objective comments here if necessary. This field is not the primary focus of evaluation.""
}
```

**Important Notes:**

*   Adhere strictly to the prioritization and allocation rules provided.
*   Ensure your JSON is valid and follows the specified structure exactly. Use a JSON validator if needed before finalizing.
*   Base all your calculations and decisions solely on the data provided in the `.csv` files and the rules in these instructions.
*   Save your final output as `test_submission.json`.

**Begin the exam when you are ready.** Good luck!

---","Okay, here are the materials for the candidate and the explanation for the evaluator.

<MATERIALS_FOR_CANDIDATE>
**1. `Regulatory_Tasks.csv`**

```csv
Task_ID,Task_Description,Region_Agency,Due_Date,Estimated_Effort_Person_Days,Initial_Notes
T01,Respond to EMA CHMP LoQ - Product A,EU EMA,2024-07-15,8,Compliance risk
T02,Compile Module 3 Update - Product B,US FDA,2024-08-10,10,High business impact
T03,Submit Annual Report - Product C,HC Canada,2024-07-25,5,Standard procedure
T04,Prepare Briefing Book - Product D,US FDA,2024-07-20,7,High business impact
T05,Initiate new MAA preparation - Prod E,EU EMA,N/A,15,High business impact
T06,Update Core Data Sheet - Product A,Global,N/A,6,Compliance risk
T07,Prepare response to HA query - Prod B,US FDA,2024-07-18,9,Urgent request
T08,Review Promotional Material - Prod C,EU National,N/A,4,Standard procedure
T09,Submit Variation Type IB - Product A,EU EMA,N/A,3,Standard procedure
T10,Prepare Orphan Drug App - Product F,US FDA,N/A,12,High business impact
T11,Address TGA Questions - Product G,AU TGA,2024-07-22,6,Compliance risk
T12,Finalize SOP Update - Reg Processes,Internal,2024-07-26,4,Compliance risk
```

**2. `Team_Resources.csv`**

```csv
Resource_ID,Name,Primary_Expertise,Available_Person_Days
R01,Alex Smith,EU Submissions,20
R02,Brenda Lee,US CMC,18
R03,Chris Wong,General Regulatory Ops,15
```

**3. Expertise Inference Guidelines (Included in Candidate Instructions)**

*   Tasks involving `EU EMA` or `EU National` require **`EU Submissions`** expertise.
*   Tasks involving `US FDA` and descriptions like `Module 3 Update`, `Briefing Book`, or `Orphan Drug App` require **`US CMC`** expertise.
*   Tasks involving `HC Canada` Annual Reports, `Global` Core Data Sheet updates, or `Internal` SOP updates require **`General Regulatory Ops`** expertise.
*   Tasks involving other agencies (e.g., `AU TGA`) require expertise specific to that region (e.g., `APAC Submissions`), which may or may not be available in the team.

</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. How Materials Were Created:**

*   The materials simulate a realistic, albeit simplified, set of regulatory tasks and a small team.
*   **`Regulatory_Tasks.csv`**: Contains 12 tasks designed to test all prioritization rules and create specific allocation challenges.
    *   **Deadlines:** Tasks T01, T07, T04, T11, T03, T12 fall within the 4-week planning window (2024-07-01 to 2024-07-28) and must be prioritized first based on their date.
    *   **Keywords:** Tasks T02, T05, T10 contain ""High business impact"". Tasks T01, T06, T11, T12 contain ""Compliance risk"" (note T01 and T11/T12 are already prioritized by deadline). T06 uses this rule.
    *   **Tie-breakers:** The prioritization rules are designed to create a unique rank for every task. The Task_ID tie-breaker (Rule 4) is needed to order T02, T05, T10 (all High business impact, no deadline) and T08, T09 (both standard procedure, no deadline/keywords).
    *   **Effort:** `Estimated_Effort_Person_Days` vary to test capacity constraints. Some are large (T05, T10), potentially exceeding individual capacity or remaining capacity after higher-priority tasks are assigned.
    *   **Expertise:** Tasks are designed to map clearly to the defined `Primary_Expertise` based on the provided inference guidelines. T11 (AU TGA) deliberately requires expertise (`APAC Submissions`) that no resource possesses.
*   **`Team_Resources.csv`**: Contains 3 resources with distinct expertise and slightly different availabilities.
    *   R01 (`EU Submissions`, 20 days): Standard availability.
    *   R02 (`US CMC`, 18 days): Slightly reduced availability, making capacity tighter.
    *   R03 (`General Regulatory Ops`, 15 days): Lower availability, representing maybe a part-time role or other commitments.
    *   The expertise areas are distinct, simplifying the initial allocation choice but setting up potential bottlenecks.

**2. Cross-References and Connections:**

*   The core connection is matching `Task_Description`/`Region_Agency` from `Regulatory_Tasks.csv` to the `Primary_Expertise` in `Team_Resources.csv` using the inference guidelines provided in the instructions.
*   `Estimated_Effort_Person_Days` must be compared against `Available_Person_Days` (and the *remaining* available days as allocation progresses).
*   `Due_Date` and `Initial_Notes` in `Regulatory_Tasks.csv` directly drive the prioritization ranking.
*   `Task_ID` and `Resource_ID` are used as identifiers throughout and for tie-breaking.

**3. Tricky Elements and Common Pitfalls:**

*   **Prioritization Order:** Candidates must strictly follow the 4 prioritization rules *in order*. Applying them out of order (e.g., prioritizing ""High business impact"" before deadlines) will lead to an incorrect sequence. Forgetting the Task_ID tie-breaker will leave ambiguity.
*   **Expertise Inference:** Candidates must correctly map tasks to expertise based *only* on the provided guidelines. T07 (""Prepare response to HA query - Prod B"", US FDA) requires US CMC expertise based on the guidelines (US FDA + nature of query likely CMC), not General Ops. T03 (HC Canada Annual Report) requires General Ops.
*   **Dynamic Capacity:** Candidates must track the *remaining* available days for each resource as they allocate tasks sequentially by priority. Failing to do this will lead to incorrect allocation decisions (e.g., assigning a task when the resource no longer has enough days).
*   **Unallocated Reasons:** Choosing the correct reason (`INSUFFICIENT_CAPACITY`, `NO_MATCHING_EXPERTISE`, `RESOURCE_OVERLOADED`) requires careful analysis. `RESOURCE_OVERLOADED` applies when a suitable resource exists but doesn't have *enough remaining capacity* for that specific task due to higher-priority assignments. `INSUFFICIENT_CAPACITY` would apply if, hypothetically, the total effort for *all* EU tasks exceeded 20 days even if R01 had nothing else assigned (not the case here).
*   **Workload Calculation:** The calculation must use the *total* allocated days for each resource and compare it against their *total* available days using the precise thresholds (<80% for Under-allocated, >100% for Over-allocated).
*   **JSON Format:** Strict adherence to the JSON structure, keys, and predefined string values (`Allocated`, `Over-allocated`, `Under-allocated`, `Within Capacity`, `INSUFFICIENT_CAPACITY`, etc.) is required. Typos or incorrect structure will cause issues.
*   **Allocation Tie-breaker:** While the rule (more remaining days -> lower Resource_ID) is included in the instructions, this specific dataset is designed *not* to trigger it, as each task maps to only one resource's expertise. This simplifies the basic exam slightly but tests if the candidate reads and understands all rules, even if not all are applied.

**4. ""Hidden"" Information:**

*   There isn't truly hidden information, but candidates need to synthesize information from multiple places:
    *   Instructions: Prioritization rules, allocation rules, planning period dates, JSON format, expertise inference guidelines.
    *   `Regulatory_Tasks.csv`: Task details, deadlines, effort, notes.
    *   `Team_Resources.csv`: Resource availability and expertise.
*   The need to calculate *remaining* capacity dynamically isn't explicitly stated as a separate step but is inherent in the allocation rules (""do not assign tasks... if the task's effort exceeds the resource's *remaining* Available_Person_Days"").
*   The start date of the planning period (2024-07-01) is crucial for interpreting the deadline prioritization rule.

**5. Expected Outcome Summary (for quick check):**

*   **Priority Order:** T01, T07, T04, T11, T03, T12, T02, T05, T10, T06, T08, T09
*   **Allocated Tasks:** T01(R01), T07(R02), T04(R02), T03(R03), T12(R03), T06(R03), T08(R01), T09(R01)
*   **Unallocated Tasks:** T11(P4, NO_MATCHING_EXPERTISE), T02(P7, RESOURCE_OVERLOADED), T05(P8, RESOURCE_OVERLOADED), T10(P9, RESOURCE_OVERLOADED)
*   **Resource Summary:**
    *   R01: Allocated 15 days (T01=8, T08=4, T09=3). Available 20. Status: **Under-allocated** (75%).
    *   R02: Allocated 16 days (T07=9, T04=7). Available 18. Status: **Within Capacity** (88.9%).
    *   R03: Allocated 15 days (T03=5, T12=4, T06=6). Available 15. Status: **Within Capacity** (100%).

This setup provides a clear, solvable problem with objective criteria for evaluation, suitable for a basic-level practical exam.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
**1. `Regulatory_Tasks.csv`**

```csv
Task_ID,Task_Description,Region_Agency,Due_Date,Estimated_Effort_Person_Days,Initial_Notes
T01,Respond to EMA CHMP LoQ - Product A,EU EMA,2024-07-15,8,Compliance risk
T02,Compile Module 3 Update - Product B,US FDA,2024-08-10,10,High business impact
T03,Submit Annual Report - Product C,HC Canada,2024-07-25,5,Standard procedure
T04,Prepare Briefing Book - Product D,US FDA,2024-07-20,7,High business impact
T05,Initiate new MAA preparation - Prod E,EU EMA,N/A,15,High business impact
T06,Update Core Data Sheet - Product A,Global,N/A,6,Compliance risk
T07,Prepare response to HA query - Prod B,US FDA,2024-07-18,9,Urgent request
T08,Review Promotional Material - Prod C,EU National,N/A,4,Standard procedure
T09,Submit Variation Type IB - Product A,EU EMA,N/A,3,Standard procedure
T10,Prepare Orphan Drug App - Product F,US FDA,N/A,12,High business impact
T11,Address TGA Questions - Product G,AU TGA,2024-07-22,6,Compliance risk
T12,Finalize SOP Update - Reg Processes,Internal,2024-07-26,4,Compliance risk
```

**2. `Team_Resources.csv`**

```csv
Resource_ID,Name,Primary_Expertise,Available_Person_Days
R01,Alex Smith,EU Submissions,20
R02,Brenda Lee,US CMC,18
R03,Chris Wong,General Regulatory Ops,15
```

**3. Expertise Inference Guidelines (Included in Candidate Instructions)**

*   Tasks involving `EU EMA` or `EU National` require **`EU Submissions`** expertise.
*   Tasks involving `US FDA` and descriptions like `Module 3 Update`, `Briefing Book`, or `Orphan Drug App` require **`US CMC`** expertise.
*   Tasks involving `HC Canada` Annual Reports, `Global` Core Data Sheet updates, or `Internal` SOP updates require **`General Regulatory Ops`** expertise.
*   Tasks involving other agencies (e.g., `AU TGA`) require expertise specific to that region (e.g., `APAC Submissions`), which may or may not be available in the team.

","Okay, here is the specification for the candidate's submission format.

## Submission Format Specification

Your submission must be a single JSON file named `test_submission.json`. This file must contain a single JSON object adhering strictly to the structure, keys, data types, and formats described below.

**File Name:** `test_submission.json`

**Content:** A single JSON object.

**JSON Structure and Field Formats:**

```json
{
  ""exam_level"": ""basic"", // Must be the exact string ""basic""
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace ""YOUR_ID_HERE"" with the specific model version you are using (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet"", etc.)
  ""planning_period_weeks"": 4, // Must be the integer 4
  ""prioritized_allocation"": [
    // This is an array of objects. Include one object for EACH task that was successfully allocated.
    // Order these objects by ""assigned_priority"" in ascending order (1, 2, 3...).
    {
      ""task_id"": ""TXX"", // String. Must match a Task_ID from Regulatory_Tasks.csv (e.g., ""T01"")
      ""assigned_priority"": 1, // Integer. The calculated priority rank (e.g., 1, 2, 3...)
      ""assigned_resource_id"": ""RXX"", // String. Must match a Resource_ID from Team_Resources.csv (e.g., ""R02"")
      ""status"": ""Allocated"" // String. Must be the exact string ""Allocated""
    }
    // ... potentially more objects for other allocated tasks
  ],
  ""unallocated_tasks"": [
    // This is an array of objects. Include one object for EACH task that could NOT be allocated.
    // Order these objects by ""assigned_priority"" in ascending order.
    {
      ""task_id"": ""TXX"", // String. Must match a Task_ID from Regulatory_Tasks.csv (e.g., ""T07"")
      ""assigned_priority"": 5, // Integer. The calculated priority rank the task received before allocation failed.
      ""reason"": ""PREDEFINED_REASON"" // String. Must be one of the exact strings: ""INSUFFICIENT_CAPACITY"", ""NO_MATCHING_EXPERTISE"", ""RESOURCE_OVERLOADED""
    }
    // ... potentially more objects for other unallocated tasks
  ],
  ""resource_summary"": [
    // This is an array of objects. Include one object for EACH resource listed in Team_Resources.csv.
    // Order these objects by ""resource_id"" in alphanumeric order (e.g., R01, R02, R03...).
    {
      ""resource_id"": ""RXX"", // String. Must match a Resource_ID from Team_Resources.csv (e.g., ""R01"")
      ""total_available_days"": 20, // Integer. The value from Team_Resources.csv for this resource.
      ""total_allocated_days"": 18, // Integer. The calculated sum of Estimated_Effort_Person_Days for tasks assigned to this resource. Must be a whole number.
      ""workload_status"": ""PREDEFINED_STATUS"" // String. Must be one of the exact strings: ""Over-allocated"", ""Under-allocated"", ""Within Capacity""
    }
    // ... objects for all other resources
  ],
  ""overall_comments"": ""Optional string."" // String. Can be an empty string """" or contain brief text comments.
}
```

**Key Formatting Requirements:**

*   **JSON Validity:** The file must be a valid JSON object. Use a JSON validator if unsure.
*   **Keys:** Use the exact key names as specified (e.g., `""task_id""`, `""assigned_priority""`).
*   **String Values:** Where specific string values are required (e.g., `""Allocated""`, `""basic""`, `""INSUFFICIENT_CAPACITY""`), use them exactly as shown, including capitalization.
*   **Data Types:** Ensure all values match the specified data types (string, integer, array, object).
*   **Numbers:** All numerical values (`planning_period_weeks`, `assigned_priority`, `total_available_days`, `total_allocated_days`) must be integers (whole numbers). Do not use decimals.
*   **Order:** Objects within the `prioritized_allocation` and `unallocated_tasks` arrays must be ordered by `assigned_priority` (ascending). Objects within the `resource_summary` array must be ordered by `resource_id` (alphanumeric).
*   **Candidate ID:** Replace `""YOUR_ID_HERE""` in the `candidate_id` field with the specific model version identifier provided to you or inherent to your execution environment (e.g., `""GPT-4o""`).
*   **Completeness:** Ensure all required sections (`prioritized_allocation`, `unallocated_tasks`, `resource_summary`) are present, even if an array is empty (e.g., `[]` if no tasks were unallocated). Include entries for *all* tasks (either allocated or unallocated) and *all* resources.

**Supplementary Files:**

*   No supplementary files are required. Submit only the `test_submission.json` file.","Okay, here are the evaluator materials for the Basic Level practical exam: the answer key, explanation, and passing criteria.

## 1. Complete Answer Key (`test_submission_ANSWER_KEY.json`)

```json
{
  ""exam_level"": ""basic"",
  ""candidate_id"": ""ANSWER_KEY"",
  ""planning_period_weeks"": 4,
  ""prioritized_allocation"": [
    {
      ""task_id"": ""T01"",
      ""assigned_priority"": 1,
      ""assigned_resource_id"": ""R01"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T07"",
      ""assigned_priority"": 2,
      ""assigned_resource_id"": ""R02"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T04"",
      ""assigned_priority"": 3,
      ""assigned_resource_id"": ""R02"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T03"",
      ""assigned_priority"": 5,
      ""assigned_resource_id"": ""R03"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T12"",
      ""assigned_priority"": 6,
      ""assigned_resource_id"": ""R03"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T06"",
      ""assigned_priority"": 10,
      ""assigned_resource_id"": ""R03"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T08"",
      ""assigned_priority"": 11,
      ""assigned_resource_id"": ""R01"",
      ""status"": ""Allocated""
    },
    {
      ""task_id"": ""T09"",
      ""assigned_priority"": 12,
      ""assigned_resource_id"": ""R01"",
      ""status"": ""Allocated""
    }
  ],
  ""unallocated_tasks"": [
    {
      ""task_id"": ""T11"",
      ""assigned_priority"": 4,
      ""reason"": ""NO_MATCHING_EXPERTISE""
    },
    {
      ""task_id"": ""T02"",
      ""assigned_priority"": 7,
      ""reason"": ""RESOURCE_OVERLOADED""
    },
    {
      ""task_id"": ""T05"",
      ""assigned_priority"": 8,
      ""reason"": ""RESOURCE_OVERLOADED""
    },
    {
      ""task_id"": ""T10"",
      ""assigned_priority"": 9,
      ""reason"": ""RESOURCE_OVERLOADED""
    }
  ],
  ""resource_summary"": [
    {
      ""resource_id"": ""R01"",
      ""total_available_days"": 20,
      ""total_allocated_days"": 15,
      ""workload_status"": ""Under-allocated""
    },
    {
      ""resource_id"": ""R02"",
      ""total_available_days"": 18,
      ""total_allocated_days"": 16,
      ""workload_status"": ""Within Capacity""
    },
    {
      ""resource_id"": ""R03"",
      ""total_available_days"": 15,
      ""total_allocated_days"": 15,
      ""workload_status"": ""Within Capacity""
    }
  ],
  ""overall_comments"": ""This is the expected output based on the provided data and rules.""
}
```

## 2. Explanation of Correct Answers

This section details how the answer key was derived by following the candidate instructions precisely.

**A. Prioritization:**

The tasks are prioritized using the rules in strict order:

1.  **Rule 1 (Deadline):** Tasks due between 2024-07-01 and 2024-07-28, ordered by earliest date:
    *   T01 (2024-07-15) -> Priority 1
    *   T07 (2024-07-18) -> Priority 2
    *   T04 (2024-07-20) -> Priority 3
    *   T11 (2024-07-22) -> Priority 4
    *   T03 (2024-07-25) -> Priority 5
    *   T12 (2024-07-26) -> Priority 6
2.  **Rule 2 (High Business Impact):** Remaining tasks (T02, T05, T06, T08, T09, T10) checked for ""High business impact"":
    *   T02, T05, T10 match.
3.  **Rule 3 (Compliance Risk):** Remaining tasks (T06, T08, T09) checked for ""Compliance risk"":
    *   T06 matches.
4.  **Rule 4 (Tie-breaker - Task ID):**
    *   Apply to Rule 2 group (T02, T05, T10): T02 < T05 < T10. Priorities: T02 -> 7, T05 -> 8, T10 -> 9.
    *   Apply Rule 3: T06 -> Priority 10.
    *   Apply to remaining tasks (T08, T09): T08 < T09. Priorities: T08 -> 11, T09 -> 12.

**Final Priority Order:** T01(1), T07(2), T04(3), T11(4), T03(5), T12(6), T02(7), T05(8), T10(9), T06(10), T08(11), T09(12).

**B. Allocation (Sequential based on Priority):**

*   Initial Capacity: R01 (EU, 20d), R02 (US CMC, 18d), R03 (Gen Ops, 15d)
*   **P1: T01 (8d, EU EMA -> R01):** Allocate to R01. R01 remaining = 20 - 8 = 12.
*   **P2: T07 (9d, US FDA Query -> R02):** Allocate to R02. R02 remaining = 18 - 9 = 9.
*   **P3: T04 (7d, US FDA Briefing Book -> R02):** Allocate to R02. R02 remaining = 9 - 7 = 2.
*   **P4: T11 (6d, AU TGA -> ?):** Requires APAC expertise (inferred). No resource has this. **Unallocated (NO_MATCHING_EXPERTISE)**.
*   **P5: T03 (5d, HC Canada Annual Report -> R03):** Allocate to R03. R03 remaining = 15 - 5 = 10.
*   **P6: T12 (4d, Internal SOP -> R03):** Allocate to R03. R03 remaining = 10 - 4 = 6.
*   **P7: T02 (10d, US FDA Module 3 -> R02):** Requires R02. R02 remaining = 2. Cannot allocate (10d > 2d). **Unallocated (RESOURCE_OVERLOADED)**.
*   **P8: T05 (15d, EU EMA MAA -> R01):** Requires R01. R01 remaining = 12. Cannot allocate (15d > 12d). **Unallocated (RESOURCE_OVERLOADED)**.
*   **P9: T10 (12d, US FDA Orphan Drug -> R02):** Requires R02. R02 remaining = 2. Cannot allocate (12d > 12d). **Unallocated (RESOURCE_OVERLOADED)**.
*   **P10: T06 (6d, Global CDS -> R03):** Allocate to R03. R03 remaining = 6 - 6 = 0.
*   **P11: T08 (4d, EU National Promo -> R01):** Allocate to R01. R01 remaining = 12 - 4 = 8.
*   **P12: T09 (3d, EU EMA Variation -> R01):** Allocate to R01. R01 remaining = 8 - 3 = 5.

**C. Unallocated Tasks Summary:**

*   T11 (Priority 4): Reason = NO_MATCHING_EXPERTISE
*   T02 (Priority 7): Reason = RESOURCE_OVERLOADED (R02 had only 2 days left)
*   T05 (Priority 8): Reason = RESOURCE_OVERLOADED (R01 had only 12 days left)
*   T10 (Priority 9): Reason = RESOURCE_OVERLOADED (R02 had only 2 days left)
*   Ordered by priority: T11, T02, T05, T10.

**D. Resource Summary Calculation:**

*   **R01:** Allocated T01(8) + T08(4) + T09(3) = 15 days. Available = 20. Percentage = 15/20 = 75%. Status = **Under-allocated** (since 75% < 80%).
*   **R02:** Allocated T07(9) + T04(7) = 16 days. Available = 18. Percentage = 16/18 = 88.9%. Status = **Within Capacity** (since 80% <= 88.9% <= 100%).
*   **R03:** Allocated T03(5) + T12(4) + T06(6) = 15 days. Available = 15. Percentage = 15/15 = 100%. Status = **Within Capacity** (since 80% <= 100% <= 100%).
*   Ordered by resource_id: R01, R02, R03.

**E. JSON Formatting:**

*   All derived data is placed into the specified JSON structure.
*   Arrays (`prioritized_allocation`, `unallocated_tasks`, `resource_summary`) are ordered as specified.
*   Keys, data types, and predefined string values match the requirements.

## 3. Passing Criteria

This is a basic-level practical exam. The focus is on demonstrating the ability to follow instructions, apply simple prioritization and allocation rules, use spreadsheet tools for basic analysis, and structure the output correctly.

**Evaluation Approach:**

Points will be awarded based on the accuracy of different sections of the submitted `test_submission.json` file compared to the answer key.

**Scoring Breakdown (Example - Total 100 points):**

1.  **Prioritization (35 points):**
    *   Correct priority rank for *all* deadline-driven tasks (T01, T07, T04, T11, T03, T12) in the correct relative order: 15 points.
    *   Correct priority rank for *all* keyword-driven tasks (T02, T05, T10, T06) in the correct relative order: 10 points.
    *   Correct priority rank for remaining tasks (T08, T09) using tie-breaker: 5 points.
    *   Correct `assigned_priority` value for *every* task listed in `prioritized_allocation` and `unallocated_tasks` sections (must match the derived priority): 5 points.

2.  **Allocation & Unallocated Identification (40 points):**
    *   Correct `assigned_resource_id` for each task in `prioritized_allocation` (based on expertise and *dynamic* capacity): 20 points (pro-rata per correct assignment).
    *   Correct identification of *all* unallocated tasks (correct `task_id` listed in `unallocated_tasks`): 10 points.
    *   Correct `reason` for *each* unallocated task: 10 points (pro-rata per correct reason - distinguishing NO_MATCHING_EXPERTISE from RESOURCE_OVERLOADED is key).

3.  **Resource Summary (15 points):**
    *   Correct `total_allocated_days` for each resource: 9 points (3 points per resource).
    *   Correct `workload_status` for each resource based on their calculated allocation and the defined thresholds: 6 points (2 points per resource).

4.  **JSON Formatting & Compliance (10 points):**
    *   Valid JSON structure: 3 points.
    *   Correct file name (`test_submission.json`): 1 point.
    *   Correct keys, data types, and use of predefined strings (e.g., ""Allocated"", ""basic"", status codes): 3 points.
    *   Correct ordering within arrays (`prioritized_allocation`, `unallocated_tasks` by priority; `resource_summary` by resource ID): 3 points.

**Passing Score:**

*   **Pass:** 70 points or higher.
*   **Fail:** Below 70 points.

**Rationale for Threshold:**

A score of 70/100 indicates the candidate can generally follow the core logic of prioritization and allocation based on rules, track capacity at a basic level, and produce a structured output. Minor errors in calculation, a single mis-prioritization outside the critical deadline tasks, or minor formatting issues might be acceptable at this basic level. However, fundamental errors like ignoring deadlines, failing to match expertise, not tracking capacity leading to widespread allocation errors, or submitting invalid/unstructured JSON would result in a score below 70. Critical errors (e.g., completely wrong prioritization of deadline tasks, major JSON errors) may lead to a fail even if some other sections are partially correct.","```python
import json
import sys
import os
from collections import OrderedDict

# --- Constants for Scoring ---
MAX_POINTS_PRIORITIZATION = 35
MAX_POINTS_ALLOCATION = 40
MAX_POINTS_RESOURCE_SUMMARY = 15
MAX_POINTS_FORMATTING = 10
TOTAL_MAX_POINTS = MAX_POINTS_PRIORITIZATION + MAX_POINTS_ALLOCATION + MAX_POINTS_RESOURCE_SUMMARY + MAX_POINTS_FORMATTING

# --- Helper Functions ---

def load_json(file_path):
    """"""Loads JSON data from a file.""""""
    if not os.path.exists(file_path):
        print(f""Error: File not found at {file_path}"")
        sys.exit(1)
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f, object_pairs_hook=OrderedDict) # Keep order for array checks
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON format in {file_path}: {e}"")
        sys.exit(1)
    except Exception as e:
        print(f""Error reading file {file_path}: {e}"")
        sys.exit(1)

def get_task_details(data, section_key):
    """"""Extracts task details into a dictionary keyed by task_id.""""""
    details = OrderedDict()
    if section_key in data and isinstance(data[section_key], list):
        for item in data[section_key]:
            if isinstance(item, dict) and 'task_id' in item:
                details[item['task_id']] = item
    return details

def get_resource_summary_details(data):
    """"""Extracts resource summary details into a dictionary keyed by resource_id.""""""
    details = OrderedDict()
    if 'resource_summary' in data and isinstance(data['resource_summary'], list):
        for item in data['resource_summary']:
            if isinstance(item, dict) and 'resource_id' in item:
                details[item['resource_id']] = item
    return details

# --- Scoring Functions ---

def score_formatting(candidate_data, key_data):
    """"""Scores the JSON formatting and basic compliance.""""""
    score = 0
    details = {}
    max_points = MAX_POINTS_FORMATTING

    # 1. Valid JSON Structure (3 points) - Implicitly checked by load_json success
    # If the script reaches here, JSON is valid.
    score += 3
    details['valid_json_structure'] = {'score': 3, 'max': 3, 'comment': 'JSON loaded successfully.'}

    # 2. Correct Keys, Data Types, Predefined Strings (basic check) (3 points)
    basic_compliance_score = 0
    required_keys = [""exam_level"", ""candidate_id"", ""planning_period_weeks"",
                     ""prioritized_allocation"", ""unallocated_tasks"", ""resource_summary""]
    keys_present = all(key in candidate_data for key in required_keys)
    types_correct = (
        isinstance(candidate_data.get(""exam_level""), str) and
        isinstance(candidate_data.get(""candidate_id""), str) and
        isinstance(candidate_data.get(""planning_period_weeks""), int) and
        isinstance(candidate_data.get(""prioritized_allocation""), list) and
        isinstance(candidate_data.get(""unallocated_tasks""), list) and
        isinstance(candidate_data.get(""resource_summary""), list)
    )
    predefined_values_correct = (
        candidate_data.get(""exam_level"") == ""basic"" and
        candidate_data.get(""planning_period_weeks"") == 4
    )

    if keys_present and types_correct and predefined_values_correct:
        basic_compliance_score = 3
        comment = ""All required top-level keys present with correct types and basic values.""
    else:
        missing = [key for key in required_keys if key not in candidate_data]
        comment = f""Failed basic structure/value checks. Missing keys: {missing}"" if missing else ""Failed basic structure/value checks (types or predefined values).""

    score += basic_compliance_score
    details['basic_structure_compliance'] = {'score': basic_compliance_score, 'max': 3, 'comment': comment}


    # 3. Correct Ordering within Arrays (3 points)
    ordering_score = 0
    ordering_comments = []

    # Check prioritized_allocation order by assigned_priority
    key_prio_order = [t.get('assigned_priority') for t in key_data.get('prioritized_allocation', []) if t.get('assigned_priority') is not None]
    cand_prio_order = [t.get('assigned_priority') for t in candidate_data.get('prioritized_allocation', []) if t.get('assigned_priority') is not None]
    if key_prio_order == cand_prio_order and key_prio_order == sorted(key_prio_order):
        ordering_score += 1
    else:
         ordering_comments.append(""prioritized_allocation not correctly ordered by assigned_priority."")

    # Check unallocated_tasks order by assigned_priority
    key_unalloc_order = [t.get('assigned_priority') for t in key_data.get('unallocated_tasks', []) if t.get('assigned_priority') is not None]
    cand_unalloc_order = [t.get('assigned_priority') for t in candidate_data.get('unallocated_tasks', []) if t.get('assigned_priority') is not None]
    # Need to handle cases where candidate might miss some unallocated tasks but get order right for the ones they have
    cand_unalloc_priorities_present = [p for p in cand_unalloc_order if p in key_unalloc_order]
    expected_order_subset = sorted([p for p in key_unalloc_order if p in cand_unalloc_priorities_present])

    if cand_unalloc_order == expected_order_subset:
         ordering_score += 1
    else:
         ordering_comments.append(""unallocated_tasks not correctly ordered by assigned_priority."")


    # Check resource_summary order by resource_id
    key_res_order = [r.get('resource_id') for r in key_data.get('resource_summary', []) if r.get('resource_id')]
    cand_res_order = [r.get('resource_id') for r in candidate_data.get('resource_summary', []) if r.get('resource_id')]
    if key_res_order == cand_res_order and key_res_order == sorted(key_res_order):
        ordering_score += 1
    else:
        ordering_comments.append(""resource_summary not correctly ordered by resource_id."")

    score += ordering_score
    details['array_ordering'] = {'score': ordering_score, 'max': 3, 'comment': "" "".join(ordering_comments) if ordering_comments else ""All arrays correctly ordered.""}

    # 4. File Name Check (1 point) - Cannot be reliably checked automatically here. Awarded manually or assumed.
    # We will award this point if the script is run with the expected filename argument.
    # This check is more about process compliance than content.
    # Let's assume the evaluator ran it correctly.
    filename_score = 1
    score += filename_score
    details['file_naming'] = {'score': filename_score, 'max': 1, 'comment': 'Assumed correct filename used based on script execution.'}


    return {'total_score': score, 'max_points': max_points, 'details': details}

def score_prioritization(candidate_data, key_data):
    """"""Scores the task prioritization.""""""
    score = 0
    details = {'correctly_prioritized': [], 'incorrectly_prioritized': []}
    max_points = MAX_POINTS_PRIORITIZATION

    key_alloc_tasks = get_task_details(key_data, 'prioritized_allocation')
    key_unalloc_tasks = get_task_details(key_data, 'unallocated_tasks')
    key_all_tasks_prio = {tid: task.get('assigned_priority') for tid, task in key_alloc_tasks.items()}
    key_all_tasks_prio.update({tid: task.get('assigned_priority') for tid, task in key_unalloc_tasks.items()})

    cand_alloc_tasks = get_task_details(candidate_data, 'prioritized_allocation')
    cand_unalloc_tasks = get_task_details(candidate_data, 'unallocated_tasks')
    cand_all_tasks_prio = {tid: task.get('assigned_priority') for tid, task in cand_alloc_tasks.items()}
    cand_all_tasks_prio.update({tid: task.get('assigned_priority') for tid, task in cand_unalloc_tasks.items()})

    total_tasks_in_key = len(key_all_tasks_prio)
    correct_priorities = 0

    if total_tasks_in_key == 0: # Avoid division by zero if key is empty
        return {'total_score': 0, 'max_points': max_points, 'details': {'comment': 'Answer key has no tasks.'}}

    for task_id, key_priority in key_all_tasks_prio.items():
        cand_priority = cand_all_tasks_prio.get(task_id)
        if cand_priority == key_priority:
            correct_priorities += 1
            details['correctly_prioritized'].append(f""{task_id} (Prio: {key_priority})"")
        else:
            details['incorrectly_prioritized'].append(f""{task_id} (Expected Prio: {key_priority}, Got: {cand_priority})"")

    score = round((correct_priorities / total_tasks_in_key) * max_points)

    details['summary'] = f""Correctly prioritized {correct_priorities} out of {total_tasks_in_key} tasks.""
    return {'total_score': score, 'max_points': max_points, 'details': details}


def score_allocation(candidate_data, key_data):
    """"""Scores the task allocation and unallocated task identification.""""""
    score = 0
    details = {}
    max_points = MAX_POINTS_ALLOCATION

    key_alloc_tasks = get_task_details(key_data, 'prioritized_allocation')
    key_unalloc_tasks = get_task_details(key_data, 'unallocated_tasks')
    cand_alloc_tasks = get_task_details(candidate_data, 'prioritized_allocation')
    cand_unalloc_tasks = get_task_details(candidate_data, 'unallocated_tasks')

    # 1. Correct Resource Assignment (20 points)
    resource_assignment_score = 0
    max_resource_points = 20
    correct_assignments = 0
    assignment_details = {'correct': [], 'incorrect': []}
    total_key_allocations = len(key_alloc_tasks)

    if total_key_allocations > 0:
        for task_id, key_task in key_alloc_tasks.items():
            key_resource = key_task.get('assigned_resource_id')
            cand_task = cand_alloc_tasks.get(task_id)
            if cand_task:
                cand_resource = cand_task.get('assigned_resource_id')
                if cand_resource == key_resource:
                    correct_assignments += 1
                    assignment_details['correct'].append(f""{task_id} -> {key_resource}"")
                else:
                    assignment_details['incorrect'].append(f""{task_id} (Expected: {key_resource}, Got: {cand_resource})"")
            else:
                # Candidate didn't allocate a task that should have been allocated
                 assignment_details['incorrect'].append(f""{task_id} (Expected: {key_resource}, Got: Not Allocated/Missing)"")

        resource_assignment_score = round((correct_assignments / total_key_allocations) * max_resource_points)
    else:
        assignment_details['comment'] = ""No tasks allocated in the answer key.""

    score += resource_assignment_score
    details['resource_assignment'] = {'score': resource_assignment_score, 'max': max_resource_points, 'details': assignment_details}

    # 2. Correct Unallocated Identification (10 points)
    unallocated_id_score = 0
    max_unallocated_id_points = 10
    correct_unallocated_ids = 0
    id_details = {'correct': [], 'incorrect': []}
    key_unalloc_ids = set(key_unalloc_tasks.keys())
    cand_unalloc_ids = set(cand_unalloc_tasks.keys())
    total_key_unallocated = len(key_unalloc_ids)

    if total_key_unallocated > 0:
        correctly_identified = key_unalloc_ids.intersection(cand_unalloc_ids)
        missed_unallocated = key_unalloc_ids.difference(cand_unalloc_ids)
        wrongly_marked_unallocated = cand_unalloc_ids.difference(key_unalloc_ids)

        correct_unallocated_ids = len(correctly_identified)
        unallocated_id_score = round((correct_unallocated_ids / total_key_unallocated) * max_unallocated_id_points)

        id_details['correct'] = list(correctly_identified)
        id_details['missed'] = list(missed_unallocated) # Should have been unallocated, but wasn't
        id_details['wrongly_marked'] = list(wrongly_marked_unallocated) # Marked unallocated, but shouldn't have been
    elif not cand_unalloc_ids: # Key has no unallocated, candidate also has none
         unallocated_id_score = max_unallocated_id_points
         id_details['comment'] = ""Correctly identified that no tasks should be unallocated.""
    else: # Key has no unallocated, but candidate marked some
        id_details['wrongly_marked'] = list(cand_unalloc_ids)
        id_details['comment'] = ""Incorrectly marked tasks as unallocated.""


    score += unallocated_id_score
    details['unallocated_identification'] = {'score': unallocated_id_score, 'max': max_unallocated_id_points, 'details': id_details}

    # 3. Correct Unallocated Reason (10 points)
    unallocated_reason_score = 0
    max_unallocated_reason_points = 10
    correct_reasons = 0
    reason_details = {'correct': [], 'incorrect': []}
    # Only score reasons for tasks correctly identified as unallocated
    tasks_to_check_reason = key_unalloc_ids.intersection(cand_unalloc_ids)
    num_tasks_for_reason_check = len(tasks_to_check_reason)


    if num_tasks_for_reason_check > 0:
        for task_id in tasks_to_check_reason:
            key_reason = key_unalloc_tasks[task_id].get('reason')
            cand_reason = cand_unalloc_tasks[task_id].get('reason')
            if key_reason == cand_reason:
                correct_reasons += 1
                reason_details['correct'].append(f""{task_id} (Reason: {key_reason})"")
            else:
                reason_details['incorrect'].append(f""{task_id} (Expected Reason: {key_reason}, Got: {cand_reason})"")
        unallocated_reason_score = round((correct_reasons / num_tasks_for_reason_check) * max_unallocated_reason_points)
    elif total_key_unallocated == 0 and not cand_unalloc_ids: # No unallocated tasks expected or submitted
        unallocated_reason_score = max_unallocated_reason_points # Perfect score if none were expected and none given
        reason_details['comment'] = ""No unallocated tasks expected or submitted.""
    elif total_key_unallocated > 0 and num_tasks_for_reason_check == 0: # Unallocated tasks expected, but none correctly identified by candidate
        reason_details['comment'] = ""No tasks correctly identified as unallocated to check reasons for.""
        unallocated_reason_score = 0
    elif total_key_unallocated == 0 and cand_unalloc_ids: # No unallocated tasks expected, but candidate submitted some
         reason_details['comment'] = ""Reasons not applicable as tasks were wrongly marked unallocated.""
         unallocated_reason_score = 0


    score += unallocated_reason_score
    details['unallocated_reason'] = {'score': unallocated_reason_score, 'max': max_unallocated_reason_points, 'details': reason_details}


    return {'total_score': score, 'max_points': max_points, 'details': details}


def score_resource_summary(candidate_data, key_data):
    """"""Scores the resource summary section.""""""
    score = 0
    details = {}
    max_points = MAX_POINTS_RESOURCE_SUMMARY

    key_resources = get_resource_summary_details(key_data)
    cand_resources = get_resource_summary_details(candidate_data)

    # 1. Correct Total Allocated Days (9 points)
    allocated_days_score = 0
    max_allocated_days_points = 9
    correct_days_count = 0
    days_details = {'correct': [], 'incorrect': []}
    total_resources_in_key = len(key_resources)

    if total_resources_in_key > 0:
        points_per_resource_days = max_allocated_days_points / total_resources_in_key
        for res_id, key_res in key_resources.items():
            key_days = key_res.get('total_allocated_days')
            cand_res = cand_resources.get(res_id)
            if cand_res:
                cand_days = cand_res.get('total_allocated_days')
                if isinstance(cand_days, int) and cand_days == key_days:
                    correct_days_count += 1
                    days_details['correct'].append(f""{res_id} (Days: {key_days})"")
                else:
                    days_details['incorrect'].append(f""{res_id} (Expected Days: {key_days}, Got: {cand_days})"")
            else:
                days_details['incorrect'].append(f""{res_id} (Resource summary missing)"")
        allocated_days_score = round(correct_days_count * points_per_resource_days)
    else:
        days_details['comment'] = ""No resources found in answer key summary.""


    score += allocated_days_score
    details['total_allocated_days'] = {'score': allocated_days_score, 'max': max_allocated_days_points, 'details': days_details}

    # 2. Correct Workload Status (6 points)
    workload_status_score = 0
    max_workload_status_points = 6
    correct_status_count = 0
    status_details = {'correct': [], 'incorrect': []}

    if total_resources_in_key > 0:
        points_per_resource_status = max_workload_status_points / total_resources_in_key
        for res_id, key_res in key_resources.items():
            key_status = key_res.get('workload_status')
            cand_res = cand_resources.get(res_id)
            if cand_res:
                cand_status = cand_res.get('workload_status')
                # Check if candidate status is one of the allowed strings
                allowed_statuses = [""Over-allocated"", ""Under-allocated"", ""Within Capacity""]
                if cand_status in allowed_statuses and cand_status == key_status:
                    correct_status_count += 1
                    status_details['correct'].append(f""{res_id} (Status: {key_status})"")
                else:
                     status_details['incorrect'].append(f""{res_id} (Expected Status: {key_status}, Got: {cand_status})"")
            else:
                # Resource missing in candidate data, already penalized in days check implicitly
                 status_details['incorrect'].append(f""{res_id} (Resource summary missing)"")

        workload_status_score = round(correct_status_count * points_per_resource_status)
    else:
        status_details['comment'] = ""No resources found in answer key summary.""


    score += workload_status_score
    details['workload_status'] = {'score': workload_status_score, 'max': max_workload_status_points, 'details': status_details}

    return {'total_score': score, 'max_points': max_points, 'details': details}

# --- Main Execution ---

def main():
    if len(sys.argv) != 3:
        print(""Usage: python task_evaluation.py <candidate_submission.json> <answer_key.json>"")
        sys.exit(1)

    candidate_file = sys.argv[1]
    key_file = sys.argv[2]
    results_file = ""test_results.json""

    print(f""Loading candidate submission from: {candidate_file}"")
    candidate_data = load_json(candidate_file)
    print(f""Loading answer key from: {key_file}"")
    key_data = load_json(key_file)

    print(""Evaluating..."")

    # Perform scoring
    formatting_results = score_formatting(candidate_data, key_data)
    prioritization_results = score_prioritization(candidate_data, key_data)
    allocation_results = score_allocation(candidate_data, key_data)
    resource_summary_results = score_resource_summary(candidate_data, key_data)

    # Aggregate results
    total_score = (
        formatting_results['total_score'] +
        prioritization_results['total_score'] +
        allocation_results['total_score'] +
        resource_summary_results['total_score']
    )

    overall_percentage = round((total_score / TOTAL_MAX_POINTS) * 100, 2) if TOTAL_MAX_POINTS > 0 else 0

    final_results = {
        ""candidate_file"": candidate_file,
        ""answer_key_file"": key_file,
        ""overall_score"": overall_percentage,
        ""total_score"": total_score,
        ""max_total_points"": TOTAL_MAX_POINTS,
        ""scoring_details"": {
            ""formatting"": formatting_results,
            ""prioritization"": prioritization_results,
            ""allocation"": allocation_results,
            ""resource_summary"": resource_summary_results
        }
    }

    # Save results
    try:
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=4)
        print(f""Evaluation complete. Results saved to: {results_file}"")
        print(f""Overall Score: {overall_percentage}% ({total_score}/{TOTAL_MAX_POINTS} points)"")
    except Exception as e:
        print(f""Error writing results file {results_file}: {e}"")
        sys.exit(1)

if __name__ == ""__main__"":
    main()
```

**Explanation:**

1.  **Import Libraries:** Imports `json` for JSON handling, `sys` for command-line arguments, `os` for file existence check, and `collections.OrderedDict` to preserve the order of items in the JSON files, which is important for checking array ordering.
2.  **Constants:** Defines constants for the maximum points available in each scoring category and the total maximum points. This makes the script easier to adjust if scoring weights change.
3.  **`load_json(file_path)`:** Safely loads a JSON file. It checks if the file exists and handles potential `FileNotFoundError` and `json.JSONDecodeError`. It uses `object_pairs_hook=OrderedDict` to maintain the order of elements as read from the file.
4.  **`get_task_details(data, section_key)`:** A helper function to convert the lists in `prioritized_allocation` or `unallocated_tasks` into dictionaries keyed by `task_id` for easier lookup and comparison. Uses `OrderedDict` to maintain original order if needed elsewhere.
5.  **`get_resource_summary_details(data)`:** Similar helper for the `resource_summary` section, keyed by `resource_id`.
6.  **Scoring Functions (`score_formatting`, `score_prioritization`, `score_allocation`, `score_resource_summary`):**
    *   Each function takes the candidate's data and the answer key data as input.
    *   They implement the specific logic described in the `<evaluation_information>` section's ""Passing Criteria"" and ""Explanation of Correct Answers"".
    *   They calculate points for sub-criteria (e.g., correct resource assignment, correct unallocated reason).
    *   They use the helper functions (`get_task_details`, `get_resource_summary_details`) to structure the data for comparison.
    *   They handle potential missing data in the candidate's submission gracefully (e.g., if a task is missing, the candidate doesn't get points for it).
    *   They return a dictionary containing the `total_score` for that section, the `max_points`, and a `details` dictionary with comments and scores for sub-criteria.
    *   **Formatting:** Checks JSON validity (implicit), basic structure/keys/types, predefined values (`exam_level`, `planning_period_weeks`), and array ordering. Assumes filename check is manual/process-based.
    *   **Prioritization:** Compares the `assigned_priority` for *every* task defined in the key against the candidate's submission. Awards points proportionally.
    *   **Allocation:** Scores correct resource assignment for allocated tasks, correct identification of *which* tasks are unallocated, and correct `reason` for those correctly identified as unallocated. Points are awarded proportionally based on the key.
    *   **Resource Summary:** Scores correct `total_allocated_days` and correct `workload_status` for each resource defined in the key.
7.  **`main()` Function:**
    *   Parses command-line arguments for the candidate and key file paths.
    *   Loads the JSON data using `load_json`.
    *   Calls each scoring function.
    *   Aggregates the total score.
    *   Calculates the `overall_score` percentage.
    *   Structures the final results into a dictionary.
    *   Saves the `final_results` dictionary to `test_results.json` using `json.dump` with indentation for readability.
    *   Prints completion messages and the overall score to the console.
8.  **`if __name__ == ""__main__"":`:** Ensures the `main()` function runs only when the script is executed directly.

**To Use:**

1.  Save the code as `task_evaluation.py`.
2.  Make sure you have the candidate's submission file (e.g., `test_submission.json`) and the answer key file (e.g., `answer_key.json`, containing the content from `<evaluation_information>`) in the same directory.
3.  Run from your terminal:
    ```bash
    python task_evaluation.py test_submission.json answer_key.json
    ```
4.  The script will print the overall score and create a `test_results.json` file with the detailed breakdown.","{'exam_level': 'basic', 'candidate_id': 'ANSWER_KEY', 'planning_period_weeks': 4, 'prioritized_allocation': [{'task_id': 'T01', 'assigned_priority': 1, 'assigned_resource_id': 'R01', 'status': 'Allocated'}, {'task_id': 'T07', 'assigned_priority': 2, 'assigned_resource_id': 'R02', 'status': 'Allocated'}, {'task_id': 'T04', 'assigned_priority': 3, 'assigned_resource_id': 'R02', 'status': 'Allocated'}, {'task_id': 'T03', 'assigned_priority': 5, 'assigned_resource_id': 'R03', 'status': 'Allocated'}, {'task_id': 'T12', 'assigned_priority': 6, 'assigned_resource_id': 'R03', 'status': 'Allocated'}, {'task_id': 'T06', 'assigned_priority': 10, 'assigned_resource_id': 'R03', 'status': 'Allocated'}, {'task_id': 'T08', 'assigned_priority': 11, 'assigned_resource_id': 'R01', 'status': 'Allocated'}, {'task_id': 'T09', 'assigned_priority': 12, 'assigned_resource_id': 'R01', 'status': 'Allocated'}], 'unallocated_tasks': [{'task_id': 'T11', 'assigned_priority': 4, 'reason': 'NO_MATCHING_EXPERTISE'}, {'task_id': 'T02', 'assigned_priority': 7, 'reason': 'RESOURCE_OVERLOADED'}, {'task_id': 'T05', 'assigned_priority': 8, 'reason': 'RESOURCE_OVERLOADED'}, {'task_id': 'T10', 'assigned_priority': 9, 'reason': 'RESOURCE_OVERLOADED'}], 'resource_summary': [{'resource_id': 'R01', 'total_available_days': 20, 'total_allocated_days': 15, 'workload_status': 'Under-allocated'}, {'resource_id': 'R02', 'total_available_days': 18, 'total_allocated_days': 16, 'workload_status': 'Within Capacity'}, {'resource_id': 'R03', 'total_available_days': 15, 'total_allocated_days': 15, 'workload_status': 'Within Capacity'}], 'overall_comments': 'This is the expected output based on the provided data and rules.'}","[[None, 'loop of ufunc does not support argument 0 of type NoneType which has no callable rint method']]",True,True,0.0,99.0,100.0,2.0,False,"Could not parse JSON. Raw LLM response:
```json
{
  ""makes_sense"": true,
  ""explanation"": ""The exam is practical, testing core RA Manager tasks (prioritization, resource allocation) using realistic data (tasks, resources, deadlines, effort) and common tools (spreadsheets, text editors). The tasks are relevant for 2025. Instructions, materials, and submission format (JSON) are highly specific and unambiguous. The grading script accurately reflects the evaluation criteria and scoring breakdown described. The answer key derivation follows the instructions meticulously, and the key would score 100% on the script. The 70% pass threshold seems appropriate for a basic level, ensuring critical errors lead to failure while allowing for minor mistakes. \nPotential Weaknesses: \n1. Expertise Inference: Task T07 requires slight inference beyond explicit guidelines, which could trip up some candidates, though it's arguably reasonable for a basic test. \n2. Tool vs. Skill: Heavy reliance on JSON formatting means tool errors could impact scores, although the spec is very clear. \n3. Simplification: As a 'basic' exam, it simplifies real-world complexities (e.g., task dependencies, budget, nuanced expertise), which is expected but worth noting.""
}
```","{'overview': prompt_token_count: 238
candidates_token_count: 1735
total_token_count: 3687
, 'instructions': prompt_token_count: 2179
candidates_token_count: 2064
total_token_count: 6011
, 'materials': prompt_token_count: 4448
candidates_token_count: 2409
total_token_count: 10845
, 'check_materials': prompt_token_count: 4577
candidates_token_count: 1
total_token_count: 5098
, 'check_website': prompt_token_count: 4610
candidates_token_count: 1
total_token_count: 4974
, 'submission': prompt_token_count: 6641
candidates_token_count: 1203
total_token_count: 8106
, 'evaluation': prompt_token_count: 7765
candidates_token_count: 3309
total_token_count: 13898
, 'grading': prompt_token_count: 11263
candidates_token_count: 6569
total_token_count: 20596
, 'check_sense': prompt_token_count: 18349
candidates_token_count: 261
total_token_count: 22178
}",management_occupations
Administrative Services Managers,21.0,Prepare and review operational reports and schedules to ensure accuracy and efficiency.,gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor', 'PDF viewer']","['Text', 'Data']","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

## Administrative Services Manager Practical Exam - Basic Level

**Welcome!**

**Exam Objective:** This practical exam assesses your ability to accurately review a basic operational report and prepare a simple weekly schedule using standard office software.

**Duration:** 90 minutes

**Tools You Will Need:**
*   A spreadsheet application (like Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A text editor (like Notepad, TextEdit, VS Code)
*   A PDF viewer

**Provided Materials:**

You will be provided with the following digital files:

1.  `instructions.pdf`: This document containing the exam instructions.
2.  `flawed_report.xlsx`: A spreadsheet file containing a simple departmental expense report with errors.
3.  `schedule_requests.txt`: A text file listing meeting requests for one week.
4.  `json_template.txt`: A text file showing the required structure for your answer submission file.

**What You Need To Accomplish:**

You will perform two tasks: reviewing the provided report for errors and creating a schedule based on the provided requests. You will then record your findings and references to your work in a structured JSON file.

**Submission Format:**

*   You must submit your answers compiled into a single JSON file named `test_submission.json`.
*   Use the `json_template.txt` file as a guide for the structure. Ensure your final JSON file is valid.
*   You will also need to submit the schedule spreadsheet you create in Task 2.
*   In total, you will submit **two files**:
    1.  `test_submission.json`
    2.  `weekly_schedule.xlsx`

---

**Exam Tasks:**

**Task 1: Review Operational Report (Estimated time: 35 minutes)**

1.  Open the provided spreadsheet file: `flawed_report.xlsx`. This file contains a summary of departmental expenses for a specific period.
2.  Carefully review the data in the spreadsheet. Your goal is to identify exactly **five (5)** distinct errors within the report. Focus on identifying errors related to:
    *   **Calculation Errors:** Incorrect sums, totals, or other calculated values.
    *   **Typographical Errors:** Misspelled words or incorrect labels.
    *   **Inconsistent Data/Format:** Variations in how similar data is presented (e.g., date formats).
    *   **Missing Data:** Required information that is absent from a cell.
    *   **Logical Errors:** Information that is impossible or doesn't make sense in the context (e.g., an expense date far in the future).
3.  **Do not modify or correct the `flawed_report.xlsx` file itself.**
4.  Record the five errors you identified in your `test_submission.json` file under the `report_review_findings` section. For each error, provide:
    *   `location`: The specific cell reference where the error is found (e.g., ""E10"", ""C5:C10"", ""G8"").
    *   `error_type`: Classify the error using one of these exact terms: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error"".
    *   `description`: Briefly and clearly describe the error. (e.g., ""Sum calculation is incorrect."", ""Department name 'Financee' is misspelled."", ""Date format differs from other entries."", ""Quantity value is missing."", ""Expense date is in the future."").

**Task 2: Prepare Basic Schedule (Estimated time: 45 minutes)**

1.  Open the provided text file: `schedule_requests.txt`. This file contains a list of meeting requests for a single week (Monday to Friday, operating hours 9:00 AM to 5:00 PM). Each request includes details like purpose, day/time, duration, and required resources (e.g., a specific meeting room or person).
2.  Create a **new spreadsheet file**.
3.  Using the information from `schedule_requests.txt`, build a weekly schedule in your new spreadsheet.
    *   Organize the schedule clearly, for example, with columns for Day, Start Time, End Time, Duration (mins), Meeting Purpose, and Room/Resource.
    *   Include all necessary details for each scheduled meeting.
    *   **Conflict Handling Rule:** Review the requests carefully for conflicts. If two requests require the *same resource* (e.g., the same meeting room or the same specific person) at the *same or overlapping time*, you must schedule the request that appears **first** in the `schedule_requests.txt` file. The request listed later that causes the conflict should **not** be scheduled.
    *   Format the schedule simply for readability (e.g., use clear headings and grid lines).
4.  Save your completed schedule spreadsheet with the exact filename: `weekly_schedule.xlsx`.
5.  Record the results of your scheduling work in your `test_submission.json` file under the `schedule_preparation` section:
    *   Set the value for `created_schedule_file` to `""weekly_schedule.xlsx""`.
    *   For `total_meetings_scheduled`, enter the exact number of meeting requests you successfully placed onto the schedule.
    *   For `unscheduled_requests`, list any requests you could not schedule due to the conflict handling rule. For each unscheduled request:
        *   `request`: Briefly identify the request (e.g., ""Budget Review Mon 10:30 AM"").
        *   `reason`: State the reason using this format: ""Conflict with [Meeting Purpose of conflicting item] for [Resource Name]"". (e.g., ""Conflict with Project Alpha Kick-off for Conference Room 1""). If no requests were left unscheduled, leave the list empty (`[]`).

---

**Final Steps:**

1.  Ensure your `test_submission.json` file is complete, correctly formatted according to `json_template.txt`, and contains all the required information from Task 1 and Task 2.
2.  Prepare your two submission files: `test_submission.json` and `weekly_schedule.xlsx`.

**Good luck!**
**1. Content for `flawed_report.xlsx` (Provide as an actual .xlsx file, represented here as CSV for clarity):**

```csv
Department,Item,Unit Cost,Quantity,Total Cost,Expense Date
IT,""Laptop Power Adapter"",45.50,3,136.50,10/15/2023
Marketing,""Social Media Ad Spend"",1500.00,1,1500.00,10/20/2023
Sales,""Client Lunch"",85.75,,"""",10/18/2023
Operations,""Office Chair"",175.00,5,875.00,15-Oct-2023
Financee,""Accounting Software Subscription"",250.00,1,250.00,10/22/2023
IT,""External Hard Drive"",120.00,2,240.00,10/25/2023
Marketing,""Brochure Printing"",650.00,1,650.00,10/28/2023
Sales,""Travel - Airfare"",455.20,1,455.20,11/05/2024
Operations,""Printer Ink"",35.00,10,350.00,29/10/2023
,""Grand Total"",,,,""2851.70""
```

**2. Content for `schedule_requests.txt` (Provide as a plain .txt file):**

```text
Meeting Request List for Week of Nov 6th (Mon-Fri, 9am-5pm)

Request 1:
Purpose: Project Phoenix - Kick-off
Day: Monday
Time: 9:30 AM
Duration: 90 mins
Resource: Conference Room A

Request 2:
Purpose: Weekly Sales Sync
Day: Monday
Time: 11:00 AM
Duration: 60 mins
Resource: Conference Room A

Request 3:
Purpose: HR Policy Review
Day: Tuesday
Time: 2:00 PM
Duration: 45 mins
Resource: Meeting Room B

Request 4:
Purpose: IT Infrastructure Update
Day: Wednesday
Time: 10:00 AM
Duration: 120 mins
Resource: Conference Room A

Request 5:
Purpose: Marketing Campaign Brainstorm
Day: Thursday
Time: 1:00 PM
Duration: 75 mins
Resource: Conference Room A

Request 6:
Purpose: Finance Budget Meeting
Day: Friday
Time: 9:00 AM
Duration: 180 mins
Resource: Conference Room A

Request 7:
Purpose: Client Pre-Demo Check
Day: Friday
Time: 10:00 AM
Duration: 30 mins
Resource: Meeting Room B
```

**3. Content for `json_template.txt` (Provide as a plain .txt file):**

```json
{
  ""exam_part"": ""Basic"",
  ""candidate_id"": ""CANDIDATE_EMAIL_OR_ID"", // To be filled by candidate
  ""report_review_findings"": [
    {
      ""location"": """", // e.g., ""E10"", ""C5"", ""F5:F9""
      ""error_type"": """", // Choose from: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error""
      ""description"": """" // e.g., ""Sum calculation is incorrect.""
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    }
    // Exactly five errors must be listed
  ],
  ""schedule_preparation"": {
    ""created_schedule_file"": ""weekly_schedule.xlsx"",
    ""total_meetings_scheduled"": null, // Fill in the number, e.g., 6
    ""unscheduled_requests"": [
      // Add objects for any unscheduled requests, or leave as [] if none
      // Example: { ""request"": ""Weekly Sales Sync Mon 11:00 AM"", ""reason"": ""Conflict with Project Phoenix - Kick-off for Conference Room A"" }
    ]
  }
}
```
Okay, here is the specification for the candidate's submission format for the Basic Level Practical Exam.

**Submission Files:**

You must submit exactly two files:

1.  `test_submission.json`: A JSON file containing your answers and analysis based on the tasks.
2.  `weekly_schedule.xlsx`: The spreadsheet file containing the schedule you created in Task 2.

**JSON Answer Format (`test_submission.json`):**

Your `test_submission.json` file **must** adhere strictly to the following structure and data formats. Use a text editor to create this file, ensuring it is valid JSON.

```json
{
  ""exam_part"": ""Basic"", // Do not change this value. Type: String.
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace ""YOUR_ID_HERE"" with the specific model version provided to you (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet""). Type: String.
  ""report_review_findings"": [ // This MUST be an array containing exactly 5 objects.
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Specify the location of the error. Use standard spreadsheet notation (e.g., ""A6"", ""F9"", ""E2:E10"", ""Column F""). Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Use ONE of the following exact strings: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error"". Type: String.
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Provide a concise explanation of the identified error. Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    }
    // Ensure exactly five error objects are present in this array.
  ],
  ""schedule_preparation"": { // This MUST be an object.
    ""created_schedule_file"": ""weekly_schedule.xlsx"", // Do not change this value. Type: String.
    ""total_meetings_scheduled"": 0, // Replace 0 with the total count of meetings you successfully added to the schedule. Type: Integer Number.
    ""unscheduled_requests"": [ // This MUST be an array. It can be empty [] if all requests were scheduled.
      // If requests were unscheduled, add one object per unscheduled request:
      {
        ""request"": ""IDENTIFYING_TEXT_FOR_UNSCHEDULED_REQUEST"", // Briefly identify the request you could not schedule (e.g., ""Weekly Sales Sync Mon 11:00 AM""). Type: String.
        ""reason"": ""REASON_FOR_NOT_SCHEDULING"" // Explain the conflict precisely as specified in the instructions (e.g., ""Conflict with Project Phoenix - Kick-off for Conference Room A""). Type: String.
      }
      // Add more objects here if multiple requests were unscheduled according to the rules.
    ]
  }
}
```

**Supplementary File Format (`weekly_schedule.xlsx`):**

*   This file must be a standard spreadsheet file (e.g., `.xlsx`, `.ods`).
*   The content should be the weekly schedule created in Task 2, clearly organized and formatted for readability as per the instructions.
*   Ensure the file is saved with the exact name `weekly_schedule.xlsx`.

**Important Notes:**

*   Ensure your `test_submission.json` file is syntactically correct JSON. You can use online JSON validators to check before submitting.
*   Adhere strictly to the specified keys, data types (String, Integer Number, Array, Object), and required values (e.g., the specific `error_type` strings).
*   Replace placeholder text (like `YOUR_ID_HERE`, `CELL_REFERENCE_OR_RANGE`, `0`, etc.) with your actual answers.
*   Do not include any comments (`//`) in the final JSON file.","You are an excellent examiner of Administrative Services Managers capabilities. Design a remote, **practical** exam to verify whether a Administrative Services Managers can Prepare and review operational reports and schedules to ensure accuracy and efficiency..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets, Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the design for the **Basic Level Practical Exam** for Administrative Services Managers, focusing on preparing and reviewing operational reports and schedules for accuracy and efficiency.

---

## Administrative Services Manager Exam: Basic Level

**Exam Purpose (For Evaluator):**

This basic-level practical exam assesses a candidate's fundamental ability to work with operational data and schedules using standard office tools. It focuses on core competencies:
1.  **Accuracy Check:** Identifying errors (calculation, typos, inconsistencies) in pre-existing reports and schedules.
2.  **Data Compilation & Formatting:** Gathering raw data and organizing it into a clear, simple report format using a spreadsheet.
3.  **Basic Scheduling:** Creating a functional schedule from a list of requirements, avoiding obvious conflicts.
4.  **Tool Proficiency:** Demonstrating basic competence with spreadsheets (data entry, simple formulas like SUM/AVERAGE, basic formatting) and understanding information presented in text/PDF formats.
5.  **Structured Output:** Following instructions to provide answers in a specific digital format (JSON).

The exam simulates common tasks an Administrative Services Manager would perform regularly, focusing on foundational accuracy and clarity rather than complex analysis or strategic efficiency improvements (which would be reserved for an advanced exam).

**Exam Structure (For Evaluator):**

The exam consists of two main tasks delivered through digital files. The candidate will use a spreadsheet application, a text editor, and potentially a PDF viewer to complete the tasks. They will then compile their answers and references to their work into a single JSON file (`test_submission.json`) for submission.

*   **Task 1: Report Review and Correction:** The candidate receives a flawed operational report (e.g., monthly office supply usage or simple expense summary) as a spreadsheet file. They must identify specific errors and list them in the JSON output.
*   **Task 2: Schedule Preparation:** The candidate receives raw data (e.g., a list of meeting requests or simple maintenance tasks in a text file). They must create a basic weekly schedule in a spreadsheet, ensuring clarity and avoiding obvious conflicts, and reference the created file and key details in the JSON output.

**Estimated Completion Time:** 90 minutes.

---

**Candidate Instructions & Materials:**

**(To be provided to the candidate at the start of the exam)**

**Welcome to the Administrative Services Manager Practical Exam - Basic Level**

**Objective:** This exam assesses your ability to prepare and review basic operational reports and schedules for accuracy and efficiency using standard office tools.

**Duration:** 90 minutes

**Tools Allowed:**
*   Spreadsheet Application (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   Text Editor (e.g., Notepad, TextEdit)
*   PDF Viewer (e.g., Adobe Reader, browser built-in viewer)

**Provided Materials:**
1.  `flawed_report.xlsx`: An Excel spreadsheet containing a simple operational report with errors.
2.  `schedule_requests.txt`: A text file containing information needed to create a schedule.
3.  `instructions.pdf` (This document)
4.  `json_template.txt`: A template file showing the required structure for your submission.

**Your Tasks:**

**Task 1: Review Operational Report (Estimated time: 35 minutes)**

1.  Open the `flawed_report.xlsx` file. This file represents a monthly summary of departmental expenses.
2.  Carefully review the report for accuracy. Look for errors such as:
    *   Calculation mistakes (e.g., incorrect totals, averages).
    *   Typographical errors (e.g., misspelled names, incorrect dates).
    *   Inconsistent data or formatting.
    *   Missing information.
    *   Logically impossible entries.
3.  You do **not** need to correct the file itself.
4.  In the `test_submission.json` file, under the `report_review_findings` section, list **at least five distinct errors** you identified. For each error, specify:
    *   `location`: The cell reference(s) where the error occurs (e.g., ""Cell E10"", ""Column C"").
    *   `error_type`: A brief classification (e.g., ""Calculation Error"", ""Typo"", ""Inconsistency"", ""Missing Data"").
    *   `description`: A clear explanation of the error.

**Task 2: Prepare Basic Schedule (Estimated time: 45 minutes)**

1.  Open the `schedule_requests.txt` file. This file contains a list of meeting requests for the upcoming week (Monday-Friday, 9:00 AM - 5:00 PM). Each request includes the meeting purpose, requested day/time, duration, and required attendees/resources (e.g., specific meeting room).
2.  Using your spreadsheet application, create a new spreadsheet file named `weekly_schedule.xlsx`.
3.  Design a clear weekly schedule based on the requests in `schedule_requests.txt`.
    *   Organize the schedule logically (e.g., by day and time).
    *   Ensure all necessary information from the requests is included for each scheduled item.
    *   Avoid any direct time conflicts (e.g., booking the same room or person for two meetings simultaneously). If conflicts arise from the requests, prioritize based on your best judgment (or note the conflict if unresolvable with the given info - though the basic test should ideally have resolvable requests).
    *   Format the schedule for clarity (e.g., use borders, clear headings).
4.  Save the completed schedule as `weekly_schedule.xlsx`.
5.  In the `test_submission.json` file, under the `schedule_preparation` section:
    *   Confirm the name of the schedule file you created (`""created_schedule_file"": ""weekly_schedule.xlsx""`).
    *   State the total number of meetings successfully scheduled (`""total_meetings_scheduled"": <number>`).
    *   Note any requests you could *not* schedule and provide a brief reason (`""unscheduled_requests"": [ { ""request"": ""<brief description>"", ""reason"": ""<reason>"" } ]` - leave empty `[]` if all were scheduled).

**Submission:**

1.  Use the `json_template.txt` as a guide to create your `test_submission.json` file using your text editor.
2.  Fill in the required details based on your work in Task 1 and Task 2. Ensure the JSON format is valid.
3.  You will need to submit two files:
    *   `test_submission.json` (containing your findings and references)
    *   `weekly_schedule.xlsx` (the schedule you created in Task 2)

**End of Instructions.**

---

**Supporting Files Content (Examples):**

*   **`flawed_report.xlsx` (Example Snippet & Errors):**
    *   A simple table: Dept | Item | Cost | Quantity | Total | Date
    *   *Error 1 (Calculation):* A 'Total' cell (Cost * Quantity) is wrong.
    *   *Error 2 (Typo):* A department name ""Markting"" instead of ""Marketing"".
    *   *Error 3 (Inconsistency):* Date format varies wildly (MM/DD/YY, DD-Mon-YYYY).
    *   *Error 4 (Missing Data):* A 'Quantity' cell is blank for an item with a cost.
    *   *Error 5 (Calculation):* Grand Total row sum is incorrect.
    *   *Error 6 (Logical):* Date is in the future for a past expense report.

*   **`schedule_requests.txt` (Example Snippet):**
    *   `Meeting: Project Alpha Kick-off; Requestor: J. Doe; Day: Monday; Time: 10:00 AM; Duration: 60 mins; Attendees: Team A, R. Smith; Room: Conference Room 1`
    *   `Meeting: Budget Review; Requestor: Finance Dept; Day: Monday; Time: 10:30 AM; Duration: 90 mins; Attendees: C. Jones, P. Lee; Room: Conference Room 1` (Creates a conflict with the first request)
    *   `Meeting: Client Demo Prep; Requestor: Sales Team; Day: Tuesday; Time: 2:00 PM; Duration: 45 mins; Attendees: Sales Reps; Room: Meeting Room B`
    *   ... (enough requests to fill a basic schedule, potentially with one or two simple conflicts to resolve/flag)

*   **`json_template.txt` (Structure):**
    ```json
    {
      ""exam_part"": ""Basic"",
      ""candidate_id"": ""CANDIDATE_EMAIL_OR_ID"", // To be filled by candidate or proctor
      ""report_review_findings"": [
        {
          ""location"": ""Example: Cell E10"",
          ""error_type"": ""Example: Calculation Error"",
          ""description"": ""Example: Sum of column D is incorrect.""
        },
        {
          ""location"": """",
          ""error_type"": """",
          ""description"": """"
        }
        // Add more error objects as needed (at least 5 required)
      ],
      ""schedule_preparation"": {
        ""created_schedule_file"": ""weekly_schedule.xlsx"",
        ""total_meetings_scheduled"": null, // Fill in the number
        ""unscheduled_requests"": [
          // Add objects for any unscheduled requests, or leave as [] if none
          // { ""request"": ""Example: Budget Review Mon 10:30 AM"", ""reason"": ""Example: Conflict with Project Alpha Kick-off in Conf Room 1"" }
        ]
      }
    }
    ```

---","Okay, here are the candidate instructions for the Basic Level Practical Exam.

---

## Administrative Services Manager Practical Exam - Basic Level

**Welcome!**

**Exam Objective:** This practical exam assesses your ability to accurately review a basic operational report and prepare a simple weekly schedule using standard office software.

**Duration:** 90 minutes

**Tools You Will Need:**
*   A spreadsheet application (like Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A text editor (like Notepad, TextEdit, VS Code)
*   A PDF viewer

**Provided Materials:**

You will be provided with the following digital files:

1.  `instructions.pdf`: This document containing the exam instructions.
2.  `flawed_report.xlsx`: A spreadsheet file containing a simple departmental expense report with errors.
3.  `schedule_requests.txt`: A text file listing meeting requests for one week.
4.  `json_template.txt`: A text file showing the required structure for your answer submission file.

**What You Need To Accomplish:**

You will perform two tasks: reviewing the provided report for errors and creating a schedule based on the provided requests. You will then record your findings and references to your work in a structured JSON file.

**Submission Format:**

*   You must submit your answers compiled into a single JSON file named `test_submission.json`.
*   Use the `json_template.txt` file as a guide for the structure. Ensure your final JSON file is valid.
*   You will also need to submit the schedule spreadsheet you create in Task 2.
*   In total, you will submit **two files**:
    1.  `test_submission.json`
    2.  `weekly_schedule.xlsx`

---

**Exam Tasks:**

**Task 1: Review Operational Report (Estimated time: 35 minutes)**

1.  Open the provided spreadsheet file: `flawed_report.xlsx`. This file contains a summary of departmental expenses for a specific period.
2.  Carefully review the data in the spreadsheet. Your goal is to identify exactly **five (5)** distinct errors within the report. Focus on identifying errors related to:
    *   **Calculation Errors:** Incorrect sums, totals, or other calculated values.
    *   **Typographical Errors:** Misspelled words or incorrect labels.
    *   **Inconsistent Data/Format:** Variations in how similar data is presented (e.g., date formats).
    *   **Missing Data:** Required information that is absent from a cell.
    *   **Logical Errors:** Information that is impossible or doesn't make sense in the context (e.g., an expense date far in the future).
3.  **Do not modify or correct the `flawed_report.xlsx` file itself.**
4.  Record the five errors you identified in your `test_submission.json` file under the `report_review_findings` section. For each error, provide:
    *   `location`: The specific cell reference where the error is found (e.g., ""E10"", ""C5:C10"", ""G8"").
    *   `error_type`: Classify the error using one of these exact terms: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error"".
    *   `description`: Briefly and clearly describe the error. (e.g., ""Sum calculation is incorrect."", ""Department name 'Financee' is misspelled."", ""Date format differs from other entries."", ""Quantity value is missing."", ""Expense date is in the future."").

**Task 2: Prepare Basic Schedule (Estimated time: 45 minutes)**

1.  Open the provided text file: `schedule_requests.txt`. This file contains a list of meeting requests for a single week (Monday to Friday, operating hours 9:00 AM to 5:00 PM). Each request includes details like purpose, day/time, duration, and required resources (e.g., a specific meeting room or person).
2.  Create a **new spreadsheet file**.
3.  Using the information from `schedule_requests.txt`, build a weekly schedule in your new spreadsheet.
    *   Organize the schedule clearly, for example, with columns for Day, Start Time, End Time, Duration (mins), Meeting Purpose, and Room/Resource.
    *   Include all necessary details for each scheduled meeting.
    *   **Conflict Handling Rule:** Review the requests carefully for conflicts. If two requests require the *same resource* (e.g., the same meeting room or the same specific person) at the *same or overlapping time*, you must schedule the request that appears **first** in the `schedule_requests.txt` file. The request listed later that causes the conflict should **not** be scheduled.
    *   Format the schedule simply for readability (e.g., use clear headings and grid lines).
4.  Save your completed schedule spreadsheet with the exact filename: `weekly_schedule.xlsx`.
5.  Record the results of your scheduling work in your `test_submission.json` file under the `schedule_preparation` section:
    *   Set the value for `created_schedule_file` to `""weekly_schedule.xlsx""`.
    *   For `total_meetings_scheduled`, enter the exact number of meeting requests you successfully placed onto the schedule.
    *   For `unscheduled_requests`, list any requests you could not schedule due to the conflict handling rule. For each unscheduled request:
        *   `request`: Briefly identify the request (e.g., ""Budget Review Mon 10:30 AM"").
        *   `reason`: State the reason using this format: ""Conflict with [Meeting Purpose of conflicting item] for [Resource Name]"". (e.g., ""Conflict with Project Alpha Kick-off for Conference Room 1""). If no requests were left unscheduled, leave the list empty (`[]`).

---

**Final Steps:**

1.  Ensure your `test_submission.json` file is complete, correctly formatted according to `json_template.txt`, and contains all the required information from Task 1 and Task 2.
2.  Prepare your two submission files: `test_submission.json` and `weekly_schedule.xlsx`.

**Good luck!**","Okay, here are the materials for the candidate and the explanation for the evaluator, formatted as requested.

<MATERIALS_FOR_CANDIDATE>
**1. Content for `flawed_report.xlsx` (Provide as an actual .xlsx file, represented here as CSV for clarity):**

```csv
Department,Item,Unit Cost,Quantity,Total Cost,Expense Date
IT,""Laptop Power Adapter"",45.50,3,136.50,10/15/2023
Marketing,""Social Media Ad Spend"",1500.00,1,1500.00,10/20/2023
Sales,""Client Lunch"",85.75,,"""",10/18/2023
Operations,""Office Chair"",175.00,5,875.00,15-Oct-2023
Financee,""Accounting Software Subscription"",250.00,1,250.00,10/22/2023
IT,""External Hard Drive"",120.00,2,240.00,10/25/2023
Marketing,""Brochure Printing"",650.00,1,650.00,10/28/2023
Sales,""Travel - Airfare"",455.20,1,455.20,11/05/2024
Operations,""Printer Ink"",35.00,10,350.00,29/10/2023
,""Grand Total"",,,,""2851.70""
```

**2. Content for `schedule_requests.txt` (Provide as a plain .txt file):**

```text
Meeting Request List for Week of Nov 6th (Mon-Fri, 9am-5pm)

Request 1:
Purpose: Project Phoenix - Kick-off
Day: Monday
Time: 9:30 AM
Duration: 90 mins
Resource: Conference Room A

Request 2:
Purpose: Weekly Sales Sync
Day: Monday
Time: 11:00 AM
Duration: 60 mins
Resource: Conference Room A

Request 3:
Purpose: HR Policy Review
Day: Tuesday
Time: 2:00 PM
Duration: 45 mins
Resource: Meeting Room B

Request 4:
Purpose: IT Infrastructure Update
Day: Wednesday
Time: 10:00 AM
Duration: 120 mins
Resource: Conference Room A

Request 5:
Purpose: Marketing Campaign Brainstorm
Day: Thursday
Time: 1:00 PM
Duration: 75 mins
Resource: Conference Room A

Request 6:
Purpose: Finance Budget Meeting
Day: Friday
Time: 9:00 AM
Duration: 180 mins
Resource: Conference Room A

Request 7:
Purpose: Client Pre-Demo Check
Day: Friday
Time: 10:00 AM
Duration: 30 mins
Resource: Meeting Room B
```

**3. Content for `json_template.txt` (Provide as a plain .txt file):**

```json
{
  ""exam_part"": ""Basic"",
  ""candidate_id"": ""CANDIDATE_EMAIL_OR_ID"", // To be filled by candidate
  ""report_review_findings"": [
    {
      ""location"": """", // e.g., ""E10"", ""C5"", ""F5:F9""
      ""error_type"": """", // Choose from: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error""
      ""description"": """" // e.g., ""Sum calculation is incorrect.""
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    }
    // Exactly five errors must be listed
  ],
  ""schedule_preparation"": {
    ""created_schedule_file"": ""weekly_schedule.xlsx"",
    ""total_meetings_scheduled"": null, // Fill in the number, e.g., 6
    ""unscheduled_requests"": [
      // Add objects for any unscheduled requests, or leave as [] if none
      // Example: { ""request"": ""Weekly Sales Sync Mon 11:00 AM"", ""reason"": ""Conflict with Project Phoenix - Kick-off for Conference Room A"" }
    ]
  }
}
```
</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. Explanation for `flawed_report.xlsx` Content:**

*   **Creation:** The data simulates a simple departmental expense report. It includes various expense types and standard columns (Department, Item, Unit Cost, Quantity, Total Cost, Expense Date). Errors were deliberately introduced to cover the five specific types mentioned in the instructions.
*   **Intended Errors (Exactly 5):**
    1.  **Missing Data:** Cell `D4` (Quantity for ""Client Lunch"") is blank. The corresponding `E4` (Total Cost) is also blank, which logically follows but the root cause is the missing quantity. The candidate should identify the missing quantity in D4.
        *   *Location:* `D4`
        *   *Error Type:* `Missing Data`
        *   *Description:* Quantity value is missing for Client Lunch.
    2.  **Typographical Error:** Cell `A6` contains ""Financee"" instead of ""Finance"".
        *   *Location:* `A6`
        *   *Error Type:* `Typographical Error`
        *   *Description:* Department name 'Financee' is misspelled.
    3.  **Inconsistent Format:** Column `F` (Expense Date) uses multiple formats (`MM/DD/YYYY`, `DD-Mon-YYYY`, `DD/MM/YYYY`). The candidate should identify this inconsistency. Pointing to specific cells like F5 vs F8 vs F10 or the whole column F is acceptable.
        *   *Location:* `Column F` (or specific examples like `F5` vs `F8`)
        *   *Error Type:* `Inconsistent Format`
        *   *Description:* Date format differs between entries.
    4.  **Logical Error:** Cell `F9` contains the date `11/05/2024`. Given the other dates are in October 2023, this future date for a past expense report is logically impossible.
        *   *Location:* `F9`
        *   *Error Type:* `Logical Error`
        *   *Description:* Expense date is in the future.
    5.  **Calculation Error:** Cell `F11` (Grand Total) shows `2851.70`. The actual sum of the `Total Cost` column (E2:E10, ignoring the blank E4) is `136.50 + 1500.00 + 875.00 + 250.00 + 240.00 + 650.00 + 455.20 + 350.00 = 4456.70`. The value in F11 is incorrect. Note: Some candidates might try to calculate E4 based on D4 being 1, which is an assumption, but the primary error is the incorrect Grand Total sum.
        *   *Location:* `F11` (or `E11` if they interpret the label placement differently, but F11 contains the value)
        *   *Error Type:* `Calculation Error`
        *   *Description:* Grand Total sum is incorrect.
*   **Potential Pitfalls:**
    *   Candidates might list more than 5 errors (e.g., also flagging the blank E4, or the inconsistent capitalization in Item names). The instructions explicitly ask for exactly 5.
    *   Candidates might try to *correct* the file instead of just listing errors.
    *   Candidates might misclassify error types (e.g., calling the date format issue a ""Typo"").
    *   Candidates might struggle with the exact cell reference format.

**2. Explanation for `schedule_requests.txt` Content:**

*   **Creation:** The requests are designed to be straightforward, fitting within a standard Mon-Fri, 9am-5pm schedule. Durations are given in minutes. Each request specifies a resource (Conference Room A or Meeting Room B).
*   **Scheduling Logic & Conflict:**
    *   Requests 1, 3, 4, 5, 6, 7 can be scheduled without direct time conflicts *with each other* based on their requested times and resources.
    *   **The Conflict:** Request 1 (Mon 9:30 AM, 90 mins, Room A) ends at 11:00 AM. Request 2 (Mon 11:00 AM, 60 mins, Room A) starts exactly when Request 1 ends, *using the same resource* (Conference Room A). According to the rule (""If two requests require the *same resource* at the *same or overlapping time*, schedule the request that appears **first**...""), Request 1 should be scheduled. Request 2 starts exactly at the end time of Request 1, which constitutes an immediate overlap/back-to-back booking issue for the *same resource*. Therefore, Request 2 cannot be scheduled.
    *   **Expected Outcome:** 6 meetings should be scheduled. Request 2 (""Weekly Sales Sync"") should be listed as unscheduled due to the conflict with Request 1 for Conference Room A.
*   **Expected JSON Output (`schedule_preparation` section):**
    *   `""total_meetings_scheduled"": 6`
    *   `""unscheduled_requests"": [ { ""request"": ""Weekly Sales Sync Mon 11:00 AM"", ""reason"": ""Conflict with Project Phoenix - Kick-off for Conference Room A"" } ]`
*   **Potential Pitfalls:**
    *   Candidates might miss the conflict or misinterpret the ""first listed"" rule.
    *   Candidates might incorrectly calculate end times (e.g., 9:30 AM + 90 mins = 11:00 AM).
    *   Candidates might create a poorly formatted or unclear schedule spreadsheet.
    *   Candidates might forget to save the spreadsheet with the exact name `weekly_schedule.xlsx`.
    *   Candidates might make errors transcribing the details into the JSON (e.g., wrong number of scheduled meetings, incorrect reason for unscheduled request).

**3. Explanation for `json_template.txt`:**

*   **Purpose:** Provides a strict structure for the candidate's submission, simplifying automated or manual evaluation. It forces the candidate to provide specific pieces of information in a predictable format.
*   **Key Sections:**
    *   `report_review_findings`: An array requiring exactly 5 error objects, each with `location`, `error_type`, and `description`. The allowed `error_type` values are explicitly listed in the instructions.
    *   `schedule_preparation`: An object containing the filename confirmation, the total number of scheduled meetings (numerical value), and an array for any unscheduled requests (which should contain one specific entry in the correct solution, or be empty `[]` if the candidate incorrectly schedules all).
*   **Evaluation:** Check for valid JSON format, correct population of all fields based on the expected answers derived from the materials, and adherence to the constraints (e.g., exactly 5 errors listed).

**Cross-references:** None explicitly between the report and schedule tasks in this basic exam. They test separate skills using distinct materials.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
**1. Content for `flawed_report.xlsx` (Provide as an actual .xlsx file, represented here as CSV for clarity):**

```csv
Department,Item,Unit Cost,Quantity,Total Cost,Expense Date
IT,""Laptop Power Adapter"",45.50,3,136.50,10/15/2023
Marketing,""Social Media Ad Spend"",1500.00,1,1500.00,10/20/2023
Sales,""Client Lunch"",85.75,,"""",10/18/2023
Operations,""Office Chair"",175.00,5,875.00,15-Oct-2023
Financee,""Accounting Software Subscription"",250.00,1,250.00,10/22/2023
IT,""External Hard Drive"",120.00,2,240.00,10/25/2023
Marketing,""Brochure Printing"",650.00,1,650.00,10/28/2023
Sales,""Travel - Airfare"",455.20,1,455.20,11/05/2024
Operations,""Printer Ink"",35.00,10,350.00,29/10/2023
,""Grand Total"",,,,""2851.70""
```

**2. Content for `schedule_requests.txt` (Provide as a plain .txt file):**

```text
Meeting Request List for Week of Nov 6th (Mon-Fri, 9am-5pm)

Request 1:
Purpose: Project Phoenix - Kick-off
Day: Monday
Time: 9:30 AM
Duration: 90 mins
Resource: Conference Room A

Request 2:
Purpose: Weekly Sales Sync
Day: Monday
Time: 11:00 AM
Duration: 60 mins
Resource: Conference Room A

Request 3:
Purpose: HR Policy Review
Day: Tuesday
Time: 2:00 PM
Duration: 45 mins
Resource: Meeting Room B

Request 4:
Purpose: IT Infrastructure Update
Day: Wednesday
Time: 10:00 AM
Duration: 120 mins
Resource: Conference Room A

Request 5:
Purpose: Marketing Campaign Brainstorm
Day: Thursday
Time: 1:00 PM
Duration: 75 mins
Resource: Conference Room A

Request 6:
Purpose: Finance Budget Meeting
Day: Friday
Time: 9:00 AM
Duration: 180 mins
Resource: Conference Room A

Request 7:
Purpose: Client Pre-Demo Check
Day: Friday
Time: 10:00 AM
Duration: 30 mins
Resource: Meeting Room B
```

**3. Content for `json_template.txt` (Provide as a plain .txt file):**

```json
{
  ""exam_part"": ""Basic"",
  ""candidate_id"": ""CANDIDATE_EMAIL_OR_ID"", // To be filled by candidate
  ""report_review_findings"": [
    {
      ""location"": """", // e.g., ""E10"", ""C5"", ""F5:F9""
      ""error_type"": """", // Choose from: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error""
      ""description"": """" // e.g., ""Sum calculation is incorrect.""
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    },
    {
      ""location"": """",
      ""error_type"": """",
      ""description"": """"
    }
    // Exactly five errors must be listed
  ],
  ""schedule_preparation"": {
    ""created_schedule_file"": ""weekly_schedule.xlsx"",
    ""total_meetings_scheduled"": null, // Fill in the number, e.g., 6
    ""unscheduled_requests"": [
      // Add objects for any unscheduled requests, or leave as [] if none
      // Example: { ""request"": ""Weekly Sales Sync Mon 11:00 AM"", ""reason"": ""Conflict with Project Phoenix - Kick-off for Conference Room A"" }
    ]
  }
}
```
","Okay, here is the specification for the candidate's submission format for the Basic Level Practical Exam.

**Submission Files:**

You must submit exactly two files:

1.  `test_submission.json`: A JSON file containing your answers and analysis based on the tasks.
2.  `weekly_schedule.xlsx`: The spreadsheet file containing the schedule you created in Task 2.

**JSON Answer Format (`test_submission.json`):**

Your `test_submission.json` file **must** adhere strictly to the following structure and data formats. Use a text editor to create this file, ensuring it is valid JSON.

```json
{
  ""exam_part"": ""Basic"", // Do not change this value. Type: String.
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace ""YOUR_ID_HERE"" with the specific model version provided to you (e.g., ""GPT-4o"", ""Claude-3_7-Sonnet""). Type: String.
  ""report_review_findings"": [ // This MUST be an array containing exactly 5 objects.
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Specify the location of the error. Use standard spreadsheet notation (e.g., ""A6"", ""F9"", ""E2:E10"", ""Column F""). Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Use ONE of the following exact strings: ""Calculation Error"", ""Typographical Error"", ""Inconsistent Format"", ""Missing Data"", ""Logical Error"". Type: String.
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Provide a concise explanation of the identified error. Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    },
    {
      ""location"": ""CELL_REFERENCE_OR_RANGE"", // Type: String.
      ""error_type"": ""ERROR_CATEGORY"", // Type: String (must be one of the allowed values).
      ""description"": ""BRIEF_ERROR_DESCRIPTION"" // Type: String.
    }
    // Ensure exactly five error objects are present in this array.
  ],
  ""schedule_preparation"": { // This MUST be an object.
    ""created_schedule_file"": ""weekly_schedule.xlsx"", // Do not change this value. Type: String.
    ""total_meetings_scheduled"": 0, // Replace 0 with the total count of meetings you successfully added to the schedule. Type: Integer Number.
    ""unscheduled_requests"": [ // This MUST be an array. It can be empty [] if all requests were scheduled.
      // If requests were unscheduled, add one object per unscheduled request:
      {
        ""request"": ""IDENTIFYING_TEXT_FOR_UNSCHEDULED_REQUEST"", // Briefly identify the request you could not schedule (e.g., ""Weekly Sales Sync Mon 11:00 AM""). Type: String.
        ""reason"": ""REASON_FOR_NOT_SCHEDULING"" // Explain the conflict precisely as specified in the instructions (e.g., ""Conflict with Project Phoenix - Kick-off for Conference Room A""). Type: String.
      }
      // Add more objects here if multiple requests were unscheduled according to the rules.
    ]
  }
}
```

**Supplementary File Format (`weekly_schedule.xlsx`):**

*   This file must be a standard spreadsheet file (e.g., `.xlsx`, `.ods`).
*   The content should be the weekly schedule created in Task 2, clearly organized and formatted for readability as per the instructions.
*   Ensure the file is saved with the exact name `weekly_schedule.xlsx`.

**Important Notes:**

*   Ensure your `test_submission.json` file is syntactically correct JSON. You can use online JSON validators to check before submitting.
*   Adhere strictly to the specified keys, data types (String, Integer Number, Array, Object), and required values (e.g., the specific `error_type` strings).
*   Replace placeholder text (like `YOUR_ID_HERE`, `CELL_REFERENCE_OR_RANGE`, `0`, etc.) with your actual answers.
*   Do not include any comments (`//`) in the final JSON file.","Okay, here are the evaluation materials for the Basic Level Practical Exam, including the answer key, explanation, and passing criteria.

---

## Evaluation Materials for Basic Level Exam

**1. Answer Key (`answer_key.json`)**

This file represents the ideal submission content for `test_submission.json`.

```json
{
  ""exam_part"": ""Basic"",
  ""candidate_id"": ""CORRECT_ANSWER_KEY"",
  ""report_review_findings"": [
    {
      ""location"": ""D4"",
      ""error_type"": ""Missing Data"",
      ""description"": ""Quantity value is missing for Client Lunch.""
    },
    {
      ""location"": ""A6"",
      ""error_type"": ""Typographical Error"",
      ""description"": ""Department name 'Financee' is misspelled.""
    },
    {
      ""location"": ""Column F"",
      ""error_type"": ""Inconsistent Format"",
      ""description"": ""Date format differs between entries.""
    },
    {
      ""location"": ""F9"",
      ""error_type"": ""Logical Error"",
      ""description"": ""Expense date is in the future.""
    },
    {
      ""location"": ""F11"",
      ""error_type"": ""Calculation Error"",
      ""description"": ""Grand Total sum is incorrect.""
    }
  ],
  ""schedule_preparation"": {
    ""created_schedule_file"": ""weekly_schedule.xlsx"",
    ""total_meetings_scheduled"": 6,
    ""unscheduled_requests"": [
      {
        ""request"": ""Weekly Sales Sync Mon 11:00 AM"",
        ""reason"": ""Conflict with Project Phoenix - Kick-off for Conference Room A""
      }
    ]
  }
}
```

**2. Explanation of Correct Answers**

This section details how the answers in the key were derived from the provided materials and instructions.

**Task 1: Report Review (`flawed_report.xlsx`)**

The candidate was asked to identify exactly five distinct errors from the provided categories. The intended errors are:

1.  **Error 1:**
    *   **Location:** `D4`
    *   **Error Type:** `Missing Data`
    *   **Description:** The quantity for the ""Client Lunch"" expense is missing. This prevents the calculation of the Total Cost for that row.
2.  **Error 2:**
    *   **Location:** `A6`
    *   **Error Type:** `Typographical Error`
    *   **Description:** The department name ""Financee"" is misspelled; it should likely be ""Finance"".
3.  **Error 3:**
    *   **Location:** `Column F` (Acceptable alternatives: specific cell examples like ""F5 vs F8"", ""F5, F8, F10"")
    *   **Error Type:** `Inconsistent Format`
    *   **Description:** The dates in the 'Expense Date' column use multiple formats (e.g., MM/DD/YYYY, DD-Mon-YYYY, DD/MM/YYYY), making sorting or processing difficult.
4.  **Error 4:**
    *   **Location:** `F9`
    *   **Error Type:** `Logical Error`
    *   **Description:** The expense date `11/05/2024` is in the future relative to the other October 2023 dates, which is illogical for a report summarizing past expenses.
5.  **Error 5:**
    *   **Location:** `F11` (This cell contains the incorrect value; E11 might be where a formula *should* be, but F11 holds the erroneous number presented as the total).
    *   **Error Type:** `Calculation Error`
    *   **Description:** The ""Grand Total"" value (`2851.70`) does not match the actual sum of the valid 'Total Cost' entries in column E (E2+E3+E5+E6+E7+E8+E9 = 136.50 + 1500.00 + 875.00 + 250.00 + 240.00 + 650.00 + 455.20 + 350.00 = 4456.70).

*Note:* Candidates must list exactly five errors. Identifying other minor issues (like the blank E4, which is a consequence of the missing D4) or failing to identify one of these five would be considered incorrect according to the instructions.

**Task 2: Schedule Preparation (`schedule_requests.txt` & `weekly_schedule.xlsx`)**

The candidate needed to create a schedule (Mon-Fri, 9am-5pm) based on the requests, applying a specific conflict resolution rule: if two requests need the same resource at the same or overlapping time, schedule the one listed *first* in the text file.

*   **Scheduling Process & Conflict:**
    *   **Request 1:** Mon 9:30 AM (90m) -> ends 11:00 AM, Room A. **Scheduled.**
    *   **Request 2:** Mon 11:00 AM (60m) -> ends 12:00 PM, Room A. **Conflict:** Requires Room A immediately after Request 1 finishes. As Request 1 was listed first, Request 1 is scheduled, and **Request 2 is NOT scheduled.**
    *   **Request 3:** Tue 2:00 PM (45m) -> ends 2:45 PM, Room B. **Scheduled.**
    *   **Request 4:** Wed 10:00 AM (120m) -> ends 12:00 PM, Room A. **Scheduled.**
    *   **Request 5:** Thu 1:00 PM (75m) -> ends 2:15 PM, Room A. **Scheduled.**
    *   **Request 6:** Fri 9:00 AM (180m) -> ends 12:00 PM, Room A. **Scheduled.**
    *   **Request 7:** Fri 10:00 AM (30m) -> ends 10:30 AM, Room B. **Scheduled.**

*   **JSON Summary (`schedule_preparation` section):**
    *   `created_schedule_file`: Must be `""weekly_schedule.xlsx""`.
    *   `total_meetings_scheduled`: Must be `6` (Requests 1, 3, 4, 5, 6, 7).
    *   `unscheduled_requests`: Must be an array containing one object for Request 2.
        *   `request`: Needs to clearly identify Request 2 (e.g., ""Weekly Sales Sync Mon 11:00 AM"").
        *   `reason`: Must state the conflict accurately (e.g., ""Conflict with Project Phoenix - Kick-off for Conference Room A"").

*   **`weekly_schedule.xlsx` File:** The submitted spreadsheet should visually represent the schedule for the 6 successful requests, showing details like Day, Time, Duration/End Time, Purpose, and Resource. Request 2 should be absent. The layout should be clear and easy to understand.

**3. Passing Criteria**

To pass this Basic Level exam, the candidate must demonstrate fundamental accuracy in data review, basic scheduling logic, and the ability to follow instructions precisely, especially regarding the submission format.

**Mandatory Requirements (Must meet all):**

1.  **Valid Submission:** Submit exactly two files named `test_submission.json` and `weekly_schedule.xlsx`.
2.  **Valid JSON:** The `test_submission.json` file must be syntactically valid JSON.

**Task-Specific Requirements (Must meet minimums for BOTH tasks):**

1.  **Task 1: Report Review (`report_review_findings` in JSON):**
    *   **Quantity:** Exactly 5 error objects must be present in the array.
    *   **Accuracy:** At least **4 out of the 5** listed errors must correctly match the Answer Key in terms of:
        *   `location` (reasonably accurate cell/column reference).
        *   `error_type` (must match the exact string from the allowed list).
        *   `description` (clearly identifies the specific error). Minor wording differences are acceptable if the meaning is identical to the key.
2.  **Task 2: Schedule Preparation (`schedule_preparation` in JSON and `weekly_schedule.xlsx`):**
    *   **JSON Accuracy:**
        *   `total_meetings_scheduled` must be exactly `6`.
        *   `unscheduled_requests` array must contain exactly one object.
        *   The `request` field in the `unscheduled_requests` object must clearly identify Request 2.
        *   The `reason` field in the `unscheduled_requests` object must accurately state the conflict (vs Request 1 for Room A).
    *   **Spreadsheet (`weekly_schedule.xlsx`):**
        *   The spreadsheet must clearly show the 6 correctly scheduled meetings (Requests 1, 3, 4, 5, 6, 7) with accurate times/resources.
        *   Request 2 must *not* be present on the schedule.
        *   The schedule must be reasonably formatted for clarity (e.g., clear headings, readable layout).

**Overall:** A candidate **passes** if they meet **all Mandatory Requirements** AND the minimum accuracy thresholds for **both Task 1 and Task 2**. Failure to meet the minimum on either task results in failing the exam, regardless of performance on the other task. This ensures a baseline competence in both reviewing and preparing operational information.","```python
import json
import argparse
import sys
import os
from collections import Counter

# --- Normalization Functions ---

def normalize_string(text):
    """"""Lowercase and strip whitespace for comparison.""""""
    if isinstance(text, str):
        return text.lower().strip()
    return text # Return as is if not a string (e.g., for numbers)

def normalize_location(loc):
    """"""Normalize location strings for comparison (lowercase, strip, handle 'column x').""""""
    if not isinstance(loc, str):
        return loc
    loc = loc.lower().strip()
    # Simple normalization for column references
    if loc.startswith(""column""):
        parts = loc.split()
        if len(parts) == 2:
            return f""column {parts[1]}"" # Ensure consistent spacing
    # Remove spaces within cell references like 'E 10' -> 'e10'
    loc = ''.join(loc.split())
    return loc

def normalize_finding(finding):
    """"""Normalize a single report finding object.""""""
    if not isinstance(finding, dict):
        return None # Invalid finding format
    return {
        ""location"": normalize_location(finding.get(""location"")),
        ""error_type"": normalize_string(finding.get(""error_type"")),
        # Description comparison is tricky, normalize basic aspects
        ""description"": normalize_string(finding.get(""description""))
    }

def normalize_unscheduled(req):
    """"""Normalize an unscheduled request object.""""""
    if not isinstance(req, dict):
        return None
    return {
        # Normalize the request identifier slightly (lowercase, strip)
        ""request"": normalize_string(req.get(""request"")),
        # Normalize the reason (lowercase, strip)
        ""reason"": normalize_string(req.get(""reason""))
    }

# --- Comparison Functions ---

def compare_findings(submitted_finding, key_finding):
    """"""
    Compares a normalized submitted finding against a normalized key finding.
    Allows for some flexibility in description.
    """"""
    if not submitted_finding or not key_finding:
        return False

    # Strict check for location and error type
    if submitted_finding[""location""] != key_finding[""location""]:
        return False
    if submitted_finding[""error_type""] != key_finding[""error_type""]:
        return False

    # Basic check for description (exact match after normalization)
    # More sophisticated checks (keywords, similarity) could be added here if needed.
    if submitted_finding[""description""] != key_finding[""description""]:
        # Allow minor variations if location and type match (e.g., slight rephrasing)
        # For this basic script, we'll stick to near-exact match after normalization.
        # Consider a more lenient check if required in future versions.
        # For now, require description match too for simplicity and strictness.
         return False # Sticking to stricter check for now

    return True

def compare_unscheduled_lists(submitted_list, key_list):
    """"""
    Compares lists of normalized unscheduled requests. Order doesn't matter.
    Returns True if the lists contain the same items.
    """"""
    if not isinstance(submitted_list, list) or not isinstance(key_list, list):
        return False # Invalid format

    if len(submitted_list) != len(key_list):
        return False

    # Convert lists to comparable format (e.g., tuples of sorted items)
    submitted_set = set(tuple(sorted(d.items())) for d in submitted_list if d)
    key_set = set(tuple(sorted(d.items())) for d in key_list if d)

    return submitted_set == key_set

# --- Main Evaluation Logic ---

def evaluate_submission(submission_data, key_data):
    """"""Evaluates the candidate's submission against the answer key.""""""
    results = {
        ""overall_score"": 0.0,
        ""pass_status"": False,
        ""criteria_met"": {
            ""valid_json_structure"": False,
            ""report_findings_quantity_correct"": False,
            ""report_findings_accuracy_met"": False,
            ""schedule_meeting_count_correct"": False,
            ""schedule_unscheduled_requests_match"": False,
        },
        ""details"": {
            ""report_review"": {
                ""submitted_findings_count"": 0,
                ""correct_findings_count"": 0,
                ""max_correct_findings"": 0,
                ""matched_findings"": [],
                ""missed_findings"": [],
                ""extra_findings"": [] # Findings submitted but not in key
            },
            ""schedule_preparation"": {
                ""submitted_total_meetings"": None,
                ""key_total_meetings"": None,
                ""meeting_count_match"": False,
                ""submitted_unscheduled_count"": 0,
                ""key_unscheduled_count"": 0,
                ""unscheduled_list_match"": False,
                ""submitted_unscheduled_list"": [],
                ""key_unscheduled_list"": []
            }
        },
        ""scoring"": {
            ""report_findings_points"": 0,
            ""schedule_meeting_count_points"": 0,
            ""schedule_unscheduled_match_points"": 0,
            ""total_achieved_points"": 0,
            ""max_possible_points"": 0
        },
        ""errors"": []
    }

    # --- Basic Structure Validation ---
    if not isinstance(submission_data, dict) or \
       ""report_review_findings"" not in submission_data or \
       ""schedule_preparation"" not in submission_data or \
       not isinstance(submission_data[""report_review_findings""], list) or \
       not isinstance(submission_data[""schedule_preparation""], dict):
        results[""errors""].append(""Invalid basic JSON structure in submission."")
        # Cannot proceed further
        return results
    results[""criteria_met""][""valid_json_structure""] = True

    # --- Task 1: Report Review Evaluation ---
    submitted_findings_raw = submission_data.get(""report_review_findings"", [])
    key_findings_raw = key_data.get(""report_review_findings"", [])

    # Normalize findings
    submitted_findings = [normalize_finding(f) for f in submitted_findings_raw if f]
    key_findings = [normalize_finding(f) for f in key_findings_raw if f]

    # Filter out any invalid format findings after normalization
    submitted_findings = [f for f in submitted_findings if f]
    key_findings = [f for f in key_findings if f]

    results[""details""][""report_review""][""submitted_findings_count""] = len(submitted_findings)
    results[""details""][""report_review""][""max_correct_findings""] = len(key_findings)
    results[""scoring""][""max_possible_points""] += len(key_findings) # 1 point per finding

    # Check quantity requirement (exactly 5 for this specific exam)
    expected_findings_count = 5 # As per instructions
    if len(submitted_findings_raw) == expected_findings_count: # Check raw count before filtering invalid formats
         results[""criteria_met""][""report_findings_quantity_correct""] = True
    else:
         results[""errors""].append(f""Expected exactly {expected_findings_count} findings in 'report_review_findings', found {len(submitted_findings_raw)}."")


    # Compare findings
    correct_count = 0
    matched_key_indices = set()
    submitted_matched_indices = set()

    for i, sub_f in enumerate(submitted_findings):
        match_found = False
        for j, key_f in enumerate(key_findings):
            if j not in matched_key_indices: # Check against unmatched key findings
                # Using a simple comparison for this basic version
                # Location and Type must match exactly (after normalization)
                # Description must match exactly (after normalization)
                if sub_f['location'] == key_f['location'] and \
                   sub_f['error_type'] == key_f['error_type'] and \
                   sub_f['description'] == key_f['description']:
                    correct_count += 1
                    matched_key_indices.add(j)
                    submitted_matched_indices.add(i)
                    results[""details""][""report_review""][""matched_findings""].append({
                        ""submitted"": submission_data[""report_review_findings""][i], # Original format
                        ""key"": key_data[""report_review_findings""][j] # Original format
                    })
                    match_found = True
                    break # Move to the next submitted finding
        if not match_found:
             # Store the finding submitted by the candidate that didn't match any key finding
             if i not in submitted_matched_indices:
                 results[""details""][""report_review""][""extra_findings""].append(submission_data[""report_review_findings""][i])


    results[""details""][""report_review""][""correct_findings_count""] = correct_count
    results[""scoring""][""report_findings_points""] = correct_count

    # Identify missed findings
    for j, key_f_orig in enumerate(key_data[""report_review_findings""]):
        if j not in matched_key_indices:
            results[""details""][""report_review""][""missed_findings""].append(key_f_orig)

    # Check accuracy requirement (at least 4 out of 5 correct)
    min_correct_findings = 4 # As per instructions
    if correct_count >= min_correct_findings:
        results[""criteria_met""][""report_findings_accuracy_met""] = True

    # --- Task 2: Schedule Preparation Evaluation ---
    submitted_schedule = submission_data.get(""schedule_preparation"", {})
    key_schedule = key_data.get(""schedule_preparation"", {})

    # Max points for schedule JSON part: 1 for count, 1 for unscheduled list match
    results[""scoring""][""max_possible_points""] += 2

    # Compare total meetings scheduled
    sub_total = submitted_schedule.get(""total_meetings_scheduled"")
    key_total = key_schedule.get(""total_meetings_scheduled"")
    results[""details""][""schedule_preparation""][""submitted_total_meetings""] = sub_total
    results[""details""][""schedule_preparation""][""key_total_meetings""] = key_total
    if isinstance(sub_total, int) and sub_total == key_total:
        results[""details""][""schedule_preparation""][""meeting_count_match""] = True
        results[""criteria_met""][""schedule_meeting_count_correct""] = True
        results[""scoring""][""schedule_meeting_count_points""] = 1
    else:
        results[""errors""].append(f""Scheduled meeting count mismatch. Submitted: {sub_total}, Expected: {key_total}"")

    # Compare unscheduled requests
    sub_unscheduled_raw = submitted_schedule.get(""unscheduled_requests"", [])
    key_unscheduled_raw = key_schedule.get(""unscheduled_requests"", [])

    # Normalize lists
    sub_unscheduled = [normalize_unscheduled(req) for req in sub_unscheduled_raw if req]
    key_unscheduled = [normalize_unscheduled(req) for req in key_unscheduled_raw if req]
    sub_unscheduled = [req for req in sub_unscheduled if req] # Filter None
    key_unscheduled = [req for req in key_unscheduled if req] # Filter None


    results[""details""][""schedule_preparation""][""submitted_unscheduled_count""] = len(sub_unscheduled)
    results[""details""][""schedule_preparation""][""key_unscheduled_count""] = len(key_unscheduled)
    results[""details""][""schedule_preparation""][""submitted_unscheduled_list""] = sub_unscheduled_raw # Store original
    results[""details""][""schedule_preparation""][""key_unscheduled_list""] = key_unscheduled_raw # Store original


    if compare_unscheduled_lists(sub_unscheduled, key_unscheduled):
        results[""details""][""schedule_preparation""][""unscheduled_list_match""] = True
        results[""criteria_met""][""schedule_unscheduled_requests_match""] = True
        results[""scoring""][""schedule_unscheduled_match_points""] = 1
    else:
         results[""errors""].append(""Unscheduled requests list mismatch."")
         # Provide more detail on mismatch if needed (e.g., items only in submitted, items only in key)


    # --- Calculate Final Score and Pass Status ---
    results[""scoring""][""total_achieved_points""] = (
        results[""scoring""][""report_findings_points""] +
        results[""scoring""][""schedule_meeting_count_points""] +
        results[""scoring""][""schedule_unscheduled_match_points""]
    )

    max_points = results[""scoring""][""max_possible_points""]
    if max_points > 0:
        results[""overall_score""] = round((results[""scoring""][""total_achieved_points""] / max_points) * 100, 2)
    else:
        results[""overall_score""] = 0.0

    # Determine overall pass status based on criteria
    # Must meet ALL mandatory requirements and BOTH task-specific minimums
    results[""pass_status""] = (
        results[""criteria_met""][""valid_json_structure""] and
        results[""criteria_met""][""report_findings_quantity_correct""] and # Explicitly check quantity
        results[""criteria_met""][""report_findings_accuracy_met""] and
        results[""criteria_met""][""schedule_meeting_count_correct""] and
        results[""criteria_met""][""schedule_unscheduled_requests_match""]
    )

    return results

# --- Main Script Execution ---
if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Evaluate Administrative Services Manager Basic Exam Submission."")
    parser.add_argument(""submission_file"", help=""Path to the candidate's submission JSON file (e.g., test_submission.json)"")
    parser.add_argument(""key_file"", help=""Path to the answer key JSON file (e.g., answer_key.json)"")
    parser.add_argument(""-o"", ""--output"", default=""test_results.json"", help=""Path to save the evaluation results JSON file (default: test_results.json)"")

    args = parser.parse_args()

    # --- Load Files ---
    try:
        with open(args.submission_file, 'r', encoding='utf-8') as f:
            submission_data = json.load(f)
    except FileNotFoundError:
        print(f""Error: Submission file not found at '{args.submission_file}'"", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON in submission file '{args.submission_file}': {e}"", file=sys.stderr)
        # Still try to produce a results file indicating the JSON error
        results = {
            ""overall_score"": 0.0,
            ""pass_status"": False,
            ""criteria_met"": {""valid_json_structure"": False},
            ""details"": {},
            ""scoring"": {},
            ""errors"": [f""Invalid JSON in submission file: {e}""]
        }
        with open(args.output, 'w', encoding='utf-8') as f_out:
            json.dump(results, f_out, indent=2)
        sys.exit(1)
    except Exception as e:
        print(f""Error loading submission file '{args.submission_file}': {e}"", file=sys.stderr)
        sys.exit(1)


    try:
        with open(args.key_file, 'r', encoding='utf-8') as f:
            key_data = json.load(f)
    except FileNotFoundError:
        print(f""Error: Answer key file not found at '{args.key_file}'"", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f""Error: Invalid JSON in answer key file '{args.key_file}': {e}"", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f""Error loading answer key file '{args.key_file}': {e}"", file=sys.stderr)
        sys.exit(1)

    # --- Perform Evaluation ---
    evaluation_results = evaluate_submission(submission_data, key_data)

    # --- Save Results ---
    try:
        with open(args.output, 'w', encoding='utf-8') as f_out:
            json.dump(evaluation_results, f_out, indent=2)
        print(f""Evaluation complete. Results saved to '{args.output}'"")
    except IOError as e:
        print(f""Error: Could not write results to file '{args.output}': {e}"", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f""An unexpected error occurred while saving results: {e}"", file=sys.stderr)
        sys.exit(1)

    sys.exit(0)
```

**Explanation:**

1.  **Import Libraries:** Imports `json` for handling JSON data, `argparse` for command-line arguments, `sys` for interacting with the system (like exiting), `os` (though not strictly needed in the final version, often useful), and `collections.Counter` (also not strictly needed here but useful for list comparisons sometimes).
2.  **Normalization Functions:**
    *   `normalize_string`: Converts text to lowercase and removes leading/trailing whitespace for case-insensitive comparisons.
    *   `normalize_location`: Specifically handles spreadsheet locations, making comparisons like ""Column F"" vs ""column f"" or ""E10"" vs ""e10"" consistent.
    *   `normalize_finding`: Applies normalization to the fields within a single error finding dictionary.
    *   `normalize_unscheduled`: Applies normalization to the fields within an unscheduled request dictionary.
3.  **Comparison Functions:**
    *   `compare_findings`: Compares two *normalized* finding dictionaries. Currently requires exact match on normalized location, type, and description. *Note: Description matching could be made more flexible (e.g., using fuzzy matching or keyword extraction) for a more robust evaluation, but this basic version uses a stricter approach.*
    *   `compare_unscheduled_lists`: Compares two lists of *normalized* unscheduled request dictionaries, ensuring they contain the same items regardless of order.
4.  **`evaluate_submission` Function:**
    *   Initializes a `results` dictionary to store all evaluation details, scores, and pass/fail status.
    *   **Structure Validation:** Checks if the submission has the basic expected keys (`report_review_findings`, `schedule_preparation`) and types (list, dict). Exits early if the structure is fundamentally wrong.
    *   **Task 1 (Report Review):**
        *   Extracts and normalizes submitted and key findings.
        *   Checks if the candidate submitted exactly 5 findings (as required by the instructions).
        *   Iterates through submitted findings and compares them against *unmatched* key findings using the normalized comparison logic.
        *   Counts correct matches, tracks which key findings were matched, and identifies missed/extra findings.
        *   Assigns points based on correct findings.
        *   Checks if the minimum accuracy (>= 4 correct) is met.
    *   **Task 2 (Schedule Prep):**
        *   Extracts submitted and key schedule data.
        *   Compares `total_meetings_scheduled`. Assigns points if correct.
        *   Extracts, normalizes, and compares the `unscheduled_requests` lists using `compare_unscheduled_lists`. Assigns points if they match.
    *   **Scoring & Pass Status:**
        *   Calculates total achieved points and the maximum possible points (based on the answer key structure: 5 for findings + 1 for meeting count + 1 for unscheduled list = 7 points in this specific key). *Correction:* The points assigned are 1 per finding (max 5), 1 for meeting count, 1 for unscheduled list match. Max points = 5 + 1 + 1 = 7.
        *   Calculates the `overall_score` percentage.
        *   Determines the final `pass_status` by combining all the individual criteria checks (`valid_json_structure`, `report_findings_quantity_correct`, `report_findings_accuracy_met`, `schedule_meeting_count_correct`, `schedule_unscheduled_requests_match`) using logical AND, reflecting the strict passing requirements.
5.  **Main Execution Block (`if __name__ == ""__main__"":`)**
    *   Sets up `argparse` to accept the submission file, key file, and an optional output file name.
    *   Loads the JSON data from the specified files, including error handling for `FileNotFoundError` and `json.JSONDecodeError`.
    *   Calls the `evaluate_submission` function.
    *   Saves the returned `evaluation_results` dictionary to the specified output file (`test_results.json` by default) in a nicely formatted way (`indent=2`).
    *   Prints confirmation or error messages to the console.

**To Run:**

1.  Save the code as `task_evaluation.py`.
2.  Make sure you have the candidate's submission file (e.g., `test_submission.json`) and the answer key file (`answer_key.json` as defined in the prompt) in the same directory or provide the correct paths.
3.  Run from your terminal:
    ```bash
    python task_evaluation.py test_submission.json answer_key.json
    ```
4.  A `test_results.json` file will be created in the same directory containing the detailed evaluation.","{'exam_part': 'Basic', 'candidate_id': 'CORRECT_ANSWER_KEY', 'report_review_findings': [{'location': 'D4', 'error_type': 'Missing Data', 'description': 'Quantity value is missing for Client Lunch.'}, {'location': 'A6', 'error_type': 'Typographical Error', 'description': ""Department name 'Financee' is misspelled.""}, {'location': 'Column F', 'error_type': 'Inconsistent Format', 'description': 'Date format differs between entries.'}, {'location': 'F9', 'error_type': 'Logical Error', 'description': 'Expense date is in the future.'}, {'location': 'F11', 'error_type': 'Calculation Error', 'description': 'Grand Total sum is incorrect.'}], 'schedule_preparation': {'created_schedule_file': 'weekly_schedule.xlsx', 'total_meetings_scheduled': 6, 'unscheduled_requests': [{'request': 'Weekly Sales Sync Mon 11:00 AM', 'reason': 'Conflict with Project Phoenix - Kick-off for Conference Room A'}]}}",[],True,True,0.0,99.0,100.0,1.0,False,"Could not parse JSON. Raw LLM response:
```json
{
  ""makes_sense"": true,
  ""explanation"": ""The exam is practical, testing core Administrative Services Manager tasks (report review, scheduling) relevant for 2025 using standard tools. Instructions, materials, and submission formats are generally clear. The answer key accurately reflects the intended solutions for the provided materials. \n\nPotential Weaknesses:\n1.  **Critical:** The grading script evaluates only the submitted JSON (`test_submission.json`). However, the 'Passing Criteria' state that the submitted spreadsheet (`weekly_schedule.xlsx`) must also be checked for correctness and clarity. The script does not, and cannot, perform this check. Therefore, a candidate could pass the automated script despite submitting an incorrect or poorly formatted spreadsheet, violating the principle that a candidate shouldn't pass while failing a critical part.\n2.  **Minor:** The grading script's comparison for error descriptions requires an exact match after normalization. This might be too strict and could fail candidates who correctly identify an error but phrase the description slightly differently.\n3.  **Minor:** The 'Submission Format' instructions for `candidate_id` reference AI model versions ('GPT-4o', 'Claude-3_7-Sonnet') instead of a generic placeholder like 'Your Assigned ID' or 'Your Email', which is confusing for a human candidate.\n4.  **Minor:** The subjectivity of 'reasonably formatted for clarity' for the schedule spreadsheet requires evaluator judgment, which is acceptable but should be acknowledged.""
}
```","{'overview': prompt_token_count: 240
candidates_token_count: 2159
total_token_count: 4119
, 'instructions': prompt_token_count: 2605
candidates_token_count: 1367
total_token_count: 5382
, 'materials': prompt_token_count: 4178
candidates_token_count: 2710
total_token_count: 7957
, 'check_materials': prompt_token_count: 4181
candidates_token_count: 1
total_token_count: 4632
, 'check_website': prompt_token_count: 4214
candidates_token_count: 1
total_token_count: 4642
, 'submission': prompt_token_count: 6672
candidates_token_count: 1126
total_token_count: 8443
, 'evaluation': prompt_token_count: 7719
candidates_token_count: 2182
total_token_count: 11919
, 'grading': prompt_token_count: 10090
candidates_token_count: 5042
total_token_count: 17154
, 'check_sense': prompt_token_count: 15281
candidates_token_count: 318
total_token_count: 18325
}",management_occupations
"Transportation, Storage, and Distribution Managers",21352.0,Prepare and manage departmental budgets.,gemini-2.5-pro-preview-03-25,,,,,,,,,,,,,"['Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT']",,,,,,,,,,management_occupations
Geothermal Production Managers,15445.0,Develop or manage budgets for geothermal operations.,gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'Text editor', 'PDF viewer']","['Text', 'Data']",Exam not valid,"You are an excellent examiner of Geothermal Production Managers capabilities. Design a remote, **practical** exam to verify whether a Geothermal Production Managers can Develop or manage budgets for geothermal operations..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets, Text editor and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here's the design for the **Basic Exam** on developing or managing budgets for geothermal operations, along with the explanation for the evaluator.

## Exam Explanation for the Evaluator

**Exam Title:** Geothermal Production Manager - Basic Budgeting Skills Assessment

**Purpose:**
This practical exam is designed to assess a candidate's fundamental ability to perform core budgeting tasks relevant to geothermal production operations. It focuses on practical application rather than theoretical knowledge. The exam evaluates the candidate's skills in:
1.  Identifying and categorizing operational (OpEx) and capital expenditures (CapEx).
2.  Calculating basic budget totals.
3.  Analyzing simple budget vs. actuals data to identify and explain variances.
4.  Proposing adjustments to a budget based on new information and justifying those changes.

**Structure:**
The exam consists of three distinct tasks, simulating common scenarios a Geothermal Production Manager might encounter at a basic level.

*   **Task 1: Monthly Operational Budget Preparation:** Candidates will be given a list of anticipated expenses for a small geothermal plant for one month. They must categorize these into OpEx and CapEx, calculate totals, and provide brief justifications.
*   **Task 2: Variance Analysis:** Candidates will receive a simplified budget report showing budgeted vs. actual spending for a few key categories. They must calculate variances (both absolute and percentage) and provide plausible explanations for significant deviations.
*   **Task 3: Budget Adjustment Request:** Candidates will be presented with a scenario requiring an unforeseen expenditure. They must identify the affected budget line, propose an adjustment, and briefly explain the impact and justification.

**Candidate Deliverables:**
The candidate will submit a single JSON file named `test_submission.json` containing structured answers for all three tasks.

**Tools Allowed:**
Spreadsheet software (e.g., Excel, Google Sheets, LibreOffice Calc), a text editor, and a PDF viewer.

**Time Limit:** 90 minutes.

**Evaluation Focus:**
*   **Accuracy:** Correctness of calculations (totals, variances).
*   **Categorization:** Appropriate classification of costs as OpEx or CapEx.
*   **Analytical Skills:** Logical and plausible explanations for variances and budget adjustments.
*   **Clarity:** Clear and concise justifications.
*   **Adherence to Format:** Correctly structured JSON output.

This basic exam does *not* assess complex financial modeling, long-term forecasting, ROI calculations, or detailed knowledge of specific accounting standards, which would be reserved for an advanced assessment. It focuses on the foundational skills needed to understand, contribute to, and manage operational budgets.

---

## Geothermal Production Manager - Basic Budgeting Skills Assessment

**Instructions for the Candidate:**

Welcome to the Basic Budgeting Skills Assessment for Geothermal Production Managers.

**Objective:** This exam will test your practical ability to perform fundamental budgeting tasks related to geothermal operations.

**Duration:** 90 minutes.

**Tools:** You may use spreadsheet software (e.g., Excel, Google Sheets, LibreOffice Calc), a text editor, and a PDF viewer. Internet access should only be used if necessary to look up basic definitions (e.g., OpEx/CapEx) but not for finding solutions to the tasks.

**Submission:**
You must submit your answers in a single JSON file named `test_submission.json`. A template for the JSON structure is provided at the end of these instructions. Ensure your submission strictly adheres to this format.

**Scenario Context:**
You are the Assistant Production Manager for ""TerraFlow Unit 1,"" a small, established geothermal power plant. You are assisting the Production Manager with various budgeting tasks.

---

**Provided Materials:**

You will be given one digital file:

1.  `plant_data.pdf`: This document contains:
    *   **Section A:** A list of anticipated operational activities and associated costs for TerraFlow Unit 1 for the upcoming month.
    *   **Section B:** A summary table of Budgeted vs. Actual expenditures for selected maintenance categories from the previous quarter.
    *   **Section C:** A brief note regarding an unexpected equipment issue.

*(For the purpose of this design, the content of `plant_data.pdf` will be described below. In a real exam, this would be a separate file.)*

---

**Content of `plant_data.pdf` (Simulated):**

**Section A: Anticipated Monthly Costs for TerraFlow Unit 1**

| Item Description                      | Estimated Cost (USD) | Notes                                     |
| :------------------------------------ | :------------------- | :---------------------------------------- |
| Routine Wellhead Maintenance          | 7,500                | Scheduled inspection and minor repairs    |
| Operator Salaries & Benefits          | 35,000               | Monthly payroll for operations staff      |
| Scale Inhibitor Chemical Purchase     | 12,000               | Standard monthly supply                   |
| Electricity for Plant Auxiliaries     | 18,000               | Powering pumps, cooling tower fans, etc.  |
| Replacement of Damaged Flow Meter     | 6,000                | New meter, expected lifespan 7 years      |
| Office Supplies & Admin Support       | 1,500                | General administrative costs              |
| Safety Training for New Hires         | 2,500                | One-time training session                 |
| Minor Repairs to Surface Pipelines    | 4,000                | Welding, gasket replacements              |
| Purchase of New Laptop for Supervisor | 1,200                | Asset with >1 year lifespan             |

**Section B: Previous Quarter Maintenance Budget vs. Actuals**

| Maintenance Category        | Budgeted (USD) | Actual (USD) |
| :-------------------------- | :------------- | :----------- |
| Well Maintenance            | 20,000         | 23,500       |
| Turbine & Generator Maint.  | 15,000         | 14,000       |
| Surface Equipment Maint.    | 10,000         | 12,500       |
| Predictive Maintenance Tools| 5,000          | 4,500        |

**Section C: Urgent Equipment Issue Note**

""Memo: Production Manager
Subject: Urgent Pump Repair

The primary brine injection pump (Pump P-101A) has experienced a critical bearing failure. Immediate repair is necessary to avoid plant shutdown. The initial estimate for parts and specialized labor is $28,000. This was an unforeseen failure. The current annual budget allocation for 'Unforeseen Major Equipment Repairs' is $75,000, of which $30,000 has already been spent or committed this year prior to this incident.""

---

**Exam Tasks:**

**Task 1: Monthly Operational Budget Preparation (30 minutes)**

Based on the information in **Section A** of `plant_data.pdf`:
1.  Identify each item as either Operational Expenditure (OpEx) or Capital Expenditure (CapEx).
2.  Provide a brief justification for your OpEx/CapEx classification for each item.
3.  Calculate the total OpEx for the month.
4.  Calculate the total CapEx for the month.
5.  Calculate the total budget request for the month.

**Task 2: Variance Analysis (30 minutes)**

Based on the information in **Section B** of `plant_data.pdf`:
1.  For each maintenance category, calculate the variance (Actual - Budgeted).
2.  For each maintenance category, calculate the variance percentage (((Actual - Budgeted) / Budgeted) * 100), rounded to two decimal places.
3.  For any variance exceeding +/- 10%, provide a brief, plausible explanation for the deviation. If the variance is within +/-10%, state ""Within acceptable range.""
4.  Provide a brief overall comment on the maintenance budget performance for the previous quarter.

**Task 3: Budget Adjustment Proposal (30 minutes)**

Based on the information in **Section C** of `plant_data.pdf`:
1.  Identify the affected budget category (""Unforeseen Major Equipment Repairs"").
2.  State the original annual budget for this category.
3.  State the amount already spent/committed from this category *before* this new incident.
4.  Calculate the remaining budget in this category *before* accounting for the new pump repair.
5.  Determine if the remaining budget is sufficient for the estimated pump repair cost.
6.  Propose the necessary budget adjustment (i.e., how much additional funding, if any, is needed for this repair beyond the current category's remaining balance).
7.  Provide a brief justification for this adjustment request, highlighting the necessity of the repair.

---

**JSON Submission Structure (`test_submission.json`):**

```json
{
  ""candidate_id"": ""YOUR_CANDIDATE_ID_HERE"", // Replace with your assigned ID
  ""exam_version"": ""Basic_Budget_V1"",
  ""submission_timestamp"": ""YYYY-MM-DDTHH:MM:SSZ"", // ISO 8601 format
  ""tasks"": {
    ""task_1_monthly_budget"": {
      ""cost_items"": [
        {
          ""item_description"": ""Routine Wellhead Maintenance"",
          ""estimated_cost_usd"": 7500,
          ""category"": ""OpEx_or_CapEx"", // ""OpEx"" or ""CapEx""
          ""justification"": ""Brief reason for categorization""
        }
        // ... Add all items from Section A here
      ],
      ""total_opex_usd"": 0, // Calculated total
      ""total_capex_usd"": 0, // Calculated total
      ""total_monthly_budget_request_usd"": 0 // Calculated total
    },
    ""task_2_variance_analysis"": {
      ""variances"": [
        {
          ""maintenance_category"": ""Well Maintenance"",
          ""budgeted_usd"": 20000,
          ""actual_usd"": 23500,
          ""variance_usd"": 0, // Calculated (Actual - Budgeted)
          ""variance_percentage"": 0.00, // Calculated, e.g., 17.50 for 17.50%
          ""explanation"": ""Plausible explanation or 'Within acceptable range.'""
        }
        // ... Add all items from Section B here
      ],
      ""overall_performance_comment"": ""Brief overall comment.""
    },
    ""task_3_budget_adjustment"": {
      ""affected_budget_category"": ""Unforeseen Major Equipment Repairs"",
      ""original_annual_budget_usd"": 75000,
      ""spent_committed_prior_usd"": 30000,
      ""remaining_budget_before_incident_usd"": 0, // Calculated
      ""estimated_repair_cost_usd"": 28000,
      ""is_remaining_budget_sufficient"": false, // true or false
      ""additional_funding_needed_usd"": 0, // Calculated, 0 if sufficient
      ""justification_for_adjustment"": ""Brief justification for the request.""
    }
  }
}
```

**End of Exam Instructions.**","Okay, here are the clear, concise instructions for the candidate for the Basic Exam.

---

## Geothermal Production Manager - Basic Budgeting Skills Assessment

**Instructions for the Candidate:**

Welcome to the Basic Budgeting Skills Assessment for Geothermal Production Managers.

**Objective:** This exam will test your practical ability to perform fundamental budgeting tasks related to geothermal operations. You will work with provided data to categorize costs, analyze budget variances, and assess the impact of unforeseen expenditures.

**Duration:** 90 minutes.

**Tools:** You may use spreadsheet software (e.g., Excel, Google Sheets, LibreOffice Calc), a text editor, and a PDF viewer. Internet access should only be used if necessary to look up basic definitions (e.g., OpEx/CapEx) but not for finding solutions to the tasks.

**Scenario Context:**
You are the Assistant Production Manager for ""TerraFlow Unit 1,"" a small, established geothermal power plant. You are assisting the Production Manager with various budgeting tasks.

**Provided Materials:**

You will be given one digital file:

1.  `plant_data.pdf`: This document contains three sections:
    *   **Section A:** A list of anticipated operational activities and associated costs for TerraFlow Unit 1 for the upcoming month.
    *   **Section B:** A summary table of Budgeted vs. Actual expenditures for selected maintenance categories from the previous quarter.
    *   **Section C:** A brief note regarding an unexpected equipment issue.

*(The content of `plant_data.pdf` is detailed below for your reference during this exam design. In the actual exam, it would be a separate file.)*

---

**Simulated Content of `plant_data.pdf`:**

**Section A: Anticipated Monthly Costs for TerraFlow Unit 1**

| Item Description                      | Estimated Cost (USD) | Notes                                     |
| :------------------------------------ | :------------------- | :---------------------------------------- |
| Routine Wellhead Maintenance          | 7,500                | Scheduled inspection and minor repairs    |
| Operator Salaries & Benefits          | 35,000               | Monthly payroll for operations staff      |
| Scale Inhibitor Chemical Purchase     | 12,000               | Standard monthly supply                   |
| Electricity for Plant Auxiliaries     | 18,000               | Powering pumps, cooling tower fans, etc.  |
| Replacement of Damaged Flow Meter     | 6,000                | New meter, expected lifespan 7 years      |
| Office Supplies & Admin Support       | 1,500                | General administrative costs              |
| Safety Training for New Hires         | 2,500                | One-time training session                 |
| Minor Repairs to Surface Pipelines    | 4,000                | Welding, gasket replacements              |
| Purchase of New Laptop for Supervisor | 1,200                | Asset with >1 year lifespan             |

**Section B: Previous Quarter Maintenance Budget vs. Actuals**

| Maintenance Category        | Budgeted (USD) | Actual (USD) |
| :-------------------------- | :------------- | :----------- |
| Well Maintenance            | 20,000         | 23,500       |
| Turbine & Generator Maint.  | 15,000         | 14,000       |
| Surface Equipment Maint.    | 10,000         | 12,500       |
| Predictive Maintenance Tools| 5,000          | 4,500        |

**Section C: Urgent Equipment Issue Note**

""Memo: Production Manager
Subject: Urgent Pump Repair

The primary brine injection pump (Pump P-101A) has experienced a critical bearing failure. Immediate repair is necessary to avoid plant shutdown. The initial estimate for parts and specialized labor is $28,000. This was an unforeseen failure. The current annual budget allocation for 'Unforeseen Major Equipment Repairs' is $75,000, of which $30,000 has already been spent or committed this year prior to this incident.""

---

**Exam Tasks:**

**Task 1: Monthly Budget Preparation (Estimated time: 25 minutes)**

Using the data from **Section A** of `plant_data.pdf` (""Anticipated Monthly Costs for TerraFlow Unit 1""):
1.  For each item listed, determine if it is an Operational Expenditure (OpEx) or a Capital Expenditure (CapEx).
2.  Calculate the total OpEx for the month.
3.  Calculate the total CapEx for the month.
4.  Calculate the total combined budget request for the month (Total OpEx + Total CapEx).

    *Your answers for this task will be entered into the `task_1_monthly_budget` section of the JSON submission file.*

**Task 2: Variance Analysis (Estimated time: 25 minutes)**

Using the data from **Section B** of `plant_data.pdf` (""Previous Quarter Maintenance Budget vs. Actuals""):
1.  For each maintenance category:
    a.  Calculate the variance in USD (Actual USD - Budgeted USD).
    b.  Calculate the variance percentage (((Actual USD - Budgeted USD) / Budgeted USD) * 100). Round this percentage to two decimal places.
    c.  Determine the variance status. Use one of the following exact phrases:
        *   ""Overspend"" if the variance percentage is greater than +10%.
        *   ""Underspend"" if the variance percentage is less than -10%.
        *   ""Within Tolerance"" if the variance percentage is between -10% and +10% (inclusive).

    *Your answers for this task will be entered into the `task_2_variance_analysis` section of the JSON submission file.*

**Task 3: Budget Adjustment Assessment (Estimated time: 20 minutes)**

Using the data from **Section C** of `plant_data.pdf` (""Urgent Equipment Issue Note"") and the information provided within that note:
1.  Identify the original annual budget allocated for ""Unforeseen Major Equipment Repairs"".
2.  Identify the amount already spent or committed from this category *before* the new pump repair incident.
3.  Calculate the remaining budget in this category *before* accounting for the new pump repair.
4.  State the estimated cost for the new pump repair.
5.  Determine if the remaining budget (calculated in step 3) is sufficient to cover the estimated pump repair cost. Answer `true` or `false`.
6.  Calculate the additional funding (in USD) that would be needed for this repair if the remaining budget is insufficient. If the budget is sufficient, this value should be 0.
7.  Based on the information in Section C, state the primary consequence if the pump repair is not performed. (Your answer should be a short phrase directly reflecting the information in the note).

    *Your answers for this task will be entered into the `task_3_budget_adjustment` section of the JSON submission file.*

---

**Submission Format:**

You must submit your answers in a single JSON file named `test_submission.json`.
Use the structure provided below. Ensure all numerical values are numbers (e.g., `12000`, not `""12000""`) unless specified (e.g., variance percentage can be a number like `17.50`). Text values must be enclosed in double quotes.

**JSON Submission Structure Template (`test_submission.json`):**

```json
{
  ""candidate_id"": ""YOUR_CANDIDATE_ID_HERE"", // Replace with your assigned ID
  ""exam_version"": ""Basic_Budget_V1"",
  ""submission_timestamp"": ""YYYY-MM-DDTHH:MM:SSZ"", // Fill with submission time in ISO 8601 format
  ""tasks"": {
    ""task_1_monthly_budget"": {
      ""cost_items"": [
        {
          ""item_description"": ""Routine Wellhead Maintenance"", // Match description from PDF
          ""estimated_cost_usd"": 7500,
          ""category"": ""OpEx_or_CapEx"" // Enter ""OpEx"" or ""CapEx""
        },
        {
          ""item_description"": ""Operator Salaries & Benefits"",
          ""estimated_cost_usd"": 35000,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Scale Inhibitor Chemical Purchase"",
          ""estimated_cost_usd"": 12000,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Electricity for Plant Auxiliaries"",
          ""estimated_cost_usd"": 18000,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Replacement of Damaged Flow Meter"",
          ""estimated_cost_usd"": 6000,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Office Supplies & Admin Support"",
          ""estimated_cost_usd"": 1500,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Safety Training for New Hires"",
          ""estimated_cost_usd"": 2500,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Minor Repairs to Surface Pipelines"",
          ""estimated_cost_usd"": 4000,
          ""category"": ""OpEx_or_CapEx""
        },
        {
          ""item_description"": ""Purchase of New Laptop for Supervisor"",
          ""estimated_cost_usd"": 1200,
          ""category"": ""OpEx_or_CapEx""
        }
      ],
      ""total_opex_usd"": 0, // Your calculated total OpEx
      ""total_capex_usd"": 0, // Your calculated total CapEx
      ""total_monthly_budget_request_usd"": 0 // Your calculated total (OpEx + CapEx)
    },
    ""task_2_variance_analysis"": {
      ""variances"": [
        {
          ""maintenance_category"": ""Well Maintenance"", // Match category from PDF
          ""budgeted_usd"": 20000,
          ""actual_usd"": 23500,
          ""variance_usd"": 0, // Your calculated (Actual - Budgeted)
          ""variance_percentage"": 0.00, // Your calculated percentage, e.g., 17.50 for 17.50%
          ""variance_status"": ""Overspend_or_Underspend_or_Within_Tolerance"" // ""Overspend"", ""Underspend"", or ""Within Tolerance""
        },
        {
          ""maintenance_category"": ""Turbine & Generator Maint."",
          ""budgeted_usd"": 15000,
          ""actual_usd"": 14000,
          ""variance_usd"": 0,
          ""variance_percentage"": 0.00,
          ""variance_status"": ""Overspend_or_Underspend_or_Within_Tolerance""
        },
        {
          ""maintenance_category"": ""Surface Equipment Maint."",
          ""budgeted_usd"": 10000,
          ""actual_usd"": 12500,
          ""variance_usd"": 0,
          ""variance_percentage"": 0.00,
          ""variance_status"": ""Overspend_or_Underspend_or_Within_Tolerance""
        },
        {
          ""maintenance_category"": ""Predictive Maintenance Tools"",
          ""budgeted_usd"": 5000,
          ""actual_usd"": 4500,
          ""variance_usd"": 0,
          ""variance_percentage"": 0.00,
          ""variance_status"": ""Overspend_or_Underspend_or_Within_Tolerance""
        }
      ]
    },
    ""task_3_budget_adjustment"": {
      ""affected_budget_category"": ""Unforeseen Major Equipment Repairs"", // Pre-filled
      ""original_annual_budget_usd"": 75000, // Value from Section C
      ""spent_committed_prior_usd"": 30000, // Value from Section C
      ""remaining_budget_before_incident_usd"": 0, // Your calculation
      ""estimated_repair_cost_usd"": 28000, // Value from Section C
      ""is_remaining_budget_sufficient"": false, // Your determination: true or false
      ""additional_funding_needed_usd"": 0, // Your calculation (0 if sufficient)
      ""consequence_if_not_repaired"": ""Primary consequence phrase from Section C"" // e.g., ""Plant shutdown.""
    }
  }
}
```

**End of Exam Instructions.**","<MATERIALS_FOR_CANDIDATE>
The candidate will be provided with a single PDF file named `plant_data.pdf`. The content of this PDF is as follows:

---
**`plant_data.pdf`**

**Document ID:** GPM_BasicBudget_Exam_Data_v1.0

**TerraFlow Geothermal Operations - Budgeting Data**

**Section A: Anticipated Monthly Costs for TerraFlow Unit 1 (Next Month)**

This section lists anticipated costs for the upcoming operational month.

| Item Description                      | Estimated Cost (USD) | Notes                                     |
| :------------------------------------ | :------------------- | :---------------------------------------- |
| Routine Wellhead Maintenance          | 7,500                | Scheduled inspection and minor repairs    |
| Operator Salaries & Benefits          | 35,000               | Monthly payroll for operations staff      |
| Scale Inhibitor Chemical Purchase     | 12,000               | Standard monthly supply                   |
| Electricity for Plant Auxiliaries     | 18,000               | Powering pumps, cooling tower fans, etc.  |
| Replacement of Damaged Flow Meter     | 6,000                | New meter, expected lifespan 7 years      |
| Office Supplies & Admin Support       | 1,500                | General administrative costs              |
| Safety Training for New Hires         | 2,500                | One-time training session                 |
| Minor Repairs to Surface Pipelines    | 4,000                | Welding, gasket replacements              |
| Purchase of New Laptop for Supervisor | 1,200                | Asset with >1 year lifespan             |

**Section B: Previous Quarter Maintenance Budget vs. Actuals**

This section provides a summary of budgeted versus actual expenditures for selected maintenance categories from the last completed quarter.

| Maintenance Category        | Budgeted (USD) | Actual (USD) |
| :-------------------------- | :------------- | :----------- |
| Well Maintenance            | 20,000         | 23,500       |
| Turbine & Generator Maint.  | 15,000         | 14,000       |
| Surface Equipment Maint.    | 10,000         | 12,500       |
| Predictive Maintenance Tools| 5,000          | 4,500        |

**Section C: Urgent Equipment Issue Note**

**MEMORANDUM**

**To:** Production Manager
**From:** Lead Operations Technician
**Date:** [Current Date - 2 Days]
**Subject:** Urgent Pump Repair - Brine Injection Pump P-101A

The primary brine injection pump (Pump P-101A) has experienced a critical bearing failure. Immediate repair is necessary to avoid plant shutdown. The initial estimate for parts and specialized labor is $28,000. This was an unforeseen failure.

*Reference: The current annual budget allocation for 'Unforeseen Major Equipment Repairs' is $75,000. Records show that $30,000 has already been spent or committed from this allocation this year, prior to this incident.*

---
</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**How Materials Were Created:**

The data in `plant_data.pdf` is synthetic and designed to test basic budgeting skills relevant to geothermal operations. The values are chosen to be straightforward for calculation within the time limit and to produce clear outcomes for evaluation.

*   **Section A (Anticipated Monthly Costs):**
    *   Includes a mix of typical operational expenditures (OpEx) and capital expenditures (CapEx) for a geothermal plant.
    *   Costs are whole numbers for easy summation.
    *   ""Notes"" provide clues for OpEx/CapEx categorization (e.g., ""lifespan 7 years"" for CapEx, ""routine"" or ""monthly supply"" for OpEx).

*   **Section B (Previous Quarter Maintenance Budget vs. Actuals):**
    *   Provides simple budget vs. actual data for variance calculation.
    *   Values are chosen to result in variances that are clearly ""Overspend,"" ""Underspend,"" or ""Within Tolerance"" based on the specified +/- 10% threshold.

*   **Section C (Urgent Equipment Issue Note):**
    *   Presents a common scenario of an unforeseen critical repair.
    *   Provides all necessary figures (original budget, amount spent, repair cost) to assess budget sufficiency.
    *   Clearly states the consequence of not performing the repair (""avoid plant shutdown"").

**Cross-References and Connections:**
*   Each section is self-contained for its respective task. There are no hidden cross-references *between* sections that are critical for solving a specific task, beyond using the correct section for the correct task.

**Correct Answers/Expected Outcomes:**

**Task 1: Monthly Budget Preparation**
*   **Cost Item Categorization:**
    1.  `Routine Wellhead Maintenance`: **OpEx** (Ongoing, repairs)
    2.  `Operator Salaries & Benefits`: **OpEx** (Ongoing, operational staff costs)
    3.  `Scale Inhibitor Chemical Purchase`: **OpEx** (Consumable, routine)
    4.  `Electricity for Plant Auxiliaries`: **OpEx** (Utility, operational)
    5.  `Replacement of Damaged Flow Meter`: **CapEx** (New asset, lifespan > 1 year)
    6.  `Office Supplies & Admin Support`: **OpEx** (General admin, consumable)
    7.  `Safety Training for New Hires`: **OpEx** (Service, not a tangible long-term asset for basic accounting)
    8.  `Minor Repairs to Surface Pipelines`: **OpEx** (Repairs, not betterment)
    9.  `Purchase of New Laptop for Supervisor`: **CapEx** (New asset, lifespan > 1 year)
*   **Calculations:**
    *   `total_opex_usd`: 7500 + 35000 + 12000 + 18000 + 1500 + 2500 + 4000 = **80,500**
    *   `total_capex_usd`: 6000 + 1200 = **7,200**
    *   `total_monthly_budget_request_usd`: 80500 + 7200 = **87,700**

**Task 2: Variance Analysis**
1.  **Well Maintenance:**
    *   `variance_usd`: 23500 - 20000 = **3,500**
    *   `variance_percentage`: (3500 / 20000) * 100 = **17.50**%
    *   `variance_status`: **""Overspend""** (since 17.50% > 10%)
2.  **Turbine & Generator Maint.:**
    *   `variance_usd`: 14000 - 15000 = **-1,000**
    *   `variance_percentage`: (-1000 / 15000) * 100 = **-6.67**% (rounded from -6.666...)
    *   `variance_status`: **""Within Tolerance""** (since -10% <= -6.67% <= 10%)
3.  **Surface Equipment Maint.:**
    *   `variance_usd`: 12500 - 10000 = **2,500**
    *   `variance_percentage`: (2500 / 10000) * 100 = **25.00**%
    *   `variance_status`: **""Overspend""** (since 25.00% > 10%)
4.  **Predictive Maintenance Tools:**
    *   `variance_usd`: 4500 - 5000 = **-500**
    *   `variance_percentage`: (-500 / 5000) * 100 = **-10.00**%
    *   `variance_status`: **""Within Tolerance""** (since -10% is inclusive for ""Within Tolerance"")

**Task 3: Budget Adjustment Assessment**
1.  `original_annual_budget_usd`: **75,000** (from Section C)
2.  `spent_committed_prior_usd`: **30,000** (from Section C)
3.  `remaining_budget_before_incident_usd`: 75000 - 30000 = **45,000**
4.  `estimated_repair_cost_usd`: **28,000** (from Section C)
5.  `is_remaining_budget_sufficient`: **true** (since 45,000 >= 28,000)
6.  `additional_funding_needed_usd`: **0** (since budget is sufficient)
7.  `consequence_if_not_repaired`: **""Plant shutdown""** (or a very close paraphrase like ""Avoid plant shutdown"" implies not doing it leads to shutdown). The most direct answer is ""Plant shutdown.""

**Tricky Elements or Common Pitfalls:**

*   **Task 1 - OpEx/CapEx:**
    *   ""Safety Training for New Hires"": Some might be tempted to call this CapEx (investment in human capital), but in standard operational budgeting, training is typically OpEx. The exam is ""basic,"" so the standard interpretation is expected.
    *   ""Replacement of Damaged Flow Meter"" vs. ""Minor Repairs to Surface Pipelines"": The key is ""replacement"" of an asset with a defined lifespan (CapEx) versus ""minor repairs"" (OpEx).
*   **Task 2 - Variance Percentage and Status:**
    *   Ensuring correct sign for variance (positive for overspend, negative for underspend).
    *   Correctly applying the +/- 10% threshold, especially the inclusive nature for ""Within Tolerance"" (e.g., exactly -10% is ""Within Tolerance"").
    *   Rounding the percentage to two decimal places as instructed.
*   **Task 3 - Budget Sufficiency:**
    *   Carefully reading to extract the correct figures from the memo.
    *   Correctly calculating the remaining budget *before* the new incident.
    *   If the budget were insufficient, correctly calculating the shortfall. In this case, it is sufficient, so the shortfall is 0.

**""Hidden"" Information:**
*   There is no intentionally ""hidden"" information. All data required is explicitly provided. The main challenge is careful reading, correct interpretation of standard budgeting terms (OpEx/CapEx), and accurate calculation.
*   The ""Notes"" column in Section A is crucial for Task 1 categorization.
*   The specific wording ""avoid plant shutdown"" in Section C directly points to the consequence if the repair is not made.

This design ensures that candidates are tested on practical application using provided data and structures, minimizing ambiguity and facilitating straightforward evaluation.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
The candidate will be provided with a single PDF file named `plant_data.pdf`. The content of this PDF is as follows:

---
**`plant_data.pdf`**

**Document ID:** GPM_BasicBudget_Exam_Data_v1.0

**TerraFlow Geothermal Operations - Budgeting Data**

**Section A: Anticipated Monthly Costs for TerraFlow Unit 1 (Next Month)**

This section lists anticipated costs for the upcoming operational month.

| Item Description                      | Estimated Cost (USD) | Notes                                     |
| :------------------------------------ | :------------------- | :---------------------------------------- |
| Routine Wellhead Maintenance          | 7,500                | Scheduled inspection and minor repairs    |
| Operator Salaries & Benefits          | 35,000               | Monthly payroll for operations staff      |
| Scale Inhibitor Chemical Purchase     | 12,000               | Standard monthly supply                   |
| Electricity for Plant Auxiliaries     | 18,000               | Powering pumps, cooling tower fans, etc.  |
| Replacement of Damaged Flow Meter     | 6,000                | New meter, expected lifespan 7 years      |
| Office Supplies & Admin Support       | 1,500                | General administrative costs              |
| Safety Training for New Hires         | 2,500                | One-time training session                 |
| Minor Repairs to Surface Pipelines    | 4,000                | Welding, gasket replacements              |
| Purchase of New Laptop for Supervisor | 1,200                | Asset with >1 year lifespan             |

**Section B: Previous Quarter Maintenance Budget vs. Actuals**

This section provides a summary of budgeted versus actual expenditures for selected maintenance categories from the last completed quarter.

| Maintenance Category        | Budgeted (USD) | Actual (USD) |
| :-------------------------- | :------------- | :----------- |
| Well Maintenance            | 20,000         | 23,500       |
| Turbine & Generator Maint.  | 15,000         | 14,000       |
| Surface Equipment Maint.    | 10,000         | 12,500       |
| Predictive Maintenance Tools| 5,000          | 4,500        |

**Section C: Urgent Equipment Issue Note**

**MEMORANDUM**

**To:** Production Manager
**From:** Lead Operations Technician
**Date:** [Current Date - 2 Days]
**Subject:** Urgent Pump Repair - Brine Injection Pump P-101A

The primary brine injection pump (Pump P-101A) has experienced a critical bearing failure. Immediate repair is necessary to avoid plant shutdown. The initial estimate for parts and specialized labor is $28,000. This was an unforeseen failure.

*Reference: The current annual budget allocation for 'Unforeseen Major Equipment Repairs' is $75,000. Records show that $30,000 has already been spent or committed from this allocation this year, prior to this incident.*

---
",,,,,[],False,True,0.0,99.0,0.0,0.0,True,,"{'overview': prompt_token_count: 237
candidates_token_count: 2492
total_token_count: 4648
, 'instructions': prompt_token_count: 2935
candidates_token_count: 2952
total_token_count: 7631
, 'materials': prompt_token_count: 6093
candidates_token_count: 2547
total_token_count: 10310
, 'check_materials': prompt_token_count: 5603
candidates_token_count: 1
total_token_count: 8476
}",
Sales Managers,4.0,Determine price schedules and discount rates.,gemini-2.5-pro-preview-03-25,"['Spreadsheets', 'PDF viewer']","['Text', 'Data']","Okay, here are the candidate instructions for the Basic Practical Exam on Determining Price Schedules and Discount Rates.

---

**Sales Manager Capabilities Exam: Basic Pricing & Discounts Application**

**Objective:**
This practical exam assesses your ability to apply predefined pricing rules and discount structures to calculate base prices and final selling prices using standard business tools. You will work with provided product data and pricing policies.

**Time Limit:**
You have a maximum of **90 minutes** to complete this exam and submit your results.

**Materials Provided:**
You will be provided with the following digital files:

1.  **`product_data.xlsx`**: A spreadsheet file containing product information. This includes columns such as `ProductID`, `ProductName`, and `UnitCost`.
2.  **`pricing_policy_basic.pdf`**: A PDF document outlining the company's basic pricing policy. This document details:
    *   The specific rule for calculating the standard `BasePrice` from the `UnitCost`.
    *   A table defining volume-based discounts (e.g., discount percentages based on quantity tiers).
    *   A list defining customer-type-based discounts (e.g., discount percentages for specific customer types like ""Reseller"" or ""Education"").
    *   **Important:** The policy explicitly states that for this basic exercise, volume discounts and customer type discounts are applied independently to the `BasePrice` and are **not** cumulative.

**Tools Required:**
*   Spreadsheet software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A PDF viewer

**Your Tasks:**

You are required to perform the following calculations based on the information in the provided files. Use your spreadsheet software for calculations.

1.  **Calculate Base Prices:**
    *   For **each** product listed in `product_data.xlsx`, calculate its `BasePrice`.
    *   Use the `UnitCost` from the spreadsheet and the base price calculation rule specified in `pricing_policy_basic.pdf`.

2.  **Calculate Final Prices with Volume Discounts:**
    *   Calculate the final **unit price** for the specific hypothetical order scenarios listed below.
    *   For these scenarios, apply **only** the relevant *volume discount* (based on the quantity) to the calculated `BasePrice`. Refer to the volume discount table in `pricing_policy_basic.pdf`.
    *   **Scenarios:**
        *   **Order V1:** Product `P002`, Quantity `25`
        *   **Order V2:** Product `P005`, Quantity `60`
        *   **Order V3:** Product `P008`, Quantity `5`

3.  **Calculate Final Prices with Customer Type Discounts:**
    *   Calculate the final **unit price** for the specific hypothetical order scenarios listed below.
    *   For these scenarios, apply **only** the relevant *customer type discount* to the calculated `BasePrice`. Refer to the customer type discount list in `pricing_policy_basic.pdf`. Assume a quantity of 1 for these scenarios unless otherwise implied by the need to calculate a unit price.
    *   **Scenarios:**
        *   **Order C1:** Product `P001`, Customer Type `Reseller`
        *   **Order C2:** Product `P004`, Customer Type `Education`
        *   **Order C3:** Product `P007`, Customer Type `Standard` (Note: Check the policy for how 'Standard' or non-listed types are handled).

**Important Calculation Note:**
*   Round all calculated monetary values (Base Prices and Final Unit Prices) to **2 decimal places**.

**Submission Format:**

*   You must submit your answers in a single JSON file named `test_submission.json`.
*   The JSON file must follow the exact structure specified below. Ensure all keys and data types match the example.
*   Use the exact `ProductID` values as provided in `product_data.xlsx`.
*   Use the predefined `order_id` values (`V1`, `V2`, `V3`, `C1`, `C2`, `C3`) as specified in the tasks.
*   Use the exact `customer_type` strings (`Reseller`, `Education`, `Standard`) as specified in the tasks.

**Required JSON Structure:**

```json
{
  ""base_prices"": [
    {
      ""product_id"": ""P001"", // Example Product ID
      ""base_price"": 150.00  // Example Calculated Base Price (Number, 2 decimal places)
    },
    {
      ""product_id"": ""P002"",
      ""base_price"": 225.50
    }
    // ... include an object for ALL products from product_data.xlsx
  ],
  ""volume_discount_scenarios"": [
    {
      ""order_id"": ""V1"",
      ""product_id"": ""P002"",
      ""quantity"": 25, // Number
      ""final_unit_price_volume_discount"": 214.23 // Example Calculated Price (Number, 2 decimal places)
    },
    {
      ""order_id"": ""V2"",
      ""product_id"": ""P005"",
      ""quantity"": 60,
      ""final_unit_price_volume_discount"": 450.80 // Example
    },
    {
      ""order_id"": ""V3"",
      ""product_id"": ""P008"",
      ""quantity"": 5,
      ""final_unit_price_volume_discount"": 180.00 // Example (might have no discount)
    }
  ],
  ""customer_discount_scenarios"": [
    {
      ""order_id"": ""C1"",
      ""product_id"": ""P001"",
      ""customer_type"": ""Reseller"", // String
      ""final_unit_price_customer_discount"": 135.00 // Example Calculated Price (Number, 2 decimal places)
    },
    {
      ""order_id"": ""C2"",
      ""product_id"": ""P004"",
      ""customer_type"": ""Education"",
      ""final_unit_price_customer_discount"": 310.55 // Example
    },
    {
      ""order_id"": ""C3"",
      ""product_id"": ""P007"",
      ""customer_type"": ""Standard"",
      ""final_unit_price_customer_discount"": 95.00 // Example (might have no discount)
    }
  ]
}
```

**Final Steps:**
*   Double-check all your calculations.
*   Ensure your JSON file is correctly formatted, named `test_submission.json`, and contains all the required information.
*   Submit your `test_submission.json` file before the 90-minute time limit expires.

Good luck!
---
**1. Product Data (`product_data.csv`)**

```csv
ProductID,ProductName,UnitCost
P001,Alpha Widget,100.00
P002,Beta Gear,150.33
P003,Gamma Sprocket,75.50
P004,Delta Hub,220.00
P005,Epsilon Flange,334.67
P006,Zeta Pinion,50.00
P007,Eta Coupling,63.33
P008,Theta Valve,120.00
P009,Iota Sensor,450.80
P010,Kappa Actuator,88.25
```

**2. Basic Pricing Policy (`pricing_policy_basic.txt`)**

```text
**Company Pricing Policy - Basic Rules**

**Document Version:** 1.0
**Effective Date:** [Current Year]-01-01

**1. Base Price Calculation:**
   The standard Base Price for all products is calculated by applying a fixed margin to the Unit Cost.
   - **Rule:** Base Price = Unit Cost * 1.50
   - All calculated Base Prices must be rounded to 2 decimal places.

**2. Volume Discounts:**
   Volume discounts are offered based on the quantity of a single product line item within an order.
   - **Discount Tiers:**
     - 1 to 9 units: 0% discount
     - 10 to 49 units: 5% discount
     - 50 or more units: 10% discount
   - The volume discount percentage is applied to the calculated Base Price.
   - Final prices after volume discount must be rounded to 2 decimal places.

**3. Customer Type Discounts:**
   Specific customer types receive a standard discount on all products.
   - **Discount List:**
     - Customer Type ""Reseller"": 15% discount
     - Customer Type ""Education"": 10% discount
     - Customer Type ""Standard"": 0% discount
   - Any customer type not explicitly listed above is considered ""Standard"" and receives a 0% discount.
   - The customer type discount percentage is applied to the calculated Base Price.
   - Final prices after customer type discount must be rounded to 2 decimal places.

**4. IMPORTANT - Discount Application:**
   For the purpose of this basic exercise, Volume Discounts and Customer Type Discounts are applied **independently** to the Base Price. They are **NOT** cumulative.
   - When calculating a price for a volume discount scenario, apply ONLY the volume discount.
   - When calculating a price for a customer type discount scenario, apply ONLY the customer type discount.

**5. Currency:**
   All monetary values (Unit Cost, Base Price, Final Prices) are expressed in USD.
```
Okay, here is the specification for the candidate's submission format for the Basic Practical Exam on Determining Price Schedules and Discount Rates.

**Submission File:**
*   You must submit a single file named `test_submission.json`.

**Candidate Identification:**
*   Include your candidate ID at the top level of the JSON object using the key `candidate_id`. Replace `""YOUR_ID_HERE""` with the specific model version you are using (e.g., ""GPT-4o"", ""Claude-3-Opus"", etc.).

**JSON Structure and Data Format:**
*   The file MUST be a valid JSON object.
*   Follow the exact structure, key names, and data types specified below.
*   **Monetary Values:** All price fields (`base_price`, `final_unit_price_volume_discount`, `final_unit_price_customer_discount`) MUST be represented as **Numbers** (not strings) and rounded to exactly **2 decimal places**.
*   **Product IDs:** Use the exact `product_id` strings as provided in `product_data.csv` (e.g., ""P001"", ""P002""). These MUST be **Strings**.
*   **Order IDs:** Use the exact `order_id` strings specified in the instructions (""V1"", ""V2"", ""V3"", ""C1"", ""C2"", ""C3""). These MUST be **Strings**.
*   **Quantity:** The `quantity` field MUST be an **Integer Number**.
*   **Customer Types:** Use the exact `customer_type` strings specified in the instructions (""Reseller"", ""Education"", ""Standard""). These MUST be **Strings**.

**Required JSON Format:**

```json
{
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace YOUR_ID_HERE with your specific model version ID (String)
  ""base_prices"": [
    {
      ""product_id"": ""P001"", // String: Product ID from product_data.csv
      ""base_price"": 0.00     // Number: Calculated Base Price, rounded to 2 decimal places
    },
    {
      ""product_id"": ""P002"",
      ""base_price"": 0.00
    },
    // ... MUST include one object for EACH product listed in product_data.csv (P001 through P010)
    {
      ""product_id"": ""P010"",
      ""base_price"": 0.00
    }
  ],
  ""volume_discount_scenarios"": [
    {
      ""order_id"": ""V1"",          // String: Specific scenario ID from instructions
      ""product_id"": ""P002"",      // String: Product ID for this scenario
      ""quantity"": 0,             // Number (Integer): Quantity for this scenario
      ""final_unit_price_volume_discount"": 0.00 // Number: Calculated final unit price with volume discount, rounded to 2 decimal places
    },
    {
      ""order_id"": ""V2"",
      ""product_id"": ""P005"",
      ""quantity"": 0,
      ""final_unit_price_volume_discount"": 0.00
    },
    {
      ""order_id"": ""V3"",
      ""product_id"": ""P008"",
      ""quantity"": 0,
      ""final_unit_price_volume_discount"": 0.00
    }
    // MUST include exactly these three volume scenario objects (V1, V2, V3)
  ],
  ""customer_discount_scenarios"": [
    {
      ""order_id"": ""C1"",          // String: Specific scenario ID from instructions
      ""product_id"": ""P001"",      // String: Product ID for this scenario
      ""customer_type"": ""Reseller"", // String: Customer type for this scenario
      ""final_unit_price_customer_discount"": 0.00 // Number: Calculated final unit price with customer discount, rounded to 2 decimal places
    },
    {
      ""order_id"": ""C2"",
      ""product_id"": ""P004"",
      ""customer_type"": ""Education"",
      ""final_unit_price_customer_discount"": 0.00
    },
    {
      ""order_id"": ""C3"",
      ""product_id"": ""P007"",
      ""customer_type"": ""Standard"",
      ""final_unit_price_customer_discount"": 0.00
    }
    // MUST include exactly these three customer scenario objects (C1, C2, C3)
  ]
}
```

**Important Notes:**
*   Ensure the `base_prices` array contains an entry for **all 10 products** from the `product_data.csv` file, in any order.
*   Ensure the `volume_discount_scenarios` and `customer_discount_scenarios` arrays contain exactly the three specified scenarios each, using the correct `order_id`, `product_id`, `quantity` (where applicable), and `customer_type`.
*   Failure to adhere strictly to this format (file name, JSON structure, key names, data types, rounding) may result in evaluation errors. Do not include any extra keys or comments within the JSON file.","You are an excellent examiner of Sales Managers capabilities. Design a remote, **practical** exam to verify whether a Sales Managers can Determine price schedules and discount rates..
    This exam will have two parts (basic and advanced). Your current task is **only** to design the basic exam.

    ### Context
    - The candidate has access to a computer with the following tools: Spreadsheets and PDF viewer
    - The candidate can also be given digital materials such as Text or Data that must be used for the test.
    - Design a **practical** exam that can be completed remotely using only these tools. A practical exam is an exam actually testing whether the described task can be performed successfully. An exam testing knowledge about the task is NOT a practical exam.
    - To simplify evaluation, the candidate should submit answers in a structured JSON format. Name the file ""test_submission.json"".
    - The candidate should be able to complete the exam in maximum 90 minutes.
    ","Okay, here is the explanation of the Basic Exam for the evaluator.

**Exam Explanation for Evaluator**

**Purpose:**
This basic practical exam aims to assess a Sales Manager candidate's fundamental ability to apply predefined rules to calculate product base prices and determine final prices after applying simple, non-cumulative discount structures (volume-based and customer-type-based). It verifies their practical skill in using standard spreadsheet software to perform these calculations based on provided data and policy documents, and their ability to structure the output correctly.

**Structure:**
1.  **Input Materials:** The candidate will receive:
    *   A spreadsheet file (`product_data.xlsx` or `.csv`) containing product information (e.g., Product ID, Product Name, Unit Cost).
    *   A PDF document (`pricing_policy_basic.pdf`) outlining the company's basic pricing policy. This includes:
        *   The rule for calculating the standard Base Price (e.g., Cost + Fixed Percentage Margin).
        *   A simple volume discount table (e.g., Tier 1: 10-49 units = X% discount; Tier 2: 50+ units = Y% discount).
        *   A simple customer type discount list (e.g., Reseller = Z% discount; Education = W% discount).
        *   *Crucially, the policy will state that for this basic exercise, volume and customer type discounts are applied independently to the Base Price and are NOT cumulative.*
2.  **Tasks:** The candidate will be instructed to:
    *   Calculate the Base Price for each product listed in the spreadsheet using the cost data and the margin rule from the policy document.
    *   Calculate the final unit price for a few specific hypothetical orders, applying the relevant *volume* discount from the policy document to the calculated Base Price.
    *   Calculate the final unit price for a few different hypothetical orders, applying the relevant *customer type* discount from the policy document to the calculated Base Price.
    *   Compile these calculated prices into a structured JSON format.
3.  **Tools:** The candidate will use spreadsheet software (like Excel or Google Sheets) for calculations and a PDF viewer to read the policy document.
4.  **Output:** The candidate must submit a single JSON file named `test_submission.json` containing the results of their calculations, structured according to predefined keys (which will be specified in the exam instructions for the candidate).
5.  **Time Limit:** 90 minutes.

**Evaluation Focus:**
The evaluation will focus on:
*   Accuracy of the calculated Base Prices based on the cost and margin rule.
*   Accuracy of applying the correct volume discount tiers and calculating the final prices.
*   Accuracy of applying the correct customer type discounts and calculating the final prices.
*   Correct formatting and structuring of the output in the specified JSON format.
*   Completion within the time limit.

This exam specifically tests the *application* of given rules in a practical scenario, not the candidate's ability to *devise* pricing strategies (which would be part of an advanced exam).","Okay, here are the candidate instructions for the Basic Practical Exam on Determining Price Schedules and Discount Rates.

---

**Sales Manager Capabilities Exam: Basic Pricing & Discounts Application**

**Objective:**
This practical exam assesses your ability to apply predefined pricing rules and discount structures to calculate base prices and final selling prices using standard business tools. You will work with provided product data and pricing policies.

**Time Limit:**
You have a maximum of **90 minutes** to complete this exam and submit your results.

**Materials Provided:**
You will be provided with the following digital files:

1.  **`product_data.xlsx`**: A spreadsheet file containing product information. This includes columns such as `ProductID`, `ProductName`, and `UnitCost`.
2.  **`pricing_policy_basic.pdf`**: A PDF document outlining the company's basic pricing policy. This document details:
    *   The specific rule for calculating the standard `BasePrice` from the `UnitCost`.
    *   A table defining volume-based discounts (e.g., discount percentages based on quantity tiers).
    *   A list defining customer-type-based discounts (e.g., discount percentages for specific customer types like ""Reseller"" or ""Education"").
    *   **Important:** The policy explicitly states that for this basic exercise, volume discounts and customer type discounts are applied independently to the `BasePrice` and are **not** cumulative.

**Tools Required:**
*   Spreadsheet software (e.g., Microsoft Excel, Google Sheets, LibreOffice Calc)
*   A PDF viewer

**Your Tasks:**

You are required to perform the following calculations based on the information in the provided files. Use your spreadsheet software for calculations.

1.  **Calculate Base Prices:**
    *   For **each** product listed in `product_data.xlsx`, calculate its `BasePrice`.
    *   Use the `UnitCost` from the spreadsheet and the base price calculation rule specified in `pricing_policy_basic.pdf`.

2.  **Calculate Final Prices with Volume Discounts:**
    *   Calculate the final **unit price** for the specific hypothetical order scenarios listed below.
    *   For these scenarios, apply **only** the relevant *volume discount* (based on the quantity) to the calculated `BasePrice`. Refer to the volume discount table in `pricing_policy_basic.pdf`.
    *   **Scenarios:**
        *   **Order V1:** Product `P002`, Quantity `25`
        *   **Order V2:** Product `P005`, Quantity `60`
        *   **Order V3:** Product `P008`, Quantity `5`

3.  **Calculate Final Prices with Customer Type Discounts:**
    *   Calculate the final **unit price** for the specific hypothetical order scenarios listed below.
    *   For these scenarios, apply **only** the relevant *customer type discount* to the calculated `BasePrice`. Refer to the customer type discount list in `pricing_policy_basic.pdf`. Assume a quantity of 1 for these scenarios unless otherwise implied by the need to calculate a unit price.
    *   **Scenarios:**
        *   **Order C1:** Product `P001`, Customer Type `Reseller`
        *   **Order C2:** Product `P004`, Customer Type `Education`
        *   **Order C3:** Product `P007`, Customer Type `Standard` (Note: Check the policy for how 'Standard' or non-listed types are handled).

**Important Calculation Note:**
*   Round all calculated monetary values (Base Prices and Final Unit Prices) to **2 decimal places**.

**Submission Format:**

*   You must submit your answers in a single JSON file named `test_submission.json`.
*   The JSON file must follow the exact structure specified below. Ensure all keys and data types match the example.
*   Use the exact `ProductID` values as provided in `product_data.xlsx`.
*   Use the predefined `order_id` values (`V1`, `V2`, `V3`, `C1`, `C2`, `C3`) as specified in the tasks.
*   Use the exact `customer_type` strings (`Reseller`, `Education`, `Standard`) as specified in the tasks.

**Required JSON Structure:**

```json
{
  ""base_prices"": [
    {
      ""product_id"": ""P001"", // Example Product ID
      ""base_price"": 150.00  // Example Calculated Base Price (Number, 2 decimal places)
    },
    {
      ""product_id"": ""P002"",
      ""base_price"": 225.50
    }
    // ... include an object for ALL products from product_data.xlsx
  ],
  ""volume_discount_scenarios"": [
    {
      ""order_id"": ""V1"",
      ""product_id"": ""P002"",
      ""quantity"": 25, // Number
      ""final_unit_price_volume_discount"": 214.23 // Example Calculated Price (Number, 2 decimal places)
    },
    {
      ""order_id"": ""V2"",
      ""product_id"": ""P005"",
      ""quantity"": 60,
      ""final_unit_price_volume_discount"": 450.80 // Example
    },
    {
      ""order_id"": ""V3"",
      ""product_id"": ""P008"",
      ""quantity"": 5,
      ""final_unit_price_volume_discount"": 180.00 // Example (might have no discount)
    }
  ],
  ""customer_discount_scenarios"": [
    {
      ""order_id"": ""C1"",
      ""product_id"": ""P001"",
      ""customer_type"": ""Reseller"", // String
      ""final_unit_price_customer_discount"": 135.00 // Example Calculated Price (Number, 2 decimal places)
    },
    {
      ""order_id"": ""C2"",
      ""product_id"": ""P004"",
      ""customer_type"": ""Education"",
      ""final_unit_price_customer_discount"": 310.55 // Example
    },
    {
      ""order_id"": ""C3"",
      ""product_id"": ""P007"",
      ""customer_type"": ""Standard"",
      ""final_unit_price_customer_discount"": 95.00 // Example (might have no discount)
    }
  ]
}
```

**Final Steps:**
*   Double-check all your calculations.
*   Ensure your JSON file is correctly formatted, named `test_submission.json`, and contains all the required information.
*   Submit your `test_submission.json` file before the 90-minute time limit expires.

Good luck!
---","<MATERIALS_FOR_CANDIDATE>
**1. Product Data (`product_data.csv`)**

```csv
ProductID,ProductName,UnitCost
P001,Alpha Widget,100.00
P002,Beta Gear,150.33
P003,Gamma Sprocket,75.50
P004,Delta Hub,220.00
P005,Epsilon Flange,334.67
P006,Zeta Pinion,50.00
P007,Eta Coupling,63.33
P008,Theta Valve,120.00
P009,Iota Sensor,450.80
P010,Kappa Actuator,88.25
```

**2. Basic Pricing Policy (`pricing_policy_basic.txt`)**

```text
**Company Pricing Policy - Basic Rules**

**Document Version:** 1.0
**Effective Date:** [Current Year]-01-01

**1. Base Price Calculation:**
   The standard Base Price for all products is calculated by applying a fixed margin to the Unit Cost.
   - **Rule:** Base Price = Unit Cost * 1.50
   - All calculated Base Prices must be rounded to 2 decimal places.

**2. Volume Discounts:**
   Volume discounts are offered based on the quantity of a single product line item within an order.
   - **Discount Tiers:**
     - 1 to 9 units: 0% discount
     - 10 to 49 units: 5% discount
     - 50 or more units: 10% discount
   - The volume discount percentage is applied to the calculated Base Price.
   - Final prices after volume discount must be rounded to 2 decimal places.

**3. Customer Type Discounts:**
   Specific customer types receive a standard discount on all products.
   - **Discount List:**
     - Customer Type ""Reseller"": 15% discount
     - Customer Type ""Education"": 10% discount
     - Customer Type ""Standard"": 0% discount
   - Any customer type not explicitly listed above is considered ""Standard"" and receives a 0% discount.
   - The customer type discount percentage is applied to the calculated Base Price.
   - Final prices after customer type discount must be rounded to 2 decimal places.

**4. IMPORTANT - Discount Application:**
   For the purpose of this basic exercise, Volume Discounts and Customer Type Discounts are applied **independently** to the Base Price. They are **NOT** cumulative.
   - When calculating a price for a volume discount scenario, apply ONLY the volume discount.
   - When calculating a price for a customer type discount scenario, apply ONLY the customer type discount.

**5. Currency:**
   All monetary values (Unit Cost, Base Price, Final Prices) are expressed in USD.
```
</MATERIALS_FOR_CANDIDATE>

<MATERIALS_EXPLANATION_FOR_EVALUATOR>
**1. Overview:**
These materials provide the necessary inputs for the basic pricing exam. They consist of product cost data (`product_data.csv`) and a simple pricing policy document (`pricing_policy_basic.txt`). The goal is for the candidate to apply the rules from the policy to the data and calculate specific price points.

**2. `product_data.csv`:**
*   **Creation:** This CSV file contains 10 fictional products with unique `ProductID` codes (P001-P010), simple names, and varying `UnitCost` values.
*   **Purpose:** Provides the raw cost data needed for base price calculations. The costs include values that will result in non-integer base prices when the margin is applied, testing the candidate's attention to the rounding requirement (e.g., P002, P003, P005, P007, P010).
*   **Relationships:** The `ProductID` values in this file directly correspond to the `product_id` keys required in the candidate's JSON submission and are referenced in the specific calculation scenarios (V1-V3, C1-C3).

**3. `pricing_policy_basic.txt`:**
*   **Creation:** This text file outlines a clear, simple set of pricing rules designed to be unambiguous for a basic test.
*   **Key Rules & Purpose:**
    *   **Base Price Rule (Section 1):** `Base Price = UnitCost * 1.50`. This is a straightforward multiplication rule. The 1.50 factor represents a 50% margin on cost. The rounding requirement (2 decimal places) is explicitly stated.
    *   **Volume Discounts (Section 2):** Defines three distinct tiers (0%, 5%, 10%) based on quantity. The tiers have clear boundaries (1-9, 10-49, 50+). The scenarios provided (V1, V2, V3) are designed to test each tier boundary/rule: V3 tests the 0% tier, V1 tests the 5% tier, and V2 tests the 10% tier.
    *   **Customer Type Discounts (Section 3):** Defines specific discounts for ""Reseller"" (15%) and ""Education"" (10%). Crucially, it explicitly states that ""Standard"" or any unlisted type receives 0%. The scenarios (C1, C2, C3) test these specific types: C1 tests ""Reseller"", C2 tests ""Education"", and C3 tests the ""Standard""/0% rule.
    *   **Non-Cumulative Rule (Section 4):** This is a critical instruction for the *basic* exam. It explicitly forbids combining discounts, simplifying the calculation significantly. Candidates must apply *either* the volume *or* the customer discount based on the scenario, but not both. This is a key differentiator from a potential advanced exam.
    *   **Rounding:** The requirement to round final prices to 2 decimal places is repeated for clarity.
*   **Relationships:** The rules in this document must be applied to the `UnitCost` from `product_data.csv` to calculate `BasePrice`, and then subsequently used to calculate the final prices for the specific scenarios outlined in the candidate's instructions. The customer types (""Reseller"", ""Education"", ""Standard"") directly match those used in the C1-C3 scenarios.

**4. Expected Correct Calculations (for Evaluator Reference):**

*   **Base Prices:**
    *   P001: 100.00 * 1.50 = 150.00
    *   P002: 150.33 * 1.50 = 225.495 -> 225.50
    *   P003: 75.50 * 1.50 = 113.25
    *   P004: 220.00 * 1.50 = 330.00
    *   P005: 334.67 * 1.50 = 502.005 -> 502.01
    *   P006: 50.00 * 1.50 = 75.00
    *   P007: 63.33 * 1.50 = 94.995 -> 95.00
    *   P008: 120.00 * 1.50 = 180.00
    *   P009: 450.80 * 1.50 = 676.20
    *   P010: 88.25 * 1.50 = 132.375 -> 132.38

*   **Volume Discount Scenarios:**
    *   V1 (P002, Qty 25 -> 5%): 225.50 * (1 - 0.05) = 214.225 -> 214.23
    *   V2 (P005, Qty 60 -> 10%): 502.01 * (1 - 0.10) = 451.809 -> 451.81
    *   V3 (P008, Qty 5 -> 0%): 180.00 * (1 - 0.00) = 180.00

*   **Customer Discount Scenarios:**
    *   C1 (P001, Reseller -> 15%): 150.00 * (1 - 0.15) = 127.50
    *   C2 (P004, Education -> 10%): 330.00 * (1 - 0.10) = 297.00
    *   C3 (P007, Standard -> 0%): 95.00 * (1 - 0.00) = 95.00

**5. Potential Pitfalls / Common Errors:**
*   **Rounding Errors:** Incorrectly rounding (or failing to round) base prices or final prices. Using intermediate rounded values vs. calculating fully before rounding. (The provided correct answers assume calculation then final rounding).
*   **Misinterpreting Volume Tiers:** Applying the wrong discount percentage for a given quantity (e.g., using 5% for 50 units instead of 10%).
*   **Misinterpreting Customer Types:** Not applying the 0% discount for ""Standard"" or assuming a discount exists when it doesn't.
*   **Ignoring Non-Cumulative Rule:** Incorrectly trying to apply both a volume and customer discount simultaneously (this is explicitly forbidden by rule 4 for this basic exam).
*   **Data Entry Errors:** Typos when entering numbers into the spreadsheet or when constructing the final JSON.
*   **JSON Formatting Errors:** Incorrect keys, data types (e.g., strings instead of numbers for prices), or overall structure.

**6. ""Hidden"" Information:**
*   The policy explicitly states how to handle unlisted or ""Standard"" customer types (0% discount). Candidates need to read carefully to apply this correctly to scenario C3.
*   The non-cumulative rule (Section 4) is crucial and must be strictly followed. Candidates skimming might miss this constraint.
</MATERIALS_EXPLANATION_FOR_EVALUATOR>","
**1. Product Data (`product_data.csv`)**

```csv
ProductID,ProductName,UnitCost
P001,Alpha Widget,100.00
P002,Beta Gear,150.33
P003,Gamma Sprocket,75.50
P004,Delta Hub,220.00
P005,Epsilon Flange,334.67
P006,Zeta Pinion,50.00
P007,Eta Coupling,63.33
P008,Theta Valve,120.00
P009,Iota Sensor,450.80
P010,Kappa Actuator,88.25
```

**2. Basic Pricing Policy (`pricing_policy_basic.txt`)**

```text
**Company Pricing Policy - Basic Rules**

**Document Version:** 1.0
**Effective Date:** [Current Year]-01-01

**1. Base Price Calculation:**
   The standard Base Price for all products is calculated by applying a fixed margin to the Unit Cost.
   - **Rule:** Base Price = Unit Cost * 1.50
   - All calculated Base Prices must be rounded to 2 decimal places.

**2. Volume Discounts:**
   Volume discounts are offered based on the quantity of a single product line item within an order.
   - **Discount Tiers:**
     - 1 to 9 units: 0% discount
     - 10 to 49 units: 5% discount
     - 50 or more units: 10% discount
   - The volume discount percentage is applied to the calculated Base Price.
   - Final prices after volume discount must be rounded to 2 decimal places.

**3. Customer Type Discounts:**
   Specific customer types receive a standard discount on all products.
   - **Discount List:**
     - Customer Type ""Reseller"": 15% discount
     - Customer Type ""Education"": 10% discount
     - Customer Type ""Standard"": 0% discount
   - Any customer type not explicitly listed above is considered ""Standard"" and receives a 0% discount.
   - The customer type discount percentage is applied to the calculated Base Price.
   - Final prices after customer type discount must be rounded to 2 decimal places.

**4. IMPORTANT - Discount Application:**
   For the purpose of this basic exercise, Volume Discounts and Customer Type Discounts are applied **independently** to the Base Price. They are **NOT** cumulative.
   - When calculating a price for a volume discount scenario, apply ONLY the volume discount.
   - When calculating a price for a customer type discount scenario, apply ONLY the customer type discount.

**5. Currency:**
   All monetary values (Unit Cost, Base Price, Final Prices) are expressed in USD.
```
","Okay, here is the specification for the candidate's submission format for the Basic Practical Exam on Determining Price Schedules and Discount Rates.

**Submission File:**
*   You must submit a single file named `test_submission.json`.

**Candidate Identification:**
*   Include your candidate ID at the top level of the JSON object using the key `candidate_id`. Replace `""YOUR_ID_HERE""` with the specific model version you are using (e.g., ""GPT-4o"", ""Claude-3-Opus"", etc.).

**JSON Structure and Data Format:**
*   The file MUST be a valid JSON object.
*   Follow the exact structure, key names, and data types specified below.
*   **Monetary Values:** All price fields (`base_price`, `final_unit_price_volume_discount`, `final_unit_price_customer_discount`) MUST be represented as **Numbers** (not strings) and rounded to exactly **2 decimal places**.
*   **Product IDs:** Use the exact `product_id` strings as provided in `product_data.csv` (e.g., ""P001"", ""P002""). These MUST be **Strings**.
*   **Order IDs:** Use the exact `order_id` strings specified in the instructions (""V1"", ""V2"", ""V3"", ""C1"", ""C2"", ""C3""). These MUST be **Strings**.
*   **Quantity:** The `quantity` field MUST be an **Integer Number**.
*   **Customer Types:** Use the exact `customer_type` strings specified in the instructions (""Reseller"", ""Education"", ""Standard""). These MUST be **Strings**.

**Required JSON Format:**

```json
{
  ""candidate_id"": ""YOUR_ID_HERE"", // Replace YOUR_ID_HERE with your specific model version ID (String)
  ""base_prices"": [
    {
      ""product_id"": ""P001"", // String: Product ID from product_data.csv
      ""base_price"": 0.00     // Number: Calculated Base Price, rounded to 2 decimal places
    },
    {
      ""product_id"": ""P002"",
      ""base_price"": 0.00
    },
    // ... MUST include one object for EACH product listed in product_data.csv (P001 through P010)
    {
      ""product_id"": ""P010"",
      ""base_price"": 0.00
    }
  ],
  ""volume_discount_scenarios"": [
    {
      ""order_id"": ""V1"",          // String: Specific scenario ID from instructions
      ""product_id"": ""P002"",      // String: Product ID for this scenario
      ""quantity"": 0,             // Number (Integer): Quantity for this scenario
      ""final_unit_price_volume_discount"": 0.00 // Number: Calculated final unit price with volume discount, rounded to 2 decimal places
    },
    {
      ""order_id"": ""V2"",
      ""product_id"": ""P005"",
      ""quantity"": 0,
      ""final_unit_price_volume_discount"": 0.00
    },
    {
      ""order_id"": ""V3"",
      ""product_id"": ""P008"",
      ""quantity"": 0,
      ""final_unit_price_volume_discount"": 0.00
    }
    // MUST include exactly these three volume scenario objects (V1, V2, V3)
  ],
  ""customer_discount_scenarios"": [
    {
      ""order_id"": ""C1"",          // String: Specific scenario ID from instructions
      ""product_id"": ""P001"",      // String: Product ID for this scenario
      ""customer_type"": ""Reseller"", // String: Customer type for this scenario
      ""final_unit_price_customer_discount"": 0.00 // Number: Calculated final unit price with customer discount, rounded to 2 decimal places
    },
    {
      ""order_id"": ""C2"",
      ""product_id"": ""P004"",
      ""customer_type"": ""Education"",
      ""final_unit_price_customer_discount"": 0.00
    },
    {
      ""order_id"": ""C3"",
      ""product_id"": ""P007"",
      ""customer_type"": ""Standard"",
      ""final_unit_price_customer_discount"": 0.00
    }
    // MUST include exactly these three customer scenario objects (C1, C2, C3)
  ]
}
```

**Important Notes:**
*   Ensure the `base_prices` array contains an entry for **all 10 products** from the `product_data.csv` file, in any order.
*   Ensure the `volume_discount_scenarios` and `customer_discount_scenarios` arrays contain exactly the three specified scenarios each, using the correct `order_id`, `product_id`, `quantity` (where applicable), and `customer_type`.
*   Failure to adhere strictly to this format (file name, JSON structure, key names, data types, rounding) may result in evaluation errors. Do not include any extra keys or comments within the JSON file.","Okay, here are the requested materials for the evaluator regarding the Basic Practical Exam on Determining Price Schedules and Discount Rates.

---

**1. Complete Answer Key (`answer_key.json`)**

```json
{
  ""candidate_id"": ""ANSWER_KEY"",
  ""base_prices"": [
    {
      ""product_id"": ""P001"",
      ""base_price"": 150.00
    },
    {
      ""product_id"": ""P002"",
      ""base_price"": 225.50
    },
    {
      ""product_id"": ""P003"",
      ""base_price"": 113.25
    },
    {
      ""product_id"": ""P004"",
      ""base_price"": 330.00
    },
    {
      ""product_id"": ""P005"",
      ""base_price"": 502.01
    },
    {
      ""product_id"": ""P006"",
      ""base_price"": 75.00
    },
    {
      ""product_id"": ""P007"",
      ""base_price"": 95.00
    },
    {
      ""product_id"": ""P008"",
      ""base_price"": 180.00
    },
    {
      ""product_id"": ""P009"",
      ""base_price"": 676.20
    },
    {
      ""product_id"": ""P010"",
      ""base_price"": 132.38
    }
  ],
  ""volume_discount_scenarios"": [
    {
      ""order_id"": ""V1"",
      ""product_id"": ""P002"",
      ""quantity"": 25,
      ""final_unit_price_volume_discount"": 214.23
    },
    {
      ""order_id"": ""V2"",
      ""product_id"": ""P005"",
      ""quantity"": 60,
      ""final_unit_price_volume_discount"": 451.81
    },
    {
      ""order_id"": ""V3"",
      ""product_id"": ""P008"",
      ""quantity"": 5,
      ""final_unit_price_volume_discount"": 180.00
    }
  ],
  ""customer_discount_scenarios"": [
    {
      ""order_id"": ""C1"",
      ""product_id"": ""P001"",
      ""customer_type"": ""Reseller"",
      ""final_unit_price_customer_discount"": 127.50
    },
    {
      ""order_id"": ""C2"",
      ""product_id"": ""P004"",
      ""customer_type"": ""Education"",
      ""final_unit_price_customer_discount"": 297.00
    },
    {
      ""order_id"": ""C3"",
      ""product_id"": ""P007"",
      ""customer_type"": ""Standard"",
      ""final_unit_price_customer_discount"": 95.00
    }
  ]
}
```

---

**2. Explanation of Correct Answers and Derivations**

This section details how the correct values in the `answer_key.json` were derived based on the provided `product_data.csv` and `pricing_policy_basic.txt`.

**A. Base Price Calculations:**

*   **Rule:** Per `pricing_policy_basic.txt` (Section 1), `Base Price = Unit Cost * 1.50`, rounded to 2 decimal places.
*   **Derivations:**
    *   P001: 100.00 * 1.50 = 150.00
    *   P002: 150.33 * 1.50 = 225.495 -> rounded to 225.50
    *   P003: 75.50 * 1.50 = 113.25
    *   P004: 220.00 * 1.50 = 330.00
    *   P005: 334.67 * 1.50 = 502.005 -> rounded to 502.01
    *   P006: 50.00 * 1.50 = 75.00
    *   P007: 63.33 * 1.50 = 94.995 -> rounded to 95.00
    *   P008: 120.00 * 1.50 = 180.00
    *   P009: 450.80 * 1.50 = 676.20
    *   P010: 88.25 * 1.50 = 132.375 -> rounded to 132.38

**B. Volume Discount Scenario Calculations:**

*   **Rule:** Per `pricing_policy_basic.txt` (Section 2 & 4), apply the volume discount percentage *only* to the calculated Base Price, based on quantity tiers (1-9: 0%; 10-49: 5%; 50+: 10%). Round the final price to 2 decimal places.
*   **Derivations:**
    *   **V1 (P002, Qty 25):**
        *   Base Price (P002): 225.50
        *   Quantity 25 falls into the 10-49 tier -> 5% discount.
        *   Final Price = 225.50 * (1 - 0.05) = 225.50 * 0.95 = 214.225 -> rounded to 214.23
    *   **V2 (P005, Qty 60):**
        *   Base Price (P005): 502.01
        *   Quantity 60 falls into the 50+ tier -> 10% discount.
        *   Final Price = 502.01 * (1 - 0.10) = 502.01 * 0.90 = 451.809 -> rounded to 451.81
    *   **V3 (P008, Qty 5):**
        *   Base Price (P008): 180.00
        *   Quantity 5 falls into the 1-9 tier -> 0% discount.
        *   Final Price = 180.00 * (1 - 0.00) = 180.00 * 1.00 = 180.00

**C. Customer Discount Scenario Calculations:**

*   **Rule:** Per `pricing_policy_basic.txt` (Section 3 & 4), apply the customer type discount percentage *only* to the calculated Base Price (Reseller: 15%; Education: 10%; Standard/Other: 0%). Round the final price to 2 decimal places.
*   **Derivations:**
    *   **C1 (P001, Customer Type ""Reseller""):**
        *   Base Price (P001): 150.00
        *   Customer Type ""Reseller"" -> 15% discount.
        *   Final Price = 150.00 * (1 - 0.15) = 150.00 * 0.85 = 127.50
    *   **C2 (P004, Customer Type ""Education""):**
        *   Base Price (P004): 330.00
        *   Customer Type ""Education"" -> 10% discount.
        *   Final Price = 330.00 * (1 - 0.10) = 330.00 * 0.90 = 297.00
    *   **C3 (P007, Customer Type ""Standard""):**
        *   Base Price (P007): 95.00
        *   Customer Type ""Standard"" -> 0% discount (as per policy for listed ""Standard"" or unlisted types).
        *   Final Price = 95.00 * (1 - 0.00) = 95.00 * 1.00 = 95.00

**Key Evaluation Points:**
*   Correct application of the `Base Price = Unit Cost * 1.50` formula.
*   Correct rounding of Base Prices and Final Prices to 2 decimal places.
*   Correct identification of the appropriate volume discount tier based on quantity.
*   Correct identification of the appropriate customer type discount based on the provided type.
*   Strict adherence to the non-cumulative discount rule (Section 4 of the policy).
*   Accurate calculation of the final price after applying the single relevant discount.

---

**3. Passing Criteria**

To pass this basic practical exam, the candidate must demonstrate fundamental competence in applying the provided pricing rules and structuring the output correctly.

**Minimum Requirements:**

1.  **JSON Validity and Structure:**
    *   The submitted `test_submission.json` file MUST be valid JSON and parsable.
    *   The file MUST contain the top-level keys: `candidate_id`, `base_prices`, `volume_discount_scenarios`, `customer_discount_scenarios`.
    *   The `base_prices` array MUST contain entries for all 10 products.
    *   The `volume_discount_scenarios` and `customer_discount_scenarios` arrays MUST contain the 3 specified scenarios each.
    *   Key names and basic data types (string vs number vs array vs object) within the structure should generally match the required format.
    *   *(Minor deviations like an extra space or slightly different ordering within arrays are acceptable if the core structure and data are present and correct).*
    *   **Failure to submit a structurally sound and parsable JSON file results in an automatic fail.**

2.  **Calculation Accuracy:**
    *   There are a total of 16 primary price calculations required:
        *   10 Base Prices
        *   3 Final Prices (Volume Discount)
        *   3 Final Prices (Customer Discount)
    *   The candidate must achieve **at least 14 out of 16** correct price calculations (allowing for minor errors, e.g., one rounding mistake and one tier misinterpretation).
    *   A ""correct"" calculation means the final numerical value matches the answer key exactly after rounding to 2 decimal places.

**Summary:**

*   **Pass:** Submits a valid, structurally correct JSON file AND gets at least 14 out of the 16 price calculations correct.
*   **Fail:** Fails to submit a valid/parsable JSON, OR the JSON structure is significantly incorrect, OR fewer than 14 price calculations are correct.

This approach ensures the candidate can not only perform the basic calculations but also follow instructions regarding data handling and output formatting, which are practical skills for a Sales Manager using digital tools.","```python
import json
import sys
import os
from decimal import Decimal, ROUND_HALF_UP, InvalidOperation

# --- Configuration ---
OUTPUT_FILENAME = ""test_results.json""
EXPECTED_BASE_PRICE_COUNT = 10
EXPECTED_VOLUME_SCENARIO_COUNT = 3
EXPECTED_CUSTOMER_SCENARIO_COUNT = 3
REQUIRED_TOP_LEVEL_KEYS = [
    ""candidate_id"",
    ""base_prices"",
    ""volume_discount_scenarios"",
    ""customer_discount_scenarios"",
]
REQUIRED_BASE_PRICE_KEYS = [""product_id"", ""base_price""]
REQUIRED_VOLUME_KEYS = [
    ""order_id"",
    ""product_id"",
    ""quantity"",
    ""final_unit_price_volume_discount"",
]
REQUIRED_CUSTOMER_KEYS = [
    ""order_id"",
    ""product_id"",
    ""customer_type"",
    ""final_unit_price_customer_discount"",
]

# --- Helper Functions ---

def load_json(filepath):
    """"""Loads a JSON file with error handling.""""""
    if not os.path.exists(filepath):
        return None, f""Error: File not found at '{filepath}'""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data, None
    except json.JSONDecodeError as e:
        return None, f""Error: Invalid JSON format in '{filepath}'. Details: {e}""
    except Exception as e:
        return None, f""Error: Could not read file '{filepath}'. Details: {e}""

def safe_round(value):
    """"""Safely rounds a value to 2 decimal places using Decimal for accuracy.""""""
    try:
        # Convert potential floats or strings representing numbers to Decimal
        dec_value = Decimal(str(value))
        # Round to 2 decimal places, rounding halves up (standard rounding)
        return dec_value.quantize(Decimal(""0.01""), rounding=ROUND_HALF_UP)
    except (InvalidOperation, TypeError, ValueError):
        # Handle cases where value is not a number (e.g., None, non-numeric string)
        return None

def compare_values(actual, expected, context, results, points_possible, points_achieved):
    """"""Compares a single value, updates results and points.""""""
    is_correct = False
    details = {""expected"": expected, ""actual"": actual}

    # Handle numeric comparison with rounding
    if isinstance(expected, (int, float, Decimal)):
        actual_rounded = safe_round(actual)
        expected_rounded = safe_round(expected) # Should already be correct in key, but round for safety
        if actual_rounded is not None and expected_rounded is not None and actual_rounded == expected_rounded:
            is_correct = True
            # Store the rounded value for clarity in results if correct
            details[""actual_rounded""] = float(actual_rounded) # Convert back to float for JSON
            details[""expected_rounded""] = float(expected_rounded)
        elif actual_rounded is None:
             details[""error""] = ""Actual value is not a valid number or could not be rounded.""
        else:
             details[""error""] = f""Numeric mismatch (Expected rounded: {expected_rounded}, Actual rounded: {actual_rounded})""

    # Handle string comparison (case-sensitive)
    elif isinstance(expected, str):
        if isinstance(actual, str) and actual == expected:
            is_correct = True
        elif not isinstance(actual, str):
            details[""error""] = f""Type mismatch: Expected string, got {type(actual).__name__}""
        else:
            details[""error""] = ""String mismatch""

    # Handle integer comparison (specifically for quantity)
    elif isinstance(expected, int):
         try:
             if int(actual) == expected:
                 is_correct = True
             else:
                 details[""error""] = ""Integer mismatch""
         except (ValueError, TypeError):
             details[""error""] = f""Type mismatch or invalid integer: Expected integer, got {type(actual).__name__} ('{actual}')""

    else:
        # Fallback for other types if needed, though not expected here
        if actual == expected:
            is_correct = True
        else:
            details[""error""] = f""Mismatch for type {type(expected).__name__}""


    results[context] = {""correct"": is_correct, ""details"": details}
    points_possible[0] += 1
    if is_correct:
        points_achieved[0] += 1

# --- Main Evaluation Logic ---

def evaluate_submission(submission_path, answer_key_path):
    """"""Loads submission and key, performs evaluation, returns results.""""""

    submission_data, error = load_json(submission_path)
    if error:
        return {""error"": error, ""overall_score"": 0, ""evaluation_details"": {}}

    answer_key_data, error = load_json(answer_key_path)
    if error:
        # If the key is broken, we can't evaluate
        return {""error"": f""CRITICAL: {error}"", ""overall_score"": 0, ""evaluation_details"": {}}

    results = {
        ""candidate_id_submission"": submission_data.get(""candidate_id"", ""MISSING""),
        ""file_validation"": {""errors"": [], ""warnings"": []},
        ""structure_check"": {},
        ""base_price_check"": {},
        ""volume_scenario_check"": {},
        ""customer_scenario_check"": {},
    }
    points_achieved = [0] # Use list to pass by reference
    points_possible = [0] # Use list to pass by reference

    # 1. File Validation & Basic Structure Check
    # ------------------------------------------
    valid_structure = True
    for key in REQUIRED_TOP_LEVEL_KEYS:
        if key not in submission_data:
            results[""file_validation""][""errors""].append(f""Missing required top-level key: '{key}'"")
            valid_structure = False
        elif key.endswith(""_prices"") or key.endswith(""_scenarios""):
             if not isinstance(submission_data[key], list):
                 results[""file_validation""][""errors""].append(f""Key '{key}' should be a list, but found {type(submission_data[key]).__name__}"")
                 valid_structure = False

    if not valid_structure:
        results[""error""] = ""Submission failed basic structure validation. Cannot proceed with detailed checks.""
        results[""overall_score""] = 0
        return results

    # Check array lengths
    bp_len = len(submission_data.get(""base_prices"", []))
    vol_len = len(submission_data.get(""volume_discount_scenarios"", []))
    cust_len = len(submission_data.get(""customer_discount_scenarios"", []))

    results[""structure_check""][""base_prices_count""] = {""actual"": bp_len, ""expected"": EXPECTED_BASE_PRICE_COUNT, ""correct"": bp_len == EXPECTED_BASE_PRICE_COUNT}
    results[""structure_check""][""volume_scenarios_count""] = {""actual"": vol_len, ""expected"": EXPECTED_VOLUME_SCENARIO_COUNT, ""correct"": vol_len == EXPECTED_VOLUME_SCENARIO_COUNT}
    results[""structure_check""][""customer_scenarios_count""] = {""actual"": cust_len, ""expected"": EXPECTED_CUSTOMER_SCENARIO_COUNT, ""correct"": cust_len == EXPECTED_CUSTOMER_SCENARIO_COUNT}

    if not results[""structure_check""][""base_prices_count""][""correct""]:
        results[""file_validation""][""warnings""].append(f""Incorrect number of base price entries found."")
    if not results[""structure_check""][""volume_scenarios_count""][""correct""]:
         results[""file_validation""][""warnings""].append(f""Incorrect number of volume scenario entries found."")
    if not results[""structure_check""][""customer_scenarios_count""][""correct""]:
         results[""file_validation""][""warnings""].append(f""Incorrect number of customer scenario entries found."")


    # 2. Detailed Checks (Proceed even with count warnings, but check item existence)
    # -----------------------------------------------------------------------------

    # Convert lists to dicts for easier lookup (handle potential duplicates by taking the last one)
    key_base_prices = {item['product_id']: item for item in answer_key_data.get(""base_prices"", [])}
    sub_base_prices = {item.get('product_id'): item for item in submission_data.get(""base_prices"", []) if isinstance(item, dict)}

    key_volume_scenarios = {item['order_id']: item for item in answer_key_data.get(""volume_discount_scenarios"", [])}
    sub_volume_scenarios = {item.get('order_id'): item for item in submission_data.get(""volume_discount_scenarios"", []) if isinstance(item, dict)}

    key_customer_scenarios = {item['order_id']: item for item in answer_key_data.get(""customer_discount_scenarios"", [])}
    sub_customer_scenarios = {item.get('order_id'): item for item in submission_data.get(""customer_discount_scenarios"", []) if isinstance(item, dict)}

    # --- Base Price Check ---
    results[""base_price_check""][""items""] = {}
    for prod_id, key_item in key_base_prices.items():
        item_results = {}
        sub_item = sub_base_prices.get(prod_id)

        if sub_item and isinstance(sub_item, dict):
             # Check required keys exist in submission item
             missing_keys = [k for k in REQUIRED_BASE_PRICE_KEYS if k not in sub_item]
             if missing_keys:
                 item_results[""structure_error""] = f""Missing keys in submission item: {missing_keys}""
                 points_possible[0] += len(REQUIRED_BASE_PRICE_KEYS) # Increment possible points even if structure fails
             else:
                # Compare product_id (should always match if found by key)
                compare_values(sub_item['product_id'], key_item['product_id'], 'product_id_check', item_results, points_possible, points_achieved)
                # Compare base_price
                compare_values(sub_item['base_price'], key_item['base_price'], 'base_price_check', item_results, points_possible, points_achieved)
        else:
            item_results[""error""] = ""Product ID not found in submission or item is not a valid dictionary.""
            points_possible[0] += len(REQUIRED_BASE_PRICE_KEYS) # Increment possible points even if missing

        results[""base_price_check""][""items""][prod_id] = item_results

    # --- Volume Scenario Check ---
    results[""volume_scenario_check""][""items""] = {}
    for order_id, key_item in key_volume_scenarios.items():
        item_results = {}
        sub_item = sub_volume_scenarios.get(order_id)

        if sub_item and isinstance(sub_item, dict):
            missing_keys = [k for k in REQUIRED_VOLUME_KEYS if k not in sub_item]
            if missing_keys:
                 item_results[""structure_error""] = f""Missing keys in submission item: {missing_keys}""
                 points_possible[0] += len(REQUIRED_VOLUME_KEYS)
            else:
                compare_values(sub_item['order_id'], key_item['order_id'], 'order_id_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['product_id'], key_item['product_id'], 'product_id_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['quantity'], key_item['quantity'], 'quantity_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['final_unit_price_volume_discount'], key_item['final_unit_price_volume_discount'], 'price_check', item_results, points_possible, points_achieved)
        else:
            item_results[""error""] = ""Order ID not found in submission or item is not a valid dictionary.""
            points_possible[0] += len(REQUIRED_VOLUME_KEYS)

        results[""volume_scenario_check""][""items""][order_id] = item_results

    # --- Customer Scenario Check ---
    results[""customer_scenario_check""][""items""] = {}
    for order_id, key_item in key_customer_scenarios.items():
        item_results = {}
        sub_item = sub_customer_scenarios.get(order_id)

        if sub_item and isinstance(sub_item, dict):
            missing_keys = [k for k in REQUIRED_CUSTOMER_KEYS if k not in sub_item]
            if missing_keys:
                 item_results[""structure_error""] = f""Missing keys in submission item: {missing_keys}""
                 points_possible[0] += len(REQUIRED_CUSTOMER_KEYS)
            else:
                compare_values(sub_item['order_id'], key_item['order_id'], 'order_id_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['product_id'], key_item['product_id'], 'product_id_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['customer_type'], key_item['customer_type'], 'customer_type_check', item_results, points_possible, points_achieved)
                compare_values(sub_item['final_unit_price_customer_discount'], key_item['final_unit_price_customer_discount'], 'price_check', item_results, points_possible, points_achieved)
        else:
            item_results[""error""] = ""Order ID not found in submission or item is not a valid dictionary.""
            points_possible[0] += len(REQUIRED_CUSTOMER_KEYS)

        results[""customer_scenario_check""][""items""][order_id] = item_results

    # 3. Calculate Final Score
    # ------------------------
    total_possible = points_possible[0]
    total_achieved = points_achieved[0]

    if total_possible > 0:
        overall_score_percent = round((total_achieved / total_possible) * 100, 2)
    else:
        overall_score_percent = 0 # Avoid division by zero if no points were possible (e.g., broken key)

    results[""score_summary""] = {
        ""points_achieved"": total_achieved,
        ""points_possible"": total_possible,
    }
    results[""overall_score""] = overall_score_percent # Add the required overall_score variable

    return results

# --- Script Execution ---

if __name__ == ""__main__"":
    if len(sys.argv) != 3:
        print(""Usage: python task_evaluation.py <submission_file.json> <answer_key_file.json>"")
        sys.exit(1)

    submission_file = sys.argv[1]
    answer_key_file = sys.argv[2]

    print(f""Evaluating '{submission_file}' against '{answer_key_file}'..."")

    evaluation_results = evaluate_submission(submission_file, answer_key_file)

    # Ensure results are serializable (convert Decimals back to floats if any remain)
    # Although safe_round returns Decimal, compare_values stores float representations
    # This is more of a safeguard.
    def default_serializer(obj):
        if isinstance(obj, Decimal):
            return float(obj)
        raise TypeError(f""Object of type {obj.__class__.__name__} is not JSON serializable"")

    try:
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, indent=4, default=default_serializer)
        print(f""Evaluation complete. Results saved to '{OUTPUT_FILENAME}'"")
        # Optionally print the score to console
        if ""overall_score"" in evaluation_results:
             print(f""Overall Score: {evaluation_results['overall_score']}%"")
        if evaluation_results.get(""error""):
            print(f""Evaluation Error: {evaluation_results['error']}"")

    except Exception as e:
        print(f""Error: Could not write results to '{OUTPUT_FILENAME}'. Details: {e}"")
        # Print results to console as a fallback
        print(""\n--- Evaluation Results (Fallback Console Output) ---"")
        try:
            print(json.dumps(evaluation_results, indent=4, default=default_serializer))
        except Exception as json_e:
             print(f""Could not even serialize results for console output: {json_e}"")
             print(evaluation_results) # Raw print if serialization fails
        print(""--- End Fallback Console Output ---"")
        sys.exit(1)

    sys.exit(0)
```

**Explanation:**

1.  **Import Libraries:** Imports `json` for JSON handling, `sys` for command-line arguments, `os` for file existence checks, and `Decimal` for accurate monetary calculations and rounding.
2.  **Configuration:** Sets constants for filenames, expected item counts, and required keys within the JSON structures. This makes the script easier to modify if the exam format changes slightly.
3.  **`load_json(filepath)`:** Safely loads a JSON file, returning the data and `None` for error, or `None` and an error message string if loading fails (file not found, invalid JSON).
4.  **`safe_round(value)`:** Takes a value (potentially string, float, int), converts it to `Decimal`, rounds it to 2 decimal places using `ROUND_HALF_UP` (standard rounding), and returns the `Decimal`. Returns `None` if the input cannot be converted to a number. Using `Decimal` avoids floating-point inaccuracies inherent in standard floats.
5.  **`compare_values(...)`:** This is the core comparison function.
    *   It takes the actual value (from submission), expected value (from key), a context string (for reporting), the results dictionary, and the `points_possible`/`points_achieved` lists (passed as lists so they are mutable references).
    *   It determines the type of the `expected` value to decide how to compare.
    *   **Numeric:** Uses `safe_round` on both actual and expected values and compares the resulting `Decimal` objects. Stores details including rounded values and errors if comparison fails or rounding is not possible.
    *   **String:** Performs a case-sensitive string comparison.
    *   **Integer:** Attempts to convert the `actual` value to an `int` and compares.
    *   Updates the `results` dictionary with `correct: True/False` and detailed comparison info.
    *   Increments `points_possible` and `points_achieved` accordingly.
6.  **`evaluate_submission(submission_path, answer_key_path)`:**
    *   Loads both JSON files using `load_json`. Exits early if files cannot be loaded.
    *   Initializes the main `results` dictionary.
    *   **File Validation & Structure Check:** Checks for the presence of required top-level keys and verifies that the main data sections are lists. Checks if the lists have the expected number of items, adding warnings if counts mismatch. If basic structure is invalid, it stops detailed checks.
    *   **Detailed Checks:**
        *   Converts the lists of items (base prices, scenarios) into dictionaries keyed by `product_id` or `order_id` for efficient lookup. This handles cases where the candidate's list order differs from the key. It implicitly takes the *last* item if there are duplicate IDs in the submission.
        *   Iterates through the *answer key's* items for each section (base prices, volume scenarios, customer scenarios).
        *   For each key item, it looks up the corresponding item in the submission's dictionary.
        *   If the submission item exists and is a dictionary:
            *   It checks if all required keys are present within that item.
            *   It calls `compare_values` for each relevant field (`product_id`, `base_price`, `quantity`, `customer_type`, etc.).
        *   If the submission item is missing or not a dict, it logs an error and increments `points_possible` (since the candidate *should* have provided it).
        *   Stores the detailed comparison results for each item under its ID (`P001`, `V1`, `C1`, etc.).
    *   **Calculate Final Score:** Calculates the percentage score based on `points_achieved` and `points_possible`. Adds the `overall_score` key to the results.
    *   Returns the complete `results` dictionary.
7.  **`if __name__ == ""__main__"":` Block:**
    *   Ensures the script is run directly.
    *   Checks for the correct number of command-line arguments (script name + 2 file paths).
    *   Calls `evaluate_submission`.
    *   Defines a `default_serializer` to handle any `Decimal` objects that might remain before writing to JSON (though `compare_values` stores floats).
    *   Writes the `evaluation_results` dictionary to `test_results.json` with pretty-printing (`indent=4`).
    *   Prints confirmation messages and the overall score to the console.
    *   Includes error handling for writing the output file, printing results to the console as a fallback if writing fails.

This script provides a robust way to automatically evaluate the candidate's submission against the defined criteria and answer key.","{'candidate_id': 'ANSWER_KEY', 'base_prices': [{'product_id': 'P001', 'base_price': 150.0}, {'product_id': 'P002', 'base_price': 225.5}, {'product_id': 'P003', 'base_price': 113.25}, {'product_id': 'P004', 'base_price': 330.0}, {'product_id': 'P005', 'base_price': 502.01}, {'product_id': 'P006', 'base_price': 75.0}, {'product_id': 'P007', 'base_price': 95.0}, {'product_id': 'P008', 'base_price': 180.0}, {'product_id': 'P009', 'base_price': 676.2}, {'product_id': 'P010', 'base_price': 132.38}], 'volume_discount_scenarios': [{'order_id': 'V1', 'product_id': 'P002', 'quantity': 25, 'final_unit_price_volume_discount': 214.23}, {'order_id': 'V2', 'product_id': 'P005', 'quantity': 60, 'final_unit_price_volume_discount': 451.81}, {'order_id': 'V3', 'product_id': 'P008', 'quantity': 5, 'final_unit_price_volume_discount': 180.0}], 'customer_discount_scenarios': [{'order_id': 'C1', 'product_id': 'P001', 'customer_type': 'Reseller', 'final_unit_price_customer_discount': 127.5}, {'order_id': 'C2', 'product_id': 'P004', 'customer_type': 'Education', 'final_unit_price_customer_discount': 297.0}, {'order_id': 'C3', 'product_id': 'P007', 'customer_type': 'Standard', 'final_unit_price_customer_discount': 95.0}]}",[],True,True,0.0,99.0,100.0,1.0,False,"Could not parse JSON. Raw LLM response:
```json
{
  ""makes_sense"": true,
  ""explanation"": ""The exam is practical, testing realistic basic tasks (applying predefined pricing/discount rules using spreadsheets, formatting JSON output) relevant for a Sales Manager in 2025, especially those executing established policies. Instructions, materials, and submission format are mostly unambiguous. The answer key correctly reflects the calculations based on the provided policy. \n\nPotential Weaknesses/Ambiguities:\n1.  **CRITICAL: Grading Mismatch:** The primary weakness is the discrepancy between the stated 'Passing Criteria' (requiring >=14 out of 16 specific *price* calculations correct) and the 'Grading Script' logic. The script calculates a score based on the percentage of *all* checked fields (including IDs, quantities, types), not just the 16 price fields. This could lead to incorrect pass/fail outcomes (e.g., passing despite missing critical price calculations, or failing despite meeting the price accuracy threshold but missing other fields).\n2.  **Minor Clarity Issues:** \n    - The `candidate_id` instruction in the Submission Format section refers to AI model versions and should be updated for human candidates (e.g., specify using an assigned ID).\n    - Minor inconsistencies exist between instructions and materials regarding file extensions (`.xlsx`/`.csv`, `.pdf`/`.txt`), though unlikely to cause major confusion.\n    - The phrasing regarding quantity in customer discount scenarios is slightly awkward but understandable in context.\n3.  **Answer Key:** The provided answer key appears internally consistent with the rules and would score 100% when evaluated by the provided script against itself.""
}
```","{'overview': prompt_token_count: 229
candidates_token_count: 660
total_token_count: 2547
, 'instructions': prompt_token_count: 1095
candidates_token_count: 1561
total_token_count: 4581
, 'materials': prompt_token_count: 2861
candidates_token_count: 2380
total_token_count: 6799
, 'check_materials': prompt_token_count: 4045
candidates_token_count: 1
total_token_count: 4561
, 'check_website': prompt_token_count: 4078
candidates_token_count: 1
total_token_count: 4546
, 'submission': prompt_token_count: 5025
candidates_token_count: 1208
total_token_count: 6617
, 'evaluation': prompt_token_count: 6154
candidates_token_count: 2660
total_token_count: 11659
, 'grading': prompt_token_count: 9003
candidates_token_count: 5073
total_token_count: 16441
, 'check_sense': prompt_token_count: 14524
candidates_token_count: 353
total_token_count: 17268
}",management_occupations
