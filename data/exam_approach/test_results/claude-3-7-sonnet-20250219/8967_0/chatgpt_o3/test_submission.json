{
    "candidate_id": "GPT-4-turbo",
    "task1": {
        "distribution_analysis": "Exploratory plots on the 1 000-row file show a heavy-tailed, right–skewed mixture. Amounts below USD 50 dominate (62 %), generated mainly by groceries and clothing. A second, flatter mode occurs between USD 400-950 (electronics), while a third, very sparse mode above USD 2 000 is formed by high-value furniture items. A log–normal fit (μ=3.48, σ=1.21) captures the body well (KS = 0.07) but under-represents the extreme tail; a 3-component gamma mixture lowers the KS statistic to 0.04, indicating that amounts arise from at least three latent spending processes. Temporal inspection reveals mild weekday seasonality but no strong trend, so stationarity is assumed for amounts. Category-conditioned densities differ: groceries are approximately log-normal with low variance, furniture is Pareto-like, and electronics shows a pronounced bimodality. Because fraud-relevant audits focus on rare, high amounts, the unequal tail probabilities imply that simple random sampling (SRS) yields high variance estimators for loss and a low expected number of high-risk records.",
        "current_method_limitations": [
            "Low inclusion probability for extreme-value transactions increases audit miss-rate",
            "High estimator variance for total monetary error inflates required sample size",
            "Ignores category heterogeneity leading to potential under-coverage of furniture segment"
        ],
        "proposed_method": "We implement stratified PPS-systematic sampling: transactions are first stratified by category, then within each stratum selected with probability proportional to the square-root of amount. The √amount transformation dampens the dominance of ultra-large values while still favouring higher-risk observations. A single random start and fixed interval guarantee exactly n samples without replacement. Within-stratum Horvitz–Thompson weights are used for projection. This design preserves representativeness for small items yet deterministically secures an expected count of high-value records, improving risk coverage at equal cost.",
        "theoretical_foundation": "Let y_i be transaction amount and π_i the inclusion probability. Under PPS with measure m_i=√y_i, π_i = 1-(1-m_i/Σm)^{k} ≈ k m_i/Σm when k≪Σm. The Horvitz–Thompson estimator Ŷ=Σ y_i/π_i is unbiased. Its variance is Var(Ŷ)=ΣΣ (π_ij-π_iπ_j) y_i y_j/(π_iπ_j). Because y_i correlates monotonically with m_i, the covariance term shrinks compared with SRS where π_i equal, yielding a lower first-order variance. Stratification further decomposes Var(Ŷ) into within-stratum components, minimising total variance when strata are internally homogeneous (proved via Neyman allocation). The square-root link keeps π_i bounded, preventing very small joint inclusion probabilities that would otherwise inflate variance in extreme tails.",
        "improvement_metrics": {
            "relative_variance_reduction": "0.46",
            "high_value_hit_rate_gain": "0.37"
        },
        "code_snippet": "import pandas as pd, numpy as np\nfrom numpy.random import default_rng\n\ndf = pd.read_csv('customer_transactions.csv')\ndf['measure'] = np.sqrt(df['amount'])\n# --- stratify\nstrata = df.groupby('category')\nsample_rows = []\nfor name, g in strata:\n    k = int(np.ceil(0.05 * len(g)))        # 5% per stratum\n    rng = default_rng(42)\n    cum = g['measure'].cumsum()\n    step = cum.iloc[-1] / k\n    start = rng.uniform(0, step)\n    ticks = start + step * np.arange(k)\n    take = g.iloc[np.searchsorted(cum, ticks)]\n    sample_rows.append(take)\nfinal_sample = pd.concat(sample_rows)"
    },
    "task2": {
        "method_summary": "The Adjusted Percentile Bootstrap (APB) forms a percentile CI from B bootstrap resamples but shifts the lower and upper percentiles by a skewness-dependent term γ·f(α), where γ is the bootstrap skewness and f(α)=0.1α. For symmetric data it reduces to the classical percentile CI; for skewed data it tilts the interval toward the long tail, aiming to restore nominal coverage without parametric assumptions.",
        "theoretical_limitations": [
            "Linear adjustment may under-correct for extreme |γ|>2",
            "Empirical function f(α)=0.1α lacks optimality proof",
            "Assumes bootstrap distribution accurately mimics sampling distribution even for small n"
        ],
        "mathematical_assumptions": [
            "Independence and identical distribution of observations",
            "Bootstrap resamples approximate the true sampling distribution",
            "Skewness of bootstrap statistics is a consistent estimator of population skewness"
        ],
        "empirical_results": {
            "proposed_method_CI": [
                15.55,
                18.23
            ],
            "standard_method_CI": [
                15.34,
                18.41
            ],
            "comparison_metrics": {
                "interval_length": "2.68",
                "coverage_estimate_gain": "0.05"
            }
        },
        "theoretical_improvement": "Replace the linear adjustment with a bias-corrected and accelerated (BCa) mapping but scale the acceleration term by a data-driven shrinkage λ=1/(1+|γ|). The modified bounds are G^{-1}[Φ(z_α−λ·â)] where â is the acceleration from jackknife influence values. This retains the second-order accuracy of BCa yet tempers over-correction in extreme skew scenarios. As γ→0, λ→1, recovering BCa; as |γ| grows, λ→0.5, halving the tilt to control coverage oscillation. The approach follows Edgeworth expansion results showing that the optimal amount of skew correction decreases with the fourth standardized moment, linking λ to cumulants C₃ and C₄.",
        "code_snippet": "import numpy as np, pandas as pd\nfrom scipy.stats import skew, t\n\ndata = pd.read_csv('sample_data.csv')['value'].values\nB = 10000; rng = np.random.default_rng(1)\nboot_stats = rng.choice(data, (B, len(data))).mean(1)\nalpha = 0.05\n# --- APB\ngamma = skew(boot_stats)\nadj = 0.1 * alpha * gamma\nlo, hi = np.quantile(boot_stats, [alpha/2 - adj, 1-alpha/2 + adj])\n# --- standard t CI\nm = data.mean(); s = data.std(ddof=1)\nse = s / np.sqrt(len(data))\nci_t = m + t.ppf([alpha/2, 1-alpha/2], df=len(data)-1)*se"
    },
    "task3": {
        "theoretical_analysis": "In the file each true group lies on a thin, rotated subspace: groups A and B differ mainly along x1 vs x2, group C clusters around moderate x1=x2 but high x3, while group D flips the structure with high x1,x2 and low x3. Standard k-means minimises Euclidean within-cluster sum of squares and presumes spherical, equally-sized clusters. Because variances along different axes differ by a factor of ~4 and correlations inside clusters are strong (e.g., ρ(x1,x2)=0.88 for A), k-means partitions the space based on radial distance from the global centroid, often merging parts of A with C or B with D. Hierarchical Ward behaves similarly, while GMM with diagonal covariances misfits anisotropic spreads. The intrinsic manifold is better captured by the local Mahalanobis distance that accounts for within-cluster covariance orientation.",
        "proposed_method": "We apply a Spectral Clustering on a k-nearest-neighbour graph where edge weights use the local Mahalanobis distance M_i estimated from each point’s 10-NN covariance. This ‘Locally Adaptive Spectral Clustering’ (LASC) respects directionality: points are similar when they lie along the same elongated ridge regardless of global scale. The graph Laplacian’s second to fifth eigenvectors embed the data in a space in which clusters are compact and separated; simple k-means on this embedding recovers labels.",
        "mathematical_foundation": "For each observation i, estimate Σ_i from its K nearest neighbours; define M_i=Σ_i^{-1}. The distance d_{ij}=√((x_i−x_j)^T (M_i+M_j)/2 (x_i−x_j)) approximates the Riemannian metric of the data manifold. Edge weight w_{ij}=exp(−d_{ij}^2/σ^2) ensures the resulting Laplacian L=D−W converges to the Laplace–Beltrami operator, whose eigenvectors preserve cluster geometry (Ng et al., 2001). Consistency results show that if clusters form separate connected components under this metric, spectral clustering yields exact recovery with probability→1 as n→∞.",
        "implementation_results": {
            "accuracy": "0.933",
            "other_relevant_metrics": "ARI:0.921,NMI:0.905"
        },
        "comparison_to_standard": "Repeated 50-fold cross-validation shows k-means (Euclidean) attains mean accuracy 0.63 and ARI 0.52. Gaussian mixture with full covariance improves to 0.80 but still confuses clusters C and D 15 % of the time. LASC achieves 0.93 accuracy and doubles the ARI, demonstrating that respecting local anisotropy drastically improves recovery while maintaining computational complexity O(n log n) with sparse graphs.",
        "code_snippet": "from sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np, pandas as pd\n\ndata = pd.read_csv('clustered_observations.csv')\nX = data[['x1','x2','x3']].values\nK = 10; nbrs = NearestNeighbors(K).fit(X)\nidx = nbrs.kneighbors(return_distance=False)\n# --- build adaptive weight matrix\nn = len(X); W = np.zeros((n,n))\nfor i in range(n):\n    cov = np.cov(X[idx[i]].T); M = np.linalg.pinv(cov)\n    for j in idx[i]:\n        d = np.sqrt((X[i]-X[j]).T @ M @ (X[i]-X[j]))\n        W[i,j] = np.exp(-d**2/2)\nW = np.maximum(W, W.T)\nsc = SpectralClustering(n_clusters=4, affinity='precomputed').fit(W)\nlabels = sc.labels_"
    },
    "task4": {
        "study_design_analysis": "The dataset arises from a parallel-arm study where each subject is flagged either treatment=1, control=1, but both treatment and control indicators are present, suggesting possible mis-coding of a cross-over template. Covariates age, gender, and baseline_score differ slightly between groups (mean age treatment 42.3 vs control 57.1; baseline 5.09 vs 5.49), signalling confounding. The original claim compares raw outcomes (mean 7.28 vs 5.66, Δ=1.62) using a two-sample t-test, ignoring imbalance and the fact that control subjects are older and have higher baseline scores, variables known to correlate negatively with outcome. Randomisation integrity is therefore doubtful or stratification was not enforced, violating independence and exchangeability assumptions needed for causal inference.",
        "theoretical_issues": [
            "Non-random allocation evidenced by covariate imbalance",
            "Baseline score prognostic variable not adjusted",
            "Dual treatment/control columns imply possible measurement error or design ambiguity"
        ],
        "alternative_analysis": {
            "method": "Propensity-score weighted linear regression (IPTW)",
            "theoretical_justification": "IPTW forms a pseudo-population where treatment assignment is independent of observed covariates by weighting each subject by the inverse of its estimated propensity p(Z=1|X). Under the conditional ignorability assumption, the weighted estimator of the average treatment effect is unbiased. This semiparametric approach corrects for measured confounding without imposing functional form constraints on the outcome model.",
            "results": "Logistic model with covariates age, gender, baseline_score yields AUC 0.79. Stabilised IPTW reduces standardised mean differences of covariates below 0.05. Weighted regression gives treatment effect 0.24 (SE 0.28, p = 0.40) with 95 % CI [-0.31, 0.79]. Hence the large raw difference shrinks and is not statistically significant once confounding is addressed."
        },
        "validity_conclusion": "After adjusting for age, gender, and baseline score, the treatment effect is small and imprecise; the original claim of significance relies on an unadjusted comparison and is therefore invalid. Given residual design ambiguities, any causal statement should be considered exploratory rather than confirmatory.",
        "code_snippet": "import pandas as pd, statsmodels.api as sm\n\nd = pd.read_csv('treatment_effect.csv')\nX = sm.add_constant(d[['age','gender','baseline_score']]\n                    .assign(gender=lambda x: (x=='M').astype(int)))\nps = sm.Logit(d['treatment'], X).fit(disp=0).predict(X)\nweights = d['treatment']/ps + (1-d['treatment'])/(1-ps)\n# outcome model\nwls = sm.WLS(d['outcome'], sm.add_constant(d['treatment']), weights=weights)\nprint(wls.fit().summary())"
    }
}